{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 0 ‚Äì Installations & environnement"
      ],
      "metadata": {
        "id": "fYN1Yr9oGx_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tIUHp2gC560",
        "outputId": "127b39d8-974c-4d60-b04a-2b8b2510ff08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installation/MAJ paquets de base‚Ä¶\n",
            "üì¶ Installation MCP‚Ä¶\n",
            "‚ö†Ô∏è  mcp-server-filesystem non disponible sur ce runtime ‚Äî on utilisera le fallback plus tard.\n",
            "üîé V√©rifications d‚Äôimports‚Ä¶\n",
            "‚úÖ Ollama d√©j√† install√©\n",
            "\n",
            "‚úÖ R√âCAPITULATIF\n",
            "streamlit  ‚Üí 1.48.1\n",
            "pandas     ‚Üí 2.3.2\n",
            "openai     ‚Üí 1.101.0\n",
            "nest-asyncio‚Üí 1.6.0\n",
            "pyngrok    ‚Üí 7.3.0\n",
            "cloudflared‚Üí 1.0.0.2\n",
            "mcp        ‚Üí 1.13.1\n",
            "mcp-server-fetch    ‚Üí 2025.4.7\n",
            "mcp-server-filesystem‚Üí NOT INSTALLED\n",
            "ollama bin : /usr/local/bin/ollama\n",
            "fetch bin  : /usr/local/bin/mcp-server-fetch\n",
            "fs bin     : None\n",
            "\n",
            "üéâ PARTIE 0 OK ‚Äî tu peux continuer aux parties suivantes.\n"
          ]
        }
      ],
      "source": [
        "# PARTIE 0 ‚Äî Installations & environnement (force install + v√©rifications)\n",
        "import sys, subprocess, shutil, os\n",
        "\n",
        "def pipi(*pkgs):\n",
        "    # installe/upgrade silencieusement\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"--quiet\", *pkgs]\n",
        "    return subprocess.check_call(cmd)\n",
        "\n",
        "# 1) Paquets essentiels (streamlit + d√©pendances)\n",
        "print(\"üì¶ Installation/MAJ paquets de base‚Ä¶\")\n",
        "pipi(\n",
        "    \"streamlit>=1.36.0\",\n",
        "    \"pandas>=2.2.2\",\n",
        "    \"openai>=1.40.0\",\n",
        "    \"nest-asyncio>=1.6.0\",\n",
        "    \"pyngrok>=7.1.3\",\n",
        "    \"cloudflared>=0.4\",\n",
        ")\n",
        "\n",
        "# 2) Paquets MCP (fetch officiel) + filesystem (si dispo)\n",
        "print(\"üì¶ Installation MCP‚Ä¶\")\n",
        "pipi(\"mcp>=1.0.1\", \"mcp-server-fetch>=0.1.2\")\n",
        "try:\n",
        "    pipi(\"mcp-server-filesystem>=0.1.2\")\n",
        "    _fs_pkg_ok = True\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"‚ö†Ô∏è  mcp-server-filesystem non disponible sur ce runtime ‚Äî on utilisera le fallback plus tard.\")\n",
        "    _fs_pkg_ok = False\n",
        "\n",
        "# 3) V√©rifications d‚Äôimports (streamlit surtout)\n",
        "print(\"üîé V√©rifications d‚Äôimports‚Ä¶\")\n",
        "try:\n",
        "    import streamlit as st  # noqa: F401\n",
        "    _streamlit_ok = True\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è  Import streamlit a √©chou√©, on force une r√©installation propre‚Ä¶\", e)\n",
        "    # R√©installe en for√ßant\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--force-reinstall\", \"streamlit>=1.36.0\"])\n",
        "    import importlib\n",
        "    importlib.invalidate_caches()\n",
        "    try:\n",
        "        import streamlit as st  # noqa: F401\n",
        "        _streamlit_ok = True\n",
        "    except Exception as e2:\n",
        "        _streamlit_ok = False\n",
        "        print(\"‚ùå  √âchec import streamlit apr√®s r√©installation:\", e2)\n",
        "\n",
        "# 4) (Optionnel) Installer Ollama si absent (utile pour les parties suivantes)\n",
        "if shutil.which(\"ollama\") is None:\n",
        "    print(\"üì¶ Installation d'Ollama (peut prendre ~30s)‚Ä¶\")\n",
        "    # Colab accepte le pipe shell -> sh\n",
        "    subprocess.check_call(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True)\n",
        "else:\n",
        "    print(\"‚úÖ Ollama d√©j√† install√©\")\n",
        "\n",
        "# 5) Affichage d‚Äô√©tat\n",
        "import importlib.metadata as md\n",
        "def v(pkg):\n",
        "    try: return md.version(pkg)\n",
        "    except md.PackageNotFoundError: return \"NOT INSTALLED\"\n",
        "\n",
        "print(\"\\n‚úÖ R√âCAPITULATIF\")\n",
        "print(\"streamlit  ‚Üí\", v(\"streamlit\"))\n",
        "print(\"pandas     ‚Üí\", v(\"pandas\"))\n",
        "print(\"openai     ‚Üí\", v(\"openai\"))\n",
        "print(\"nest-asyncio‚Üí\", v(\"nest-asyncio\"))\n",
        "print(\"pyngrok    ‚Üí\", v(\"pyngrok\"))\n",
        "print(\"cloudflared‚Üí\", v(\"cloudflared\"))\n",
        "print(\"mcp        ‚Üí\", v(\"mcp\"))\n",
        "print(\"mcp-server-fetch    ‚Üí\", v(\"mcp-server-fetch\"))\n",
        "print(\"mcp-server-filesystem‚Üí\", v(\"mcp-server-filesystem\"))\n",
        "print(\"ollama bin :\", shutil.which(\"ollama\"))\n",
        "print(\"fetch bin  :\", shutil.which(\"mcp-server-fetch\"))\n",
        "print(\"fs bin     :\", shutil.which(\"mcp-server-filesystem\"))\n",
        "\n",
        "# 6) Garde-fous clairs\n",
        "assert _streamlit_ok, \"Streamlit n'est pas importable. Relance cette cellule; si le probl√®me persiste, red√©marre le runtime.\"\n",
        "os.environ.setdefault(\"OLLAMA_MODEL\", \"llama3.2:1b\")  # mod√®le l√©ger par d√©faut\n",
        "print(\"\\nüéâ PARTIE 0 OK ‚Äî tu peux continuer aux parties suivantes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 1 ‚Äì D√©marrer le serveur Ollama en arri√®re-plan"
      ],
      "metadata": {
        "id": "fR_f2tJcG11n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß© PARTIE 1-bis ‚Äî Restart Ollama + wait + pull mod√®le\n",
        "import subprocess, time, socket, os, json, urllib.request\n",
        "\n",
        "MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.2:1b\")\n",
        "\n",
        "# 1) stop √©ventuels processus\n",
        "subprocess.run([\"pkill\", \"-f\", \"ollama\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(2)\n",
        "\n",
        "# 2) start ollama serve\n",
        "ollama_proc = subprocess.Popen([\"ollama\", \"serve\"],\n",
        "                               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# 3) wait port\n",
        "def wait_for_port(host=\"127.0.0.1\", port=11434, timeout=90):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            with socket.create_connection((host, port), timeout=2):\n",
        "                return True\n",
        "        except OSError:\n",
        "            time.sleep(1)\n",
        "    return False\n",
        "\n",
        "if not wait_for_port():\n",
        "    raise RuntimeError(\"‚ùå Ollama ne r√©pond pas sur 127.0.0.1:11434. Installe/d√©marre 'ollama serve'.\")\n",
        "\n",
        "# 4) pull du mod√®le si absent\n",
        "def have_model(name: str) -> bool:\n",
        "    try:\n",
        "        with urllib.request.urlopen(\"http://127.0.0.1:11434/api/tags\", timeout=5) as r:\n",
        "            if r.status == 200:\n",
        "                tags = json.loads(r.read().decode(\"utf-8\"))\n",
        "                return any(m.get(\"name\")==name for m in tags.get(\"models\",[]))\n",
        "    except Exception:\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "if not have_model(MODEL):\n",
        "    print(f\"‚¨áÔ∏è Pull mod√®le {MODEL}‚Ä¶\")\n",
        "    code = subprocess.call([\"ollama\", \"pull\", MODEL])\n",
        "    if code != 0 or not have_model(MODEL):\n",
        "        raise RuntimeError(f\"‚ùå √âchec du pull du mod√®le {MODEL}. Essaie un mod√®le plus l√©ger (ex: llama3.2:1b).\")\n",
        "\n",
        "print(f\"‚úÖ Ollama OK, mod√®le pr√™t: {MODEL}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROSHaucnG1t8",
        "outputId": "71ee9794-abc8-4855-fcbd-fab15d46871b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ollama OK, mod√®le pr√™t: llama3.2:1b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 2 ‚Äì Cr√©ation du serveur MCP perso (dossiers)"
      ],
      "metadata": {
        "id": "7yAQNj7DG6SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2Ô∏è‚É£ Cr√©er les dossiers n√©cessaires\n",
        "import os, pathlib\n",
        "\n",
        "MCP_DEMO_DIR = \"/tmp/mcp_demo\"\n",
        "ALLOWED_FS_DIR = f\"{MCP_DEMO_DIR}/allowed\"\n",
        "\n",
        "pathlib.Path(MCP_DEMO_DIR).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(ALLOWED_FS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Dossiers cr√©√©s: {MCP_DEMO_DIR}, {ALLOWED_FS_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCtNmdusG6IO",
        "outputId": "25ab3045-2954-4d04-ae20-b34740cab9fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dossiers cr√©√©s: /tmp/mcp_demo, /tmp/mcp_demo/allowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 3 ‚Äì Serveur MCP personnalis√© (fix import pandas)"
      ],
      "metadata": {
        "id": "5UGOnU36G-ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3Ô∏è‚É£ Cr√©ation du serveur MCP personnalis√©\n",
        "%%writefile /tmp/mcp_demo/my_mcp_server.py\n",
        "# /tmp/mcp_demo/app.py\n",
        "# ------------------------------------------------------------\n",
        "# Streamlit UI pour MCP + Ollama (Llama 3.1) ‚Äî robuste + fallback + logs\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import asyncio\n",
        "import pathlib\n",
        "import textwrap\n",
        "import importlib.util\n",
        "\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# O√π un √©ventuel lanceur externe redirige les logs Streamlit (voir ta Partie 10)\n",
        "LOG_PATH = \"/tmp/mcp_demo/streamlit.log\"\n",
        "\n",
        "# ============================================================\n",
        "# MCP helper (stdio JSON-RPC)\n",
        "# ============================================================\n",
        "class MCPServer:\n",
        "    def __init__(self, name: str, cmd: list[str]):\n",
        "        self.name = name\n",
        "        self.cmd = cmd\n",
        "        self.proc: asyncio.subprocess.Process | None = None\n",
        "\n",
        "    async def start(self):\n",
        "        # Lance le processus serveur MCP (stdio)\n",
        "        self.proc = await asyncio.create_subprocess_exec(\n",
        "            *self.cmd,\n",
        "            stdin=asyncio.subprocess.PIPE,\n",
        "            stdout=asyncio.subprocess.PIPE,\n",
        "            stderr=asyncio.subprocess.PIPE,\n",
        "        )\n",
        "\n",
        "        # Envoie \"initialize\"\n",
        "        init_msg = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 1,\n",
        "            \"method\": \"initialize\",\n",
        "            \"params\": {\n",
        "                \"protocolVersion\": \"2024-11-05\",\n",
        "                \"capabilities\": {},\n",
        "                \"clientInfo\": {\"name\": \"streamlit-app\"},\n",
        "            },\n",
        "        }\n",
        "        assert self.proc.stdin is not None\n",
        "        self.proc.stdin.write((json.dumps(init_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        await asyncio.sleep(0.8)  # petit d√©lai pour laisser d√©marrer\n",
        "\n",
        "    async def list_tools(self):\n",
        "        if not self.proc or not self.proc.stdin or not self.proc.stdout:\n",
        "            raise RuntimeError(f\"{self.name}: process not started\")\n",
        "\n",
        "        list_msg = {\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/list\"}\n",
        "        self.proc.stdin.write((json.dumps(list_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if not line:\n",
        "            # Essaie de lire stderr pour donner un indice\n",
        "            stderr_tail = \"\"\n",
        "            try:\n",
        "                if self.proc.stderr:\n",
        "                    stderr_tail = (await self.proc.stderr.read(4096)).decode(errors=\"ignore\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            raise RuntimeError(f\"{self.name}: no response to tools/list. Stderr: {stderr_tail[:500]}\")\n",
        "\n",
        "        data = json.loads(line.decode().strip())\n",
        "        return data.get(\"result\", {}).get(\"tools\", [])\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: dict):\n",
        "        if not self.proc or not self.proc.stdin or not self.proc.stdout:\n",
        "            raise RuntimeError(f\"{self.name}: process not started\")\n",
        "\n",
        "        call_msg = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 3,\n",
        "            \"method\": \"tools/call\",\n",
        "            \"params\": {\"name\": tool_name, \"arguments\": arguments},\n",
        "        }\n",
        "        self.proc.stdin.write((json.dumps(call_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if line:\n",
        "            return json.loads(line.decode().strip())\n",
        "        return {\"error\": \"no output from tools/call\"}\n",
        "\n",
        "    async def shutdown(self):\n",
        "        # Arr√™t doux ‚Üí puis kill si n√©cessaire\n",
        "        if self.proc:\n",
        "            try:\n",
        "                self.proc.terminate()\n",
        "                await asyncio.sleep(0.5)\n",
        "                if self.proc.returncode is None:\n",
        "                    self.proc.kill()\n",
        "                await self.proc.wait()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Boot MCP servers (robuste: binaire OU module) + timeouts\n",
        "# ============================================================\n",
        "ALLOWED_FS_DIR = \"/tmp/mcp_demo/allowed\"\n",
        "pathlib.Path(ALLOWED_FS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _has_module(mod: str) -> bool:\n",
        "    return importlib.util.find_spec(mod) is not None\n",
        "\n",
        "def _bin_or_module(bin_name: str, module_name: str, extra_args: list[str] | None = None) -> list[str]:\n",
        "    \"\"\"\n",
        "    Pr√©f√®re le binaire s'il est pr√©sent, sinon lance `python -m module`.\n",
        "    \"\"\"\n",
        "    path = shutil.which(bin_name)\n",
        "    if path:\n",
        "        return [bin_name] + (extra_args or [])\n",
        "    if _has_module(module_name):\n",
        "        return [sys.executable, \"-m\", module_name] + (extra_args or [])\n",
        "    raise RuntimeError(\n",
        "        f\"Ni binaire '{bin_name}' ni module '{module_name}' trouv√©.\\n\"\n",
        "        \"‚Üí Installe dans ce runtime: pip install -q mcp-server-fetch mcp-server-filesystem mcp\"\n",
        "    )\n",
        "\n",
        "# Commandes robustes (binaire ‚Üí module)\n",
        "fetch_cmd = _bin_or_module(\"mcp-server-fetch\", \"mcp_server_fetch\")\n",
        "fs_cmd    = _bin_or_module(\"mcp-server-filesystem\", \"mcp_server_filesystem\", [ALLOWED_FS_DIR])\n",
        "\n",
        "# Custom server Python (fichier local)\n",
        "MY_MCP_PATH = \"/tmp/mcp_demo/my_mcp_server.py\"\n",
        "my_cmd = [sys.executable, MY_MCP_PATH] if os.path.exists(MY_MCP_PATH) else [sys.executable, \"-m\", \"runpy\", MY_MCP_PATH]\n",
        "\n",
        "# Instances serveurs\n",
        "fetch_srv = MCPServer(\"fetch\", fetch_cmd)\n",
        "fs_srv    = MCPServer(\"filesystem\", fs_cmd)\n",
        "my_srv    = MCPServer(\"my_mcp\", my_cmd)\n",
        "\n",
        "# Helper: appelle tools/list avec timeout pour √©viter de bloquer l'UI\n",
        "async def _list_tools_with_timeout(server: MCPServer, name: str, timeout: float = 6.0):\n",
        "    try:\n",
        "        return await asyncio.wait_for(server.list_tools(), timeout=timeout)\n",
        "    except asyncio.TimeoutError:\n",
        "        return {\"__error__\": f\"{name}: timeout on tools/list (>{timeout}s)\"}\n",
        "    except Exception as e:\n",
        "        return {\"__error__\": f\"{name}: {type(e).__name__}: {e}\"}\n",
        "\n",
        "# D√©marrage + collecte des outils (robuste)\n",
        "async def start_all():\n",
        "    # 1) D√©marre les 3 serveurs en parall√®le\n",
        "    try:\n",
        "        await asyncio.gather(fetch_srv.start(), fs_srv.start(), my_srv.start())\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"√âchec au d√©marrage d'un serveur MCP: {e}\")\n",
        "\n",
        "    # 2) R√©cup√®re les outils avec timeouts pour ne pas bloquer\n",
        "    fetch_tools = await _list_tools_with_timeout(fetch_srv, \"fetch\")\n",
        "    fs_tools    = await _list_tools_with_timeout(fs_srv, \"filesystem\")\n",
        "    my_tools    = await _list_tools_with_timeout(my_srv, \"my_mcp\")\n",
        "\n",
        "    # 3) Aggr√®ge les erreurs √©ventuelles\n",
        "    errors = []\n",
        "    for name, tools in ((\"fetch\", fetch_tools), (\"filesystem\", fs_tools), (\"my_mcp\", my_tools)):\n",
        "        if isinstance(tools, dict) and \"__error__\" in tools:\n",
        "            errors.append(tools[\"__error__\"])\n",
        "\n",
        "    if errors:\n",
        "        # On stoppe ici pour afficher l‚Äôerreur dans l‚ÄôUI (pas de spinner infini)\n",
        "        raise RuntimeError(\" / \".join(errors))\n",
        "\n",
        "    # 4) Construit la liste et la map des outils\n",
        "    all_tools = fetch_tools + fs_tools + my_tools\n",
        "    tool_map = {\n",
        "        \"fetch\": {t[\"name\"] for t in fetch_tools},\n",
        "        \"fs\":    {t[\"name\"] for t in fs_tools},\n",
        "        \"my\":    {t[\"name\"] for t in my_tools},\n",
        "    }\n",
        "\n",
        "    if not all_tools:\n",
        "        raise RuntimeError(\"Aucun outil MCP disponible (fetch/filesystem/my_mcp ont renvoy√© 0 outil).\")\n",
        "\n",
        "    return all_tools, tool_map\n",
        "\n",
        "# Arr√™t propre (accessible depuis l‚ÄôUI)\n",
        "async def stop_all():\n",
        "    await asyncio.gather(fetch_srv.shutdown(), fs_srv.shutdown(), my_srv.shutdown())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# LLM & Orchestrateur (Ollama via OpenAI client-compatible)\n",
        "# ============================================================\n",
        "OLLAMA_BASE = os.environ.get(\"OLLAMA_ENDPOINT\", \"http://localhost:11434/v1\")\n",
        "MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.1\")\n",
        "client = OpenAI(base_url=OLLAMA_BASE, api_key=os.environ.get(\"OLLAMA_API_KEY\", \"ollama\"))\n",
        "\n",
        "async def run_agent(goal: str, all_tools: list[dict], tool_map: dict[str, set[str]]):\n",
        "    SYSTEM_PROMPT = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "        You are an AI assistant with access to tools. You must use JSON format for all responses.\n",
        "\n",
        "        Available tools:\n",
        "        {chr(10).join([f\"- {t['name']}: {t.get('description', 'No description')}\" for t in all_tools])}\n",
        "\n",
        "        Response format:\n",
        "        - To use a tool: {{\"tool\": \"tool_name\", \"arguments\": {{\"arg1\": \"value1\", \"arg2\": \"value2\"}}}}\n",
        "        - To give final answer: {{\"answer\": \"your final answer\"}}\n",
        "\n",
        "        Always think step by step and use the appropriate tools.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": goal},\n",
        "    ]\n",
        "\n",
        "    outputs: list[str] = []\n",
        "    for step in range(8):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL, messages=messages, temperature=0.1, max_tokens=1000\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "\n",
        "            # Tente de parser en JSON\n",
        "            try:\n",
        "                decision = json.loads(content)\n",
        "            except json.JSONDecodeError:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": \"Please respond with valid JSON format only.\"})\n",
        "                outputs.append(f\"‚ö†Ô∏è Invalid JSON response: {content}\")\n",
        "                continue\n",
        "\n",
        "            if \"answer\" in decision:\n",
        "                outputs.append(f\"‚úÖ {decision['answer']}\")\n",
        "                break\n",
        "\n",
        "            if \"tool\" in decision and \"arguments\" in decision:\n",
        "                tool_name = decision[\"tool\"]\n",
        "                arguments = decision[\"arguments\"]\n",
        "\n",
        "                server = None\n",
        "                if tool_name in tool_map[\"fetch\"]:\n",
        "                    server = fetch_srv\n",
        "                elif tool_name in tool_map[\"fs\"]:\n",
        "                    server = fs_srv\n",
        "                elif tool_name in tool_map[\"my\"]:\n",
        "                    server = my_srv\n",
        "\n",
        "                if server:\n",
        "                    result = await server.call_tool(tool_name, arguments)\n",
        "                    observation = str(result)\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
        "                    outputs.append(\n",
        "                        json.dumps(\n",
        "                            {\"step\": step + 1, \"tool\": tool_name, \"arguments\": arguments, \"result\": observation},\n",
        "                            indent=2,\n",
        "                        )\n",
        "                    )\n",
        "                else:\n",
        "                    err = f\"Tool '{tool_name}' not found\"\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Error: {err}\"})\n",
        "                    outputs.append(f\"‚ùå {err}\")\n",
        "            else:\n",
        "                err = \"Invalid response format - must contain either 'answer' or 'tool' with 'arguments'\"\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": err})\n",
        "                outputs.append(f\"‚ùå {err}\")\n",
        "        except Exception as e:\n",
        "            outputs.append(f\"‚ùå Error: {e}\")\n",
        "            break\n",
        "    else:\n",
        "        outputs.append(\"‚ùå Maximum steps reached without completing the task.\")\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Streamlit UI\n",
        "# ============================================================\n",
        "st.set_page_config(page_title=\"ü§ñ MCP Agent avec Ollama\", layout=\"wide\", initial_sidebar_state=\"expanded\")\n",
        "st.title(\"ü§ñ MCP Agent avec Ollama\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "Cette d√©mo utilise:\n",
        "- **Ollama** avec Llama 3.1\n",
        "- **Serveurs MCP** (fetch, filesystem, custom)\n",
        "- **Streamlit** pour l'interface\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Boot (avec erreurs visibles)\n",
        "if \"boot_done\" not in st.session_state:\n",
        "    try:\n",
        "        with st.spinner(\"D√©marrage des serveurs MCP‚Ä¶\"):\n",
        "            all_tools, tool_map = asyncio.run(start_all())\n",
        "        st.session_state.boot_done = True\n",
        "        st.session_state.all_tools = all_tools\n",
        "        st.session_state.tool_map = tool_map\n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå √âchec du d√©marrage des serveurs MCP : {e}\")\n",
        "        st.info(\n",
        "            \"V√©rifie que les paquets sont install√©s (`mcp-server-fetch`, `mcp-server-filesystem`, `mcp`), \"\n",
        "            \"que `/tmp/mcp_demo/allowed` existe, et que `my_mcp_server.py` est accessible.\"\n",
        "        )\n",
        "        st.stop()\n",
        "\n",
        "with st.expander(\"‚öôÔ∏è Configuration\"):\n",
        "    st.write(\"**Serveurs MCP actifs (process lanc√©s):**\")\n",
        "    st.write(f\"- üì° Fetch Server: ‚úÖ\")\n",
        "    st.write(f\"- üìÅ Filesystem Server: ‚úÖ\")\n",
        "    st.write(f\"- üõ†Ô∏è Custom Server: ‚úÖ\")\n",
        "    st.write(\"**Outils disponibles:**\")\n",
        "    for tool in st.session_state.all_tools:\n",
        "        st.write(f\"- `{tool['name']}`: {tool.get('description', 'No description')}\")\n",
        "\n",
        "# Zone de saisie & presets\n",
        "goal = st.text_area(\n",
        "    \"üéØ Objectif (en anglais pour de meilleurs r√©sultats):\",\n",
        "    height=120,\n",
        "    placeholder=\"Ex: Download https://example.com/data.csv, convert to JSON, summarize, and save to summary.txt\",\n",
        ")\n",
        "\n",
        "c1, c2, c3 = st.columns(3)\n",
        "with c1:\n",
        "    if st.button(\"üì• T√©l√©charger et convertir\"):\n",
        "        goal = \"Download https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv, convert to JSON, and show first 3 records\"\n",
        "with c2:\n",
        "    if st.button(\"üìù R√©sumer du texte\"):\n",
        "        goal = (\n",
        "            \"Summarize this text: 'Artificial intelligence is transforming many industries. \"\n",
        "            \"Machine learning algorithms can now recognize patterns in data that humans might miss. \"\n",
        "            \"This technology is being used in healthcare, finance, and transportation. \"\n",
        "            \"The future of AI looks promising with continued advancements.'\"\n",
        "        )\n",
        "with c3:\n",
        "    if st.button(\"üìÅ Lister les fichiers\"):\n",
        "        goal = \"List files in the current directory and show their contents\"\n",
        "\n",
        "# Ex√©cution agent\n",
        "if st.button(\"‚ñ∂Ô∏è Ex√©cuter\", type=\"primary\"):\n",
        "    if not goal.strip():\n",
        "        st.warning(\"Veuillez entrer un objectif\")\n",
        "    else:\n",
        "        progress = st.progress(0)\n",
        "        status = st.empty()\n",
        "        out = st.empty()\n",
        "        with st.spinner(\"Ex√©cution‚Ä¶\"):\n",
        "            outputs = asyncio.run(run_agent(goal, st.session_state.all_tools, st.session_state.tool_map))\n",
        "        lines: list[str] = []\n",
        "        steps = max(1, len(outputs))\n",
        "        for i, item in enumerate(outputs):\n",
        "            lines.append(item)\n",
        "            out.code(\"\\n\".join(lines), language=\"json\")\n",
        "            progress.progress(min((i + 1) / steps, 1.0))\n",
        "            status.text(f\"Step {i + 1}/{steps}\")\n",
        "        status.success(\"‚úÖ T√¢che termin√©e!\")\n",
        "\n",
        "# Sidebar: outils de debug & nettoyage\n",
        "st.sidebar.markdown(\"### ü™µ Logs & Nettoyage\")\n",
        "\n",
        "# Bouton \"Voir les logs\" ‚Äî affiche streamlit.log si pr√©sent\n",
        "if st.sidebar.button(\"Voir les logs\"):\n",
        "    if os.path.exists(LOG_PATH):\n",
        "        try:\n",
        "            with open(LOG_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                content = f.read()\n",
        "            st.sidebar.success(f\"Ouverture de {LOG_PATH}\")\n",
        "            # On affiche dans la zone principale pour avoir plus d'espace\n",
        "            st.subheader(\"ü™µ Contenu de streamlit.log\")\n",
        "            st.code(content[-20000:] or \"(log vide)\", language=\"bash\")\n",
        "        except Exception as e:\n",
        "            st.sidebar.error(f\"Impossible de lire {LOG_PATH} : {e}\")\n",
        "    else:\n",
        "        st.sidebar.warning(f\"Fichier de log introuvable : {LOG_PATH}\\n\"\n",
        "                           \"‚û°Ô∏è Lance l'app via une cellule qui redirige stdout/err vers ce fichier.\")\n",
        "\n",
        "if st.sidebar.button(\"Arr√™ter les serveurs MCP\"):\n",
        "    asyncio.run(stop_all())\n",
        "    st.sidebar.success(\"Serveurs arr√™t√©s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJn3Dq6FFSCH",
        "outputId": "ced82e84-45f4-45b8-aa4c-4bb796ed5f62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/mcp_demo/my_mcp_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 4 ‚Äì Clone des serveurs tiers (dossiers & fichier exemple)"
      ],
      "metadata": {
        "id": "cIRAOIrSHCGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''#@title 4Ô∏è‚É£ Pr√©paration des dossiers\n",
        "import pathlib\n",
        "BASE = pathlib.Path(\"/tmp/mcp_demo\")\n",
        "BASE.mkdir(exist_ok=True)\n",
        "ALLOWED_FS = BASE / \"allowed\"\n",
        "ALLOWED_FS.mkdir(exist_ok=True)\n",
        "\n",
        "with open(ALLOWED_FS / \"example.txt\", \"w\") as f:\n",
        "    f.write(\"Ceci est un fichier d'exemple\\nLigne 2\\nLigne 3\")\n",
        "print(\"‚úÖ example.txt cr√©√©\")'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B6YenbJyHB-x",
        "outputId": "35252e6b-9ad6-4174-9d04-90a814ce72a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#@title 4Ô∏è‚É£ Pr√©paration des dossiers\\nimport pathlib\\nBASE = pathlib.Path(\"/tmp/mcp_demo\")\\nBASE.mkdir(exist_ok=True)\\nALLOWED_FS = BASE / \"allowed\"\\nALLOWED_FS.mkdir(exist_ok=True)\\n\\nwith open(ALLOWED_FS / \"example.txt\", \"w\") as f:\\n    f.write(\"Ceci est un fichier d\\'exemple\\nLigne 2\\nLigne 3\")\\nprint(\"‚úÖ example.txt cr√©√©\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 5 ‚Äì Helper MCPServer (inchang√© + robuste)"
      ],
      "metadata": {
        "id": "GljcyWw4HGg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß© PARTIE 5 ‚Äî Classe helper pour les serveurs MCP (corrig√©e)\n",
        "# Lit et \"√©carte\" la r√©ponse √† `initialize` avant d'envoyer d'autres requ√™tes,\n",
        "# pour √©viter que tools/list r√©cup√®re la mauvaise ligne.\n",
        "\n",
        "import asyncio, json, sys\n",
        "from typing import Callable, Any, Dict, Optional\n",
        "\n",
        "class MCPServer:\n",
        "    def __init__(self, name: str, cmd: list[str]):\n",
        "        self.name = name\n",
        "        self.cmd = cmd\n",
        "        self.proc: Optional[asyncio.subprocess.Process] = None\n",
        "        self._msg_id = 0\n",
        "\n",
        "    # --- utilitaires internes ---\n",
        "    def _next_id(self) -> int:\n",
        "        self._msg_id += 1\n",
        "        return self._msg_id\n",
        "\n",
        "    async def _write(self, payload: Dict[str, Any]) -> None:\n",
        "        \"\"\"√âcrit une ligne JSON sur stdin du serveur MCP.\"\"\"\n",
        "        if not self.proc or not self.proc.stdin:\n",
        "            raise RuntimeError(f\"[{self.name}] Process not started\")\n",
        "        line = json.dumps(payload, ensure_ascii=False) + \"\\n\"\n",
        "        self.proc.stdin.write(line.encode(\"utf-8\"))\n",
        "        await self.proc.stdin.drain()\n",
        "\n",
        "    async def _read_until(\n",
        "        self,\n",
        "        predicate: Callable[[Dict[str, Any]], bool],\n",
        "        timeout: float = 15.0,\n",
        "        max_lines: int = 200,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Lit des lignes JSON depuis stdout jusqu'√† ce que `predicate(obj)` soit True.\n",
        "        Ignore silencieusement les lignes non-JSON (logs). Timeout s√©curis√©.\n",
        "        \"\"\"\n",
        "        if not self.proc or not self.proc.stdout:\n",
        "            raise RuntimeError(f\"[{self.name}] Process not started\")\n",
        "\n",
        "        lines = 0\n",
        "        while lines < max_lines:\n",
        "            try:\n",
        "                raw = await asyncio.wait_for(self.proc.stdout.readline(), timeout=timeout)\n",
        "            except asyncio.TimeoutError:\n",
        "                raise asyncio.TimeoutError(f\"[{self.name}] Timeout lecture (>{timeout}s)\")\n",
        "            if not raw:\n",
        "                raise RuntimeError(f\"[{self.name}] EOF sur stdout (process arr√™t√© ?)\")\n",
        "            text = raw.decode(\"utf-8\", errors=\"replace\").strip()\n",
        "            lines += 1\n",
        "            try:\n",
        "                obj = json.loads(text)\n",
        "            except json.JSONDecodeError:\n",
        "                # Certains serveurs √©crivent des logs ‚Üí on ignore ce qui n'est pas JSON\n",
        "                continue\n",
        "            if predicate(obj):\n",
        "                return obj\n",
        "        raise RuntimeError(f\"[{self.name}] Trop de lignes sans match (>{max_lines})\")\n",
        "\n",
        "    # --- cycle de vie ---\n",
        "    async def start(self) -> None:\n",
        "        \"\"\"D√©marre le processus MCP et traite la s√©quence initialize ‚Üí r√©ponse.\"\"\"\n",
        "        print(f\"[{self.name}] D√©marrage ‚Üí {' '.join(self.cmd)}\")\n",
        "        self.proc = await asyncio.create_subprocess_exec(\n",
        "            *self.cmd,\n",
        "            stdin=asyncio.subprocess.PIPE,\n",
        "            stdout=asyncio.subprocess.PIPE,\n",
        "            stderr=asyncio.subprocess.PIPE,  # on ne lit pas stderr ici (logs)\n",
        "        )\n",
        "\n",
        "        # Envoie initialize\n",
        "        init_id = self._next_id()\n",
        "        init_msg = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": init_id,\n",
        "            \"method\": \"initialize\",\n",
        "            \"params\": {\n",
        "                \"protocolVersion\": \"2024-11-05\",\n",
        "                \"capabilities\": {},\n",
        "                \"clientInfo\": {\"name\": \"colab-client\"},\n",
        "            },\n",
        "        }\n",
        "        await self._write(init_msg)\n",
        "\n",
        "        # Lit (et \"√©carte\") la r√©ponse d'init AVANT toute autre requ√™te\n",
        "        def _is_init(resp: Dict[str, Any]) -> bool:\n",
        "            return resp.get(\"id\") == init_id and \"result\" in resp\n",
        "\n",
        "        await self._read_until(_is_init, timeout=15.0)\n",
        "        print(f\"[{self.name}] ‚úÖ Initialis√©\")\n",
        "\n",
        "    async def shutdown(self) -> None:\n",
        "        \"\"\"Arr√™te proprement le processus MCP.\"\"\"\n",
        "        if self.proc:\n",
        "            try:\n",
        "                self.proc.terminate()\n",
        "                await asyncio.sleep(0.7)\n",
        "                if self.proc.returncode is None:\n",
        "                    self.proc.kill()\n",
        "                await self.proc.wait()\n",
        "            except Exception as e:\n",
        "                print(f\"[{self.name}] Erreur arr√™t: {e}\", file=sys.stderr)\n",
        "\n",
        "    # --- appels JSON-RPC ---\n",
        "    async def list_tools(self) -> list[Dict[str, Any]]:\n",
        "        \"\"\"Appelle tools/list et renvoie la liste des outils.\"\"\"\n",
        "        if not self.proc:\n",
        "            raise RuntimeError(f\"[{self.name}] Process not started\")\n",
        "\n",
        "        req_id = self._next_id()\n",
        "        req = {\"jsonrpc\": \"2.0\", \"id\": req_id, \"method\": \"tools/list\"}\n",
        "        await self._write(req)\n",
        "\n",
        "        def _is_tools(resp: Dict[str, Any]) -> bool:\n",
        "            # On privil√©gie le match par id, mais on tol√®re les serveurs qui ne renvoient pas l'id.\n",
        "            if resp.get(\"id\") == req_id and \"result\" in resp:\n",
        "                tools = resp[\"result\"].get(\"tools\")\n",
        "                return isinstance(tools, list)\n",
        "            if \"result\" in resp and isinstance(resp[\"result\"].get(\"tools\"), list):\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        resp = await self._read_until(_is_tools, timeout=15.0)\n",
        "        return resp.get(\"result\", {}).get(\"tools\", []) or []\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Appelle tools/call avec {name, arguments} et renvoie la r√©ponse brute (result/error).\"\"\"\n",
        "        if not self.proc:\n",
        "            return {\"error\": \"Process not started\"}\n",
        "\n",
        "        req_id = self._next_id()\n",
        "        req = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": req_id,\n",
        "            \"method\": \"tools/call\",\n",
        "            \"params\": {\"name\": tool_name, \"arguments\": arguments},\n",
        "        }\n",
        "        await self._write(req)\n",
        "\n",
        "        def _is_call(resp: Dict[str, Any]) -> bool:\n",
        "            return resp.get(\"id\") == req_id and (\"result\" in resp or \"error\" in resp)\n",
        "\n",
        "        try:\n",
        "            resp = await self._read_until(_is_call, timeout=35.0)\n",
        "        except asyncio.TimeoutError:\n",
        "            return {\"error\": f\"Timeout calling tool {tool_name}\"}\n",
        "        return resp\n"
      ],
      "metadata": {
        "id": "ApZi1zm3HGY7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 6 ‚Äì D√©marrage des serveurs MCP + m√©morisation des outils"
      ],
      "metadata": {
        "id": "y7oto4K8HKI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üßπ PARTIE 6-fix ‚Äî Purge de mcp-server-fetch pour forcer le fallback\n",
        "import sys, os, shutil, subprocess\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"mcp-server-fetch\"], check=False)\n",
        "bin_path = shutil.which(\"mcp-server-fetch\")\n",
        "if bin_path:\n",
        "    try:\n",
        "        os.remove(bin_path)\n",
        "        print(\"üóëÔ∏è  Supprim√©:\", bin_path)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è  Impossible de supprimer\", bin_path, \"‚Üí\", e)\n",
        "else:\n",
        "    print(\"‚úÖ Aucun binaire mcp-server-fetch actif dans le PATH.\")\n",
        "os.environ[\"MCP_FORCE_FETCH_FALLBACK\"] = \"1\"\n",
        "print(\"‚úÖ Purge termin√©e. Relance maintenant la PARTIE 6 v3.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aelDXYhDhCNd",
        "outputId": "1c01bd88-4194-4c2f-ae77-62f659cd56e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Aucun binaire mcp-server-fetch actif dans le PATH.\n",
            "‚úÖ Purge termin√©e. Relance maintenant la PARTIE 6 v3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß© PARTIE 6 v3 ‚Äî D√©marrage MCP robuste (timeouts stricts + fallbacks fetch/fs/my)\n",
        "# N√©cessite la PARTIE 5 corrig√©e (classe MCPServer qui lit la r√©ponse initialize)\n",
        "\n",
        "import sys, os, asyncio, importlib.util, pathlib, subprocess, json, textwrap\n",
        "\n",
        "# --- pr√©requis: PARTIE 5 doit avoir d√©fini MCPServer ---\n",
        "if \"MCPServer\" not in globals():\n",
        "    raise RuntimeError(\"La PARTIE 5 (MCPServer corrig√©e) doit √™tre ex√©cut√©e avant la PARTIE 6 v3.\")\n",
        "\n",
        "# --- constantes & chemins ---\n",
        "BASE_DIR = \"/tmp/mcp_demo\"\n",
        "ALLOWED_FS_DIR = f\"{BASE_DIR}/allowed\"\n",
        "pathlib.Path(ALLOWED_FS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FS_FALLBACK_PATH   = f\"{BASE_DIR}/fs_mcp_server.py\"\n",
        "FETCH_FALLBACK_PATH= f\"{BASE_DIR}/fetch_mcp_server.py\"\n",
        "MY_FALLBACK_PATH   = f\"{BASE_DIR}/my_mcp_fallback.py\"\n",
        "MY_MCP_PATH        = f\"{BASE_DIR}/my_mcp_server.py\"  # de la PARTIE 3\n",
        "\n",
        "# --- timeouts stricts ---\n",
        "START_TIMEOUT_S = float(os.environ.get(\"MCP_START_TIMEOUT_S\", \"6.0\"))\n",
        "LIST_TIMEOUT_S  = float(os.environ.get(\"MCP_LIST_TIMEOUT_S\",  \"5.0\"))\n",
        "\n",
        "# --- utils module / install ---\n",
        "def _has_module(mod: str) -> bool:\n",
        "    return importlib.util.find_spec(mod) is not None\n",
        "\n",
        "def _pip_install(pkg: str) -> bool:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", pkg])\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è pip install √©chou√© pour {pkg}: {e}\")\n",
        "        return False\n",
        "\n",
        "def _ensure_module(mod: str, pip_name: str | None = None) -> bool:\n",
        "    if _has_module(mod):\n",
        "        return True\n",
        "    if pip_name and _pip_install(pip_name):\n",
        "        return _has_module(mod)\n",
        "    return False\n",
        "\n",
        "# --- √©crire fallbacks si besoin ---\n",
        "def _ensure_fs_fallback():\n",
        "    if os.path.exists(FS_FALLBACK_PATH): return\n",
        "    code = r'''\n",
        "#!/usr/bin/env python3\n",
        "import asyncio, json, sys, os, pathlib\n",
        "from typing import Any, Dict\n",
        "\n",
        "ROOT = os.path.abspath(sys.argv[1]) if len(sys.argv) > 1 else os.getcwd()\n",
        "pathlib.Path(ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def safe_join(root: str, rel: str) -> str:\n",
        "    p = os.path.abspath(os.path.join(root, rel))\n",
        "    if not p.startswith(root): raise ValueError(\"Path outside allowed directory\")\n",
        "    return p\n",
        "\n",
        "class FsMCP:\n",
        "    def __init__(self, root: str):\n",
        "        self.root = root\n",
        "        self.tools = [\n",
        "            {\"name\":\"list_dir\",\"description\":\"List files/dirs\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"path\":{\"type\":\"string\"}},\"required\":[]}},\n",
        "            {\"name\":\"read_file\",\"description\":\"Read text file\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"path\":{\"type\":\"string\"}},\"required\":[\"path\"]}},\n",
        "            {\"name\":\"write_file\",\"description\":\"Write text file\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"path\":{\"type\":\"string\"},\"content\":{\"type\":\"string\"}},\"required\":[\"path\",\"content\"]}},\n",
        "        ]\n",
        "\n",
        "    async def handle(self, msg: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        mid = msg.get(\"id\"); method = msg.get(\"method\"); params = msg.get(\"params\", {})\n",
        "        if method == \"initialize\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"tools\":{}},\n",
        "                    \"serverInfo\":{\"name\":\"fs_fallback\",\"version\":\"0.1.0\"}}}\n",
        "        if method == \"tools/list\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"tools\": self.tools}}\n",
        "        if method == \"tools/call\":\n",
        "            name = params.get(\"name\"); args = params.get(\"arguments\", {})\n",
        "            try:\n",
        "                out = await self.call(name, args)\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"content\":[{\"type\":\"text\",\"text\":out}]}}\n",
        "            except Exception as e:\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32603,\"message\":str(e)}}\n",
        "        return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32601,\"message\":f\"Unknown method {method}\"}}\n",
        "\n",
        "    async def call(self, name: str, args: Dict[str, Any]) -> str:\n",
        "        if name == \"list_dir\":\n",
        "            rel = args.get(\"path\",\".\"); p = safe_join(self.root, rel)\n",
        "            if not os.path.isdir(p): return f\"Not a directory: {rel}\"\n",
        "            items = []\n",
        "            for e in sorted(os.listdir(p)):\n",
        "                t = \"dir\" if os.path.isdir(os.path.join(p,e)) else \"file\"\n",
        "                items.append(f\"{e} ({t})\")\n",
        "            return \"Directory listing for \"+rel+\":\\n\"+\"\\n\".join(items)\n",
        "        if name == \"read_file\":\n",
        "            rel = args[\"path\"]; p = safe_join(self.root, rel)\n",
        "            if not os.path.exists(p): return f\"File not found: {rel}\"\n",
        "            if os.path.isdir(p): return f\"Is a directory: {rel}\"\n",
        "            with open(p,\"r\",encoding=\"utf-8\",errors=\"replace\") as f: data=f.read()\n",
        "            return f\"=== {rel} ===\\n{data}\"\n",
        "        if name == \"write_file\":\n",
        "            rel = args[\"path\"]; content = args[\"content\"]\n",
        "            p = safe_join(self.root, rel); os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "            with open(p,\"w\",encoding=\"utf-8\") as f: f.write(content)\n",
        "            return f\"Wrote {len(content)} bytes to {rel}\"\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "async def main():\n",
        "    srv = FsMCP(ROOT)\n",
        "    loop = asyncio.get_event_loop()\n",
        "    while True:\n",
        "        line = await loop.run_in_executor(None, sys.stdin.readline)\n",
        "        if not line: break\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try: msg = json.loads(line)\n",
        "        except Exception: continue\n",
        "        resp = await srv.handle(msg)\n",
        "        print(json.dumps(resp), flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "    pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    with open(FS_FALLBACK_PATH, \"w\") as f: f.write(code)\n",
        "    os.chmod(FS_FALLBACK_PATH, 0o755)\n",
        "\n",
        "def _ensure_fetch_fallback():\n",
        "    if os.path.exists(FETCH_FALLBACK_PATH): return\n",
        "    code = r'''\n",
        "#!/usr/bin/env python3\n",
        "import asyncio, json, sys, urllib.request, urllib.error, os, pathlib\n",
        "from typing import Any, Dict\n",
        "ALLOWED = os.environ.get(\"ALLOWED_FS_DIR\") or \"/tmp/mcp_demo/allowed\"\n",
        "pathlib.Path(ALLOWED).mkdir(parents=True, exist_ok=True)\n",
        "def safe_join(root: str, rel: str) -> str:\n",
        "    p = os.path.abspath(os.path.join(root, rel))\n",
        "    if not p.startswith(root): raise ValueError(\"Path outside allowed directory\")\n",
        "    return p\n",
        "class FetchMCP:\n",
        "    def __init__(self):\n",
        "        self.tools = [\n",
        "            {\"name\":\"http_get\",\"description\":\"GET a URL (status+snippet)\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\"},\"max_bytes\":{\"type\":\"integer\",\"default\":20000}},\"required\":[\"url\"]}},\n",
        "            {\"name\":\"download_file\",\"description\":\"Download URL to allowed dir\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\"},\"path\":{\"type\":\"string\"}},\"required\":[\"url\",\"path\"]}},\n",
        "        ]\n",
        "    async def handle(self, msg: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        mid = msg.get(\"id\"); method = msg.get(\"method\"); params = msg.get(\"params\", {})\n",
        "        if method == \"initialize\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"tools\":{}},\n",
        "                    \"serverInfo\":{\"name\":\"fetch_fallback\",\"version\":\"0.1.0\"}}}\n",
        "        if method == \"tools/list\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"tools\": self.tools}}\n",
        "        if method == \"tools/call\":\n",
        "            name = params.get(\"name\"); args = params.get(\"arguments\", {})\n",
        "            try:\n",
        "                out = await self.call(name, args)\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"content\":[{\"type\":\"text\",\"text\":out}]}}\n",
        "            except Exception as e:\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32603,\"message\":str(e)}}\n",
        "        return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32601,\"message\":f\"Unknown method {method}\"}}\n",
        "    async def call(self, name: str, args: Dict[str, Any]) -> str:\n",
        "        if name == \"http_get\":\n",
        "            url = args[\"url\"]; max_bytes = int(args.get(\"max_bytes\", 20000))\n",
        "            req = urllib.request.Request(url, headers={\"User-Agent\":\"MCP-Fetch/0.1\"})\n",
        "            with urllib.request.urlopen(req, timeout=20) as r:\n",
        "                status = r.status; raw = r.read(max_bytes)\n",
        "            snippet = raw.decode(\"utf-8\", errors=\"replace\")\n",
        "            return f\"HTTP {status}\\n\\n{snippet}\"\n",
        "        if name == \"download_file\":\n",
        "            url = args[\"url\"]; rel = args[\"path\"]\n",
        "            dst = safe_join(ALLOWED, rel); os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "            req = urllib.request.Request(url, headers={\"User-Agent\":\"MCP-Fetch/0.1\"})\n",
        "            with urllib.request.urlopen(req, timeout=60) as r, open(dst, \"wb\") as f:\n",
        "                total = 0\n",
        "                while True:\n",
        "                    chunk = r.read(65536)\n",
        "                    if not chunk: break\n",
        "                    f.write(chunk); total += len(chunk)\n",
        "            return f\"Downloaded {total} bytes to {os.path.relpath(dst, ALLOWED)}\"\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "async def main():\n",
        "    srv = FetchMCP()\n",
        "    loop = asyncio.get_event_loop()\n",
        "    while True:\n",
        "        line = await loop.run_in_executor(None, sys.stdin.readline)\n",
        "        if not line: break\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try: msg = json.loads(line)\n",
        "        except Exception: continue\n",
        "        resp = await srv.handle(msg)\n",
        "        print(json.dumps(resp), flush=True)\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "    pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    with open(FETCH_FALLBACK_PATH, \"w\") as f: f.write(code)\n",
        "    os.chmod(FETCH_FALLBACK_PATH, 0o755)\n",
        "\n",
        "def _ensure_my_fallback():\n",
        "    if os.path.exists(MY_FALLBACK_PATH): return\n",
        "    code = r'''\n",
        "#!/usr/bin/env python3\n",
        "import asyncio, json, sys, datetime, re\n",
        "\n",
        "class MyFallback:\n",
        "    def __init__(self):\n",
        "        self.tools = [\n",
        "            {\"name\":\"get_current_time\",\"description\":\"Get the current time and date\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{},\"required\":[]}},\n",
        "            {\"name\":\"calculate\",\"description\":\"Perform basic math\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"expression\":{\"type\":\"string\"}},\"required\":[\"expression\"]}},\n",
        "            {\"name\":\"text_stats\",\"description\":\"Text statistics\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"text\":{\"type\":\"string\"}},\"required\":[\"text\"]}},\n",
        "        ]\n",
        "    async def handle(self, msg):\n",
        "        mid = msg.get(\"id\"); method = msg.get(\"method\"); params = msg.get(\"params\", {})\n",
        "        if method == \"initialize\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"tools\":{}},\n",
        "                    \"serverInfo\":{\"name\":\"my_fallback\",\"version\":\"0.1.0\"}}}\n",
        "        if method == \"tools/list\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"tools\": self.tools}}\n",
        "        if method == \"tools/call\":\n",
        "            name = params.get(\"name\"); args = params.get(\"arguments\", {})\n",
        "            try:\n",
        "                out = await self.call(name, args)\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"content\":[{\"type\":\"text\",\"text\":out}]}}\n",
        "            except Exception as e:\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32603,\"message\":str(e)}}\n",
        "        return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32601,\"message\":f\"Unknown method {method}\"}}\n",
        "    async def call(self, name, args):\n",
        "        if name == \"get_current_time\":\n",
        "            return \"Current time: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        if name == \"calculate\":\n",
        "            expr = str(args.get(\"expression\",\"\"))\n",
        "            # Autoriser chiffres, op√©rateurs simples, espaces, parenth√®ses, points\n",
        "            if not re.fullmatch(r\"[0-9+\\-*/().,\\s]+\", expr):\n",
        "                return \"Error: Expression contains invalid characters\"\n",
        "            try:\n",
        "                res = eval(expr, {\"__builtins__\": {}}, {})\n",
        "            except Exception as e:\n",
        "                return f\"Error calculating '{expr}': {e}\"\n",
        "            return f\"Result of '{expr}' = {res}\"\n",
        "        if name == \"text_stats\":\n",
        "            text = str(args.get(\"text\",\"\"))\n",
        "            words = len(text.split())\n",
        "            chars = len(text)\n",
        "            nosp  = len(text.replace(\" \",\"\"))\n",
        "            lines = len(text.splitlines()) or (1 if text else 0)\n",
        "            return f\"Text statistics:\\n- Characters: {chars}\\n- Characters (no spaces): {nosp}\\n- Words: {words}\\n- Lines: {lines}\"\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "async def main():\n",
        "    srv = MyFallback()\n",
        "    loop = asyncio.get_event_loop()\n",
        "    while True:\n",
        "        line = await loop.run_in_executor(None, sys.stdin.readline)\n",
        "        if not line: break\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try: msg = json.loads(line)\n",
        "        except Exception: continue\n",
        "        resp = await srv.handle(msg)\n",
        "        print(json.dumps(resp), flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "    pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    with open(MY_FALLBACK_PATH, \"w\") as f: f.write(code)\n",
        "    os.chmod(MY_FALLBACK_PATH, 0o755)\n",
        "\n",
        "# --- fabrique commandes primaires/fallback ---\n",
        "def _fs_cmd_primary():\n",
        "    if _ensure_module(\"mcp_server_filesystem\", \"mcp-server-filesystem\"):\n",
        "        return [sys.executable, \"-m\", \"mcp_server_filesystem\", ALLOWED_FS_DIR]\n",
        "    return None\n",
        "\n",
        "def _fs_cmd_fallback():\n",
        "    _ensure_fs_fallback()\n",
        "    return [sys.executable, FS_FALLBACK_PATH, ALLOWED_FS_DIR]\n",
        "\n",
        "def _fetch_cmd_primary():\n",
        "    if _ensure_module(\"mcp_server_fetch\", \"mcp-server-fetch\"):\n",
        "        return [sys.executable, \"-m\", \"mcp_server_fetch\"]\n",
        "    return None\n",
        "\n",
        "def _fetch_cmd_fallback():\n",
        "    _ensure_fetch_fallback()\n",
        "    return [sys.executable, FETCH_FALLBACK_PATH]\n",
        "\n",
        "def _my_cmd_primary():\n",
        "    return [sys.executable, MY_MCP_PATH] if os.path.exists(MY_MCP_PATH) else None\n",
        "\n",
        "def _my_cmd_fallback():\n",
        "    _ensure_my_fallback()\n",
        "    return [sys.executable, MY_FALLBACK_PATH]\n",
        "\n",
        "# --- instanciation globale des serveurs (remplac√©s si fallback) ---\n",
        "fetch_srv = None\n",
        "fs_srv    = None\n",
        "my_srv    = None\n",
        "os.environ[\"ALLOWED_FS_DIR\"] = ALLOWED_FS_DIR  # utile pour fetch fallback\n",
        "\n",
        "# --- helpers stricts ---\n",
        "async def _start_with_timeout(server: MCPServer, name: str, timeout: float) -> bool:\n",
        "    try:\n",
        "        await asyncio.wait_for(server.start(), timeout=timeout)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå start {name}: {e}\")\n",
        "        return False\n",
        "\n",
        "async def _tools_with_timeout(server: MCPServer, name: str, timeout: float):\n",
        "    try:\n",
        "        return await asyncio.wait_for(server.list_tools(), timeout=timeout)\n",
        "    except Exception as e:\n",
        "        return {\"__error__\": f\"{name}: {type(e).__name__}: {e}\"}\n",
        "\n",
        "def _tools_ok(tools) -> bool:\n",
        "    return isinstance(tools, list) and len(tools) > 0\n",
        "\n",
        "# --- pipeline start_all v3 ---\n",
        "async def start_all_servers_v3():\n",
        "    global fetch_srv, fs_srv, my_srv\n",
        "\n",
        "    # FETCH\n",
        "    primary = _fetch_cmd_primary()\n",
        "    fetch_srv = MCPServer(\"fetch\", primary or _fetch_cmd_fallback())\n",
        "    ok = await _start_with_timeout(fetch_srv, \"fetch-primary\" if primary else \"fetch-fallback\", START_TIMEOUT_S)\n",
        "    if not ok and primary:\n",
        "        print(\"‚ö†Ô∏è bascule fetch ‚Üí fallback\")\n",
        "        fetch_srv = MCPServer(\"fetch\", _fetch_cmd_fallback())\n",
        "        if not await _start_with_timeout(fetch_srv, \"fetch-fallback\", START_TIMEOUT_S):\n",
        "            raise RuntimeError(\"fetch: primaire et fallback ont √©chou√© au d√©marrage\")\n",
        "\n",
        "    # FS\n",
        "    primary = _fs_cmd_primary()\n",
        "    fs_srv = MCPServer(\"filesystem\", primary or _fs_cmd_fallback())\n",
        "    ok = await _start_with_timeout(fs_srv, \"fs-primary\" if primary else \"fs-fallback\", START_TIMEOUT_S)\n",
        "    if not ok and primary:\n",
        "        print(\"‚ö†Ô∏è bascule filesystem ‚Üí fallback\")\n",
        "        fs_srv = MCPServer(\"filesystem\", _fs_cmd_fallback())\n",
        "        if not await _start_with_timeout(fs_srv, \"fs-fallback\", START_TIMEOUT_S):\n",
        "            raise RuntimeError(\"filesystem: primaire et fallback ont √©chou√© au d√©marrage\")\n",
        "\n",
        "    # MY\n",
        "    primary = _my_cmd_primary()\n",
        "    my_srv = MCPServer(\"my_mcp\", primary or _my_cmd_fallback())\n",
        "    ok = await _start_with_timeout(my_srv, \"my-primary\" if primary else \"my-fallback\", START_TIMEOUT_S)\n",
        "    if not ok and primary:\n",
        "        print(\"‚ö†Ô∏è bascule my_mcp ‚Üí fallback\")\n",
        "        my_srv = MCPServer(\"my_mcp\", _my_cmd_fallback())\n",
        "        if not await _start_with_timeout(my_srv, \"my-fallback\", START_TIMEOUT_S):\n",
        "            raise RuntimeError(\"my_mcp: primaire et fallback ont √©chou√© au d√©marrage\")\n",
        "\n",
        "    # LIST TOOLS (strict)\n",
        "    fetch_tools = await _tools_with_timeout(fetch_srv, \"fetch\", LIST_TIMEOUT_S)\n",
        "    if (isinstance(fetch_tools, dict) and \"__error__\" in fetch_tools) or not _tools_ok(fetch_tools):\n",
        "        # dernier recours: re-bascule fetch->fallback puis relist\n",
        "        print(\"‚ö†Ô∏è fetch tools/list KO ‚Üí re-bascule sur fallback\")\n",
        "        try: await fetch_srv.shutdown()\n",
        "        except: pass\n",
        "        fetch_srv = MCPServer(\"fetch\", _fetch_cmd_fallback())\n",
        "        if await _start_with_timeout(fetch_srv, \"fetch-fallback\", START_TIMEOUT_S):\n",
        "            fetch_tools = await _tools_with_timeout(fetch_srv, \"fetch\", LIST_TIMEOUT_S)\n",
        "\n",
        "    fs_tools  = await _tools_with_timeout(fs_srv,  \"filesystem\", LIST_TIMEOUT_S)\n",
        "    my_tools  = await _tools_with_timeout(my_srv,  \"my_mcp\",     LIST_TIMEOUT_S)\n",
        "\n",
        "    # agr√©gation\n",
        "    all_tools = []\n",
        "    tool_map = {\"fetch\": set(), \"fs\": set(), \"my\": set()}\n",
        "    errors = []\n",
        "\n",
        "    for name, tools, key in ((\"fetch\", fetch_tools, \"fetch\"), (\"filesystem\", fs_tools, \"fs\"), (\"my_mcp\", my_tools, \"my\")):\n",
        "        if isinstance(tools, list) and tools:\n",
        "            all_tools += tools\n",
        "            tool_map[key] = {t[\"name\"] for t in tools}\n",
        "            print(f\"‚úÖ {name}: {len(tools)} outil(s)\")\n",
        "        else:\n",
        "            msg = tools.get(\"__error__\") if isinstance(tools, dict) else f\"{name}: aucun outil\"\n",
        "            errors.append(msg)\n",
        "            print(f\"‚ö†Ô∏è {msg}\")\n",
        "\n",
        "    if not all_tools:\n",
        "        raise RuntimeError(\"Aucun outil MCP disponible. D√©tails: \" + \" | \".join(errors))\n",
        "\n",
        "    print(f\"‚úÖ Total outils MCP: {len(all_tools)}\")\n",
        "    return all_tools, tool_map\n",
        "\n",
        "async def stop_all_servers_v3():\n",
        "    tasks = []\n",
        "    for srv in (fetch_srv, fs_srv, my_srv):\n",
        "        if srv:\n",
        "            tasks.append(srv.shutdown())\n",
        "    if tasks:\n",
        "        await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "# --- API publique utilis√©e par les autres parties ---\n",
        "async def start_all(): return await start_all_servers_v3()\n",
        "async def stop_all():  await stop_all_servers_v3()\n",
        "\n",
        "print(f\"‚úÖ PARTIE 6 v3 charg√©e ‚Äî timeouts: start={START_TIMEOUT_S}s, list={LIST_TIMEOUT_S}s; fallbacks actifs (fetch/fs/my).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866jGqtSHJ80",
        "outputId": "aa4b8dbf-28fc-403d-efda-c2a6857d61a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PARTIE 6 v3 charg√©e ‚Äî timeouts: start=6.0s, list=5.0s; fallbacks actifs (fetch/fs/my).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 7 ‚Äì Client LLM (Ollama)"
      ],
      "metadata": {
        "id": "cBGAe9WqHOVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß© PARTIE 7 ‚Äî Client LLM (Ollama) robuste pour Colab\n",
        "import os, time, socket, json, urllib.request, urllib.error, subprocess\n",
        "from openai import OpenAI\n",
        "\n",
        "# Mod√®le : utilise OLLAMA_MODEL si d√©fini, sinon un mod√®le l√©ger par d√©faut\n",
        "MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.2:1b\")\n",
        "\n",
        "def wait_for_port(host=\"127.0.0.1\", port=11434, timeout=60):\n",
        "    \"\"\"Attend que le serveur Ollama √©coute sur le port donn√©.\"\"\"\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            with socket.create_connection((host, port), timeout=2):\n",
        "                return True\n",
        "        except OSError:\n",
        "            time.sleep(1)\n",
        "    return False\n",
        "\n",
        "def ollama_tags():\n",
        "    \"\"\"Retourne le JSON de /api/tags ou {} si indisponible.\"\"\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(\"http://127.0.0.1:11434/api/tags\", timeout=5) as r:\n",
        "            if r.status == 200:\n",
        "                return json.loads(r.read().decode(\"utf-8\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "def have_model(name: str) -> bool:\n",
        "    tags = ollama_tags()\n",
        "    # structure: {\"models\":[{\"name\":\"llama3.2:1b\", ...}, ...]}\n",
        "    models = tags.get(\"models\", [])\n",
        "    return any(m.get(\"name\") == name for m in models)\n",
        "\n",
        "def pull_model(name: str):\n",
        "    print(f\"‚¨áÔ∏è Pull du mod√®le '{name}' (petit et rapide pour Colab si c'est la 1√®re fois)‚Ä¶\")\n",
        "    try:\n",
        "        subprocess.check_call([\"ollama\", \"pull\", name])\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise RuntimeError(f\"√âchec du pull du mod√®le '{name}': {e}\")\n",
        "\n",
        "# 1) V√©rifie que le serveur Ollama r√©pond (PARTIE 1 est cens√©e l'avoir lanc√©)\n",
        "if not wait_for_port():\n",
        "    raise RuntimeError(\"‚ùå Ollama ne r√©pond pas sur 127.0.0.1:11434. Lance la PARTIE 1 avant.\")\n",
        "\n",
        "# 2) V√©rifie/installe le mod√®le demand√©\n",
        "if not have_model(MODEL):\n",
        "    pull_model(MODEL)\n",
        "    # Re-v√©rifie\n",
        "    if not have_model(MODEL):\n",
        "        raise RuntimeError(f\"‚ùå Le mod√®le '{MODEL}' n'appara√Æt pas dans /api/tags apr√®s le pull.\")\n",
        "\n",
        "# 3) Client OpenAI (API compatible Ollama)\n",
        "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "\n",
        "# 4) Test de la connexion + compl√©tion (retries)\n",
        "last_err = None\n",
        "for attempt in range(3):\n",
        "    try:\n",
        "        _ = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "            max_tokens=8\n",
        "        )\n",
        "        print(\"‚úÖ Connect√© √† Ollama avec succ√®s\")\n",
        "        print(f\"üìã Mod√®le utilis√©: {MODEL}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "        if attempt < 2:\n",
        "            print(f\"‚è≥ Retry connexion Ollama ({attempt+1}/3)‚Ä¶\")\n",
        "            time.sleep(3)\n",
        "        else:\n",
        "            print(f\"‚ùå Erreur de connexion √† Ollama: {e}\\n\"\n",
        "                  f\"   ‚Ä¢ V√©rifie que 'ollama serve' tourne (PARTIE 1)\\n\"\n",
        "                  f\"   ‚Ä¢ V√©rifie que le mod√®le existe: !ollama list\\n\"\n",
        "                  f\"   ‚Ä¢ Tu peux changer le mod√®le via: os.environ['OLLAMA_MODEL'] = 'llama3.2:1b' (ou autre)\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "on_Cidz6HONF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9409b5-3b6a-465f-dd75-0db1828d0e39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Connect√© √† Ollama avec succ√®s\n",
            "üìã Mod√®le utilis√©: llama3.2:1b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 8 ‚Äì Orchestrateur agentique (+ wrapper collecteur)"
      ],
      "metadata": {
        "id": "JTek6qxrHR4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß© PARTIE 8 ‚Äî Orchestrateur agentique + UI Streamlit (STRICT : pas d'ex√©cution en bare mode)\n",
        "\n",
        "'''import os\n",
        "\n",
        "#Option d'override (si tu VEUX quand m√™me faire tourner l'UI dans la cellule) :\n",
        "ALLOW_PART8_UI = os.environ.get(\"ALLOW_PART8_UI\", \"0\") == \"1\"\n",
        "\n",
        "# D√©tection du bare mode (ex√©cution dans une cellule, sans `streamlit run`)\n",
        "#def _is_bare_mode() -> bool:\n",
        "    #try:\n",
        "        from streamlit.runtime.scriptrunner import get_script_run_ctx  # type: ignore\n",
        "        return get_script_run_ctx() is None\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "if _is_bare_mode() and not ALLOW_PART8_UI:\n",
        "    raise SystemExit(\n",
        "        \"‚õî Cette PARTIE 8 ne s'ex√©cute pas en bare mode.\\n\"\n",
        "        \"‚û°Ô∏è Utilise la PARTIE 10 pour lancer l'UI Web (streamlit run + tunnel).\\n\"\n",
        "        \"üí° Si tu veux forcer l'UI dans cette cellule (non recommand√©), ex√©cute avant :\\n\"\n",
        "        \"    import os; os.environ['ALLOW_PART8_UI']='1'\\n\"\n",
        "    )\n",
        "\n",
        "# --- Si on arrive ici, c'est que tu as explicitement autoris√© l'UI en cellule (ALLOW_PART8_UI=1) ---\n",
        "# On injecte la version UI (la m√™me logique que pr√©c√©demment), mais **on n‚Äô√©crit rien**\n",
        "# ici pour √©viter de rallonger : tu peux coller l‚Äôancienne UI de la PARTIE 8 si besoin.\n",
        "print(\"‚úÖ ALLOW_PART8_UI=1 -> tu peux coller ici la version UI de la PARTIE 8 si tu veux tester en cellule.\")'''\n"
      ],
      "metadata": {
        "id": "8MNP_Y6bHRxX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "805d4783-6a36-413b-9d1a-9e19320b8e94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import os\\n\\n#Option d\\'override (si tu VEUX quand m√™me faire tourner l\\'UI dans la cellule) :\\nALLOW_PART8_UI = os.environ.get(\"ALLOW_PART8_UI\", \"0\") == \"1\"\\n\\n# D√©tection du bare mode (ex√©cution dans une cellule, sans `streamlit run`)\\n#def _is_bare_mode() -> bool:\\n    #try:\\n        from streamlit.runtime.scriptrunner import get_script_run_ctx  # type: ignore\\n        return get_script_run_ctx() is None\\n    except Exception:\\n        return True\\n\\nif _is_bare_mode() and not ALLOW_PART8_UI:\\n    raise SystemExit(\\n        \"‚õî Cette PARTIE 8 ne s\\'ex√©cute pas en bare mode.\\n\"\\n        \"‚û°Ô∏è Utilise la PARTIE 10 pour lancer l\\'UI Web (streamlit run + tunnel).\\n\"\\n        \"üí° Si tu veux forcer l\\'UI dans cette cellule (non recommand√©), ex√©cute avant :\\n\"\\n        \"    import os; os.environ[\\'ALLOW_PART8_UI\\']=\\'1\\'\\n\"\\n    )\\n\\n# --- Si on arrive ici, c\\'est que tu as explicitement autoris√© l\\'UI en cellule (ALLOW_PART8_UI=1) ---\\n# On injecte la version UI (la m√™me logique que pr√©c√©demment), mais **on n‚Äô√©crit rien**\\n# ici pour √©viter de rallonger : tu peux coller l‚Äôancienne UI de la PARTIE 8 si besoin.\\nprint(\"‚úÖ ALLOW_PART8_UI=1 -> tu peux coller ici la version UI de la PARTIE 8 si tu veux tester en cellule.\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 9 ‚Äì Application Streamlit autonome (ex√©cutable en Colab)"
      ],
      "metadata": {
        "id": "Hy55tUlJHVl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9Ô∏è‚É£ √âcrire l'app Streamlit autonome\n",
        "%%writefile /tmp/mcp_demo/app.py\n",
        "import asyncio, json, textwrap, time, subprocess, os\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------- MCP helper (copi√© simplifi√©) ----------\n",
        "class MCPServer:\n",
        "    def __init__(self, name, cmd):\n",
        "        self.name, self.cmd = name, cmd\n",
        "        self.proc = None\n",
        "\n",
        "    async def start(self):\n",
        "        self.proc = await asyncio.create_subprocess_exec(\n",
        "            *self.cmd,\n",
        "            stdin=asyncio.subprocess.PIPE,\n",
        "            stdout=asyncio.subprocess.PIPE,\n",
        "            stderr=asyncio.subprocess.PIPE\n",
        "        )\n",
        "        init_msg = {\n",
        "            \"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\",\n",
        "            \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"streamlit-app\"}}\n",
        "        }\n",
        "        self.proc.stdin.write((json.dumps(init_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        await asyncio.sleep(0.8)\n",
        "\n",
        "    async def list_tools(self):\n",
        "        list_msg = {\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/list\"}\n",
        "        self.proc.stdin.write((json.dumps(list_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if line:\n",
        "            data = json.loads(line.decode().strip())\n",
        "            return data.get(\"result\", {}).get(\"tools\", [])\n",
        "        return []\n",
        "\n",
        "    async def call_tool(self, tool_name, arguments):\n",
        "        call_msg = {\"jsonrpc\": \"2.0\", \"id\": 3, \"method\": \"tools/call\", \"params\": {\"name\": tool_name, \"arguments\": arguments}}\n",
        "        self.proc.stdin.write((json.dumps(call_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if line:\n",
        "            return json.loads(line.decode().strip())\n",
        "        return {\"error\": \"no output\"}\n",
        "\n",
        "    async def shutdown(self):\n",
        "        if self.proc:\n",
        "            try:\n",
        "                self.proc.terminate()\n",
        "                await asyncio.sleep(0.5)\n",
        "                if self.proc.returncode is None:\n",
        "                    self.proc.kill()\n",
        "                await self.proc.wait()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# ---------- Boot MCP servers ----------\n",
        "ALLOWED_FS_DIR = \"/tmp/mcp_demo/allowed\"\n",
        "fetch_srv = MCPServer(\"fetch\", [\"mcp-server-fetch\"])\n",
        "fs_srv    = MCPServer(\"fs\", [\"mcp-server-filesystem\", ALLOWED_FS_DIR])\n",
        "my_srv    = MCPServer(\"my_mcp\", [\"python3\", \"/tmp/mcp_demo/my_mcp_server.py\"])\n",
        "\n",
        "async def start_all():\n",
        "    await asyncio.gather(fetch_srv.start(), fs_srv.start(), my_srv.start())\n",
        "    fetch_tools = await fetch_srv.list_tools()\n",
        "    fs_tools    = await fs_srv.list_tools()\n",
        "    my_tools    = await my_srv.list_tools()\n",
        "    all_tools = fetch_tools + fs_tools + my_tools\n",
        "    tool_map = {\"fetch\": {t[\"name\"] for t in fetch_tools},\n",
        "                \"fs\":    {t[\"name\"] for t in fs_tools},\n",
        "                \"my\":    {t[\"name\"] for t in my_tools}}\n",
        "    return all_tools, tool_map\n",
        "\n",
        "# ---------- LLM & Orchestrator ----------\n",
        "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "MODEL = \"llama3.1\"\n",
        "\n",
        "async def run_agent(goal: str, all_tools, tool_map):\n",
        "    SYSTEM_PROMPT = textwrap.dedent(f\"\"\"\n",
        "    You are an AI assistant with access to tools. You must use JSON format for all responses.\n",
        "\n",
        "    Available tools:\n",
        "    {chr(10).join([f\"- {t['name']}: {t.get('description', 'No description')}\" for t in all_tools])}\n",
        "\n",
        "    Response format:\n",
        "    - To use a tool: {{\"tool\": \"tool_name\", \"arguments\": {{\"arg1\": \"value1\", \"arg2\": \"value2\"}}}}\n",
        "    - To give final answer: {{\"answer\": \"your final answer\"}}\n",
        "\n",
        "    Always think step by step and use the appropriate tools.\n",
        "    \"\"\")\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": goal}]\n",
        "\n",
        "    outputs = []\n",
        "    for step in range(8):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL, messages=messages, temperature=0.1, max_tokens=1000\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "            try:\n",
        "                decision = json.loads(content)\n",
        "            except json.JSONDecodeError:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": \"Please respond with valid JSON format only.\"})\n",
        "                outputs.append(f\"‚ö†Ô∏è Invalid JSON response: {content}\")\n",
        "                continue\n",
        "\n",
        "            if \"answer\" in decision:\n",
        "                outputs.append(f\"‚úÖ {decision['answer']}\")\n",
        "                break\n",
        "\n",
        "            elif \"tool\" in decision and \"arguments\" in decision:\n",
        "                tool_name = decision[\"tool\"]\n",
        "                arguments = decision[\"arguments\"]\n",
        "                server = None\n",
        "                if tool_name in tool_map[\"fetch\"]:\n",
        "                    server = fetch_srv\n",
        "                elif tool_name in tool_map[\"fs\"]:\n",
        "                    server = fs_srv\n",
        "                elif tool_name in tool_map[\"my\"]:\n",
        "                    server = my_srv\n",
        "\n",
        "                if server:\n",
        "                    result = await server.call_tool(tool_name, arguments)\n",
        "                    observation = str(result)\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
        "                    outputs.append(json.dumps({\n",
        "                        \"step\": step + 1, \"tool\": tool_name, \"arguments\": arguments, \"result\": observation\n",
        "                    }, indent=2))\n",
        "                else:\n",
        "                    err = f\"Tool '{tool_name}' not found\"\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Error: {err}\"})\n",
        "                    outputs.append(f\"‚ùå {err}\")\n",
        "            else:\n",
        "                err = \"Invalid response format - must contain either 'answer' or 'tool' with 'arguments'\"\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": err})\n",
        "                outputs.append(f\"‚ùå {err}\")\n",
        "        except Exception as e:\n",
        "            outputs.append(f\"‚ùå Error: {str(e)}\")\n",
        "            break\n",
        "    else:\n",
        "        outputs.append(\"‚ùå Maximum steps reached without completing the task.\")\n",
        "    return outputs\n",
        "\n",
        "# ---------- Streamlit UI ----------\n",
        "st.set_page_config(page_title=\"ü§ñ MCP Agent avec Ollama\", layout=\"wide\", initial_sidebar_state=\"expanded\")\n",
        "st.title(\"ü§ñ MCP Agent avec Ollama\")\n",
        "st.markdown(\"\"\"\n",
        "Cette d√©mo utilise:\n",
        "- **Ollama** avec Llama 3.1\n",
        "- **Serveurs MCP** (fetch, filesystem, custom)\n",
        "- **Streamlit** pour l'interface\n",
        "\"\"\")\n",
        "\n",
        "if \"boot_done\" not in st.session_state:\n",
        "    with st.spinner(\"D√©marrage des serveurs MCP‚Ä¶\"):\n",
        "        all_tools, tool_map = asyncio.run(start_all())\n",
        "    st.session_state.boot_done = True\n",
        "    st.session_state.all_tools = all_tools\n",
        "    st.session_state.tool_map = tool_map\n",
        "\n",
        "with st.expander(\"‚öôÔ∏è Configuration\"):\n",
        "    st.write(\"**Serveurs MCP actifs:**\")\n",
        "    st.write(f\"- üì° Fetch Server: ‚úÖ\")\n",
        "    st.write(f\"- üìÅ Filesystem Server: ‚úÖ\")\n",
        "    st.write(f\"- üõ†Ô∏è Custom Server: ‚úÖ\")\n",
        "    st.write(\"**Outils disponibles:**\")\n",
        "    for tool in st.session_state.all_tools:\n",
        "        st.write(f\"- `{tool['name']}`: {tool.get('description','No description')}\")\n",
        "\n",
        "goal = st.text_area(\n",
        "    \"üéØ Objectif (en anglais pour de meilleurs r√©sultats):\",\n",
        "    height=120,\n",
        "    placeholder=\"Ex: Download https://example.com/data.csv, convert to JSON, summarize, and save to summary.txt\"\n",
        ")\n",
        "\n",
        "cols = st.columns(3)\n",
        "with cols[0]:\n",
        "    if st.button(\"üì• T√©l√©charger et convertir\"):\n",
        "        goal = \"Download https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv, convert to JSON, and show first 3 records\"\n",
        "with cols[1]:\n",
        "    if st.button(\"üìù R√©sumer du texte\"):\n",
        "        goal = \"Summarize this text: 'Artificial intelligence is transforming many industries. Machine learning algorithms can now recognize patterns in data that humans might miss. This technology is being used in healthcare, finance, and transportation. The future of AI looks promising with continued advancements.'\"\n",
        "with cols[2]:\n",
        "    if st.button(\"üìÅ Lister les fichiers\"):\n",
        "        goal = \"List files in the current directory and show their contents\"\n",
        "\n",
        "if st.button(\"‚ñ∂Ô∏è Ex√©cuter\", type=\"primary\"):\n",
        "    if not goal.strip():\n",
        "        st.warning(\"Veuillez entrer un objectif\")\n",
        "    else:\n",
        "        progress = st.progress(0)\n",
        "        status   = st.empty()\n",
        "        output   = st.empty()\n",
        "        with st.spinner(\"Ex√©cution‚Ä¶\"):\n",
        "            outputs = asyncio.run(run_agent(goal, st.session_state.all_tools, st.session_state.tool_map))\n",
        "        lines = []\n",
        "        for i, item in enumerate(outputs):\n",
        "            lines.append(item)\n",
        "            output.code(\"\\n\".join(lines), language=\"json\")\n",
        "            progress.progress(min((i+1)/max(1,len(outputs)), 1.0))\n",
        "            status.text(f\"Step {i+1}/{max(1,len(outputs))}\")\n",
        "        status.success(\"‚úÖ T√¢che termin√©e!\")\n",
        "\n",
        "st.sidebar.markdown(\"### üßπ Nettoyage\")\n",
        "if st.sidebar.button(\"Arr√™ter les serveurs MCP\"):\n",
        "    asyncio.run(fetch_srv.shutdown())\n",
        "    asyncio.run(fs_srv.shutdown())\n",
        "    asyncio.run(my_srv.shutdown())\n",
        "    st.sidebar.success(\"Serveurs arr√™t√©s\")\n"
      ],
      "metadata": {
        "id": "9MiGhj5BHVRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c07a4f-2b84-4136-8ca3-9e055e2ce539"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/mcp_demo/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rapide sanity check\n",
        "import asyncio\n",
        "\n",
        "try:\n",
        "    tools, tmap = asyncio.run(start_all())\n",
        "    print(\"OK, outils:\", [t[\"name\"] for t in tools])\n",
        "    asyncio.run(stop_all())\n",
        "except Exception as e:\n",
        "    print(\"Erreur MCP:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILEol38IS08c",
        "outputId": "fee45d91-45fd-4e58-d5d8-66da4600e051"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur MCP: asyncio.run() cannot be called from a running event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3366835543.py:9: RuntimeWarning: coroutine 'start_all' was never awaited\n",
            "  print(\"Erreur MCP:\", e)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ PARTIE 9.5 ‚Äî Sanity check MCP ‚Äî compatible Colab (pas d'asyncio.run direct)\n",
        "import asyncio, time\n",
        "import nest_asyncio; nest_asyncio.apply()\n",
        "\n",
        "def run_async(coro):\n",
        "    \"\"\"Si une loop tourne d√©j√† (Colab), retourne un Task; sinon ex√©cute et retourne le r√©sultat.\"\"\"\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "    except RuntimeError:\n",
        "        loop = None\n",
        "    if loop and loop.is_running():\n",
        "        return asyncio.ensure_future(coro)\n",
        "    else:\n",
        "        return asyncio.run(coro)\n",
        "\n",
        "def await_result(x, poll_s: float = 0.05):\n",
        "    \"\"\"Attend un Task/Future; si x est d√©j√† une valeur, la renvoie telle quelle.\"\"\"\n",
        "    if not isinstance(x, (asyncio.Task, asyncio.Future)):\n",
        "        return x\n",
        "    while not x.done():\n",
        "        time.sleep(poll_s)\n",
        "    return x.result()\n",
        "\n",
        "# ‚ö†Ô∏è Pr√©-requis : PARTIE 5 (corrig√©e) et PARTIE 6 v2 doivent √™tre ex√©cut√©es avant.\n",
        "if \"start_all\" not in globals() or \"stop_all\" not in globals():\n",
        "    raise RuntimeError(\"Ex√©cute la PARTIE 5 (corrig√©e) puis la PARTIE 6 v2 avant ce sanity check.\")\n",
        "\n",
        "# D√©marrer ‚Üí lister ‚Üí arr√™ter avec logs\n",
        "print(\"‚è≥ D√©marrage des serveurs MCP...\")\n",
        "started = run_async(start_all())\n",
        "try:\n",
        "    tools, tmap = await_result(started, poll_s=0.2)\n",
        "    print(\"‚úÖ Outils disponibles :\", [t[\"name\"] for t in tools])\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Erreur pendant start_all:\", e)\n",
        "\n",
        "print(\"‚è≥ Arr√™t des serveurs MCP...\")\n",
        "try:\n",
        "    await_result(run_async(stop_all()))\n",
        "    print(\"‚úÖ Serveurs MCP stopp√©s proprement.\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Erreur pendant stop_all:\", e)\n"
      ],
      "metadata": {
        "id": "CqBM2H84Zpuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8df26a7-e74f-41e3-f2ed-d3958440df14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ D√©marrage des serveurs MCP...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 10 ‚Äì Lancer Streamlit dans Colab"
      ],
      "metadata": {
        "id": "tetQqjFhHdn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîü PARTIE 10 ‚Äî URL externe via ngrok (avec authtoken)\n",
        "import os, time, subprocess, urllib.request, urllib.error\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ‚¨áÔ∏è Mets tes tokens ici si tu veux\n",
        "os.environ[\"MY_API_TOKEN\"] = \"TON_TOKEN_APP_ICI\"            # si besoin dans app.py\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = \"30Y4I20WWMblLpoQmZBTJyvdfis_4tfap1sfjHNepzEv9yrdk\"   # OBLIGATOIRE pour ngrok\n",
        "\n",
        "# 1) V√©rifier authtoken ngrok\n",
        "authtoken = os.environ.get(\"NGROK_AUTHTOKEN\", \"\").strip()\n",
        "if not authtoken:\n",
        "    raise RuntimeError(\n",
        "        \"Aucun NGROK_AUTHTOKEN trouv√©. Va sur https://dashboard.ngrok.com/get-started/your-authtoken \"\n",
        "        \"puis place-le dans os.environ['NGROK_AUTHTOKEN'].\"\n",
        "    )\n",
        "\n",
        "# 2) D√©marrer Streamlit en arri√®re-plan\n",
        "print(\"üöÄ Lancement de Streamlit (port 8501)‚Ä¶\")\n",
        "log_path = \"/tmp/mcp_demo/streamlit.log\"\n",
        "streamlit_cmd = [\n",
        "    \"streamlit\", \"run\", \"/tmp/mcp_demo/app.py\",\n",
        "    \"--server.port=8501\",\n",
        "    \"--server.headless=true\",\n",
        "    \"--server.address=0.0.0.0\",\n",
        "    \"--server.enableCORS=false\",\n",
        "    \"--server.enableXsrfProtection=false\",\n",
        "    \"--server.enableWebsocketCompression=false\",\n",
        "    \"--browser.gatherUsageStats=false\",\n",
        "    \"--server.fileWatcherType=none\",\n",
        "]\n",
        "with open(log_path, \"w\") as lf:\n",
        "    streamlit_proc = subprocess.Popen(\n",
        "        streamlit_cmd, stdout=lf, stderr=subprocess.STDOUT, text=True\n",
        "    )\n",
        "print(f\"üìÑ Logs: {log_path}\")\n",
        "\n",
        "# 3) Attendre que √ßa r√©ponde en local\n",
        "def wait_http_ready(url, timeout=40):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            with urllib.request.urlopen(url, timeout=2) as r:\n",
        "                if r.status in (200, 302, 403):\n",
        "                    return True\n",
        "        except Exception:\n",
        "            time.sleep(1)\n",
        "    return False\n",
        "\n",
        "if not wait_http_ready(\"http://127.0.0.1:8501\"):\n",
        "    raise RuntimeError(\"Streamlit ne r√©pond pas en local (voir logs).\")\n",
        "\n",
        "print(\"‚úÖ Streamlit OK en local.\")\n",
        "\n",
        "# 4) Configurer ngrok + ouvrir le tunnel\n",
        "print(\"üåê Ouverture du tunnel ngrok‚Ä¶\")\n",
        "ngrok.set_auth_token(authtoken)\n",
        "public = ngrok.connect(8501, \"http\")\n",
        "public_url = str(public)\n",
        "print(f\"üîó URL publique: {public_url}\")\n",
        "\n",
        "# 5) Probe rapide externe (optionnel)\n",
        "try:\n",
        "    with urllib.request.urlopen(public_url, timeout=15) as r:\n",
        "        print(f\"ü©∫ Probe externe: HTTP {r.status}\")\n",
        "except urllib.error.HTTPError as e:\n",
        "    print(f\"ü©∫ Probe externe: HTTP {e.code}\")\n",
        "except Exception as e:\n",
        "    print(f\"ü©∫ Probe externe: error {e}\")\n",
        "\n",
        "print(\"üëâ Ouvre cette URL dans un onglet navigateur (pas en iframe):\")\n",
        "print(public_url)\n"
      ],
      "metadata": {
        "id": "8obhiGduHdba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57026241-26f4-45fa-d728-db08e7f6a81d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Lancement de Streamlit (port 8501)‚Ä¶\n",
            "üìÑ Logs: /tmp/mcp_demo/streamlit.log\n",
            "‚úÖ Streamlit OK en local.\n",
            "üåê Ouverture du tunnel ngrok‚Ä¶\n",
            "üîó URL publique: NgrokTunnel: \"https://ae553af22eaa.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "ü©∫ Probe externe: error <urlopen error unknown url type: ngroktunnel>\n",
            "üëâ Ouvre cette URL dans un onglet navigateur (pas en iframe):\n",
            "NgrokTunnel: \"https://ae553af22eaa.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logs Streamlit (100 derni√®res lignes)\n",
        "!tail -n 100 /tmp/mcp_demo/streamlit.log\n"
      ],
      "metadata": {
        "id": "KquttwIk9wKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which mcp-server-fetch\n",
        "!which mcp-server-filesystem\n",
        "!python3 -c \"import mcp, sys; print('mcp', mcp.__version__); import pandas; print('pandas', pandas.__version__)\"\n"
      ],
      "metadata": {
        "id": "CM5a80mt99yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list\n",
        "!curl -s http://localhost:11434/api/tags\n"
      ],
      "metadata": {
        "id": "U-o9lf7t9-27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© PARTIE 11 ‚Äì Nettoyage automatique"
      ],
      "metadata": {
        "id": "OqL8EuURHlAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1Ô∏è‚É£1Ô∏è‚É£ Nettoyage des ressources\n",
        "import atexit, asyncio\n",
        "\n",
        "async def cleanup():\n",
        "    print(\"üßπ Nettoyage des serveurs MCP (notebook)‚Ä¶\")\n",
        "    try:\n",
        "        await asyncio.gather(\n",
        "            fetch_srv.shutdown(),\n",
        "            fs_srv.shutdown(),\n",
        "            my_srv.shutdown()\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if 'ollama_process' in globals() and ollama_process:\n",
        "            ollama_process.terminate()\n",
        "    except:\n",
        "        pass\n",
        "    print(\"‚úÖ Nettoyage termin√©\")\n",
        "\n",
        "atexit.register(lambda: asyncio.get_event_loop().run_until_complete(cleanup()))\n"
      ],
      "metadata": {
        "id": "Qd_2ahuDHkxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}