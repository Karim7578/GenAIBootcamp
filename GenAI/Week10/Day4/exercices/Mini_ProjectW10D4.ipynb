{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fbG_6VBcM0Mv"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit google-generativeai python-dotenv langchain PyPDF2 chromadb faiss-cpu langchain_google_genai langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurez votre cl√© API Google :"
      ],
      "metadata": {
        "id": "DgBqDWAkNvWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyAnTbYRayfCWmW-uegiW-KTbF_r8cdytw0'"
      ],
      "metadata": {
        "id": "4lCsrEDENCWI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cr√©ez le fichier .env (optionnel si vous utilisez os.environ)"
      ],
      "metadata": {
        "id": "u_Pxw6OsNsYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write('GOOGLE_API_KEY=VOTRE_CLE_API_ICI')"
      ],
      "metadata": {
        "id": "-P6Nb35ENFFx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nArn-YvANie0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import csv\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Charger les variables d'environnement\n",
        "load_dotenv()\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"Extrait le texte de tous les fichiers PDF upload√©s\"\"\"\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"Divise le texte en chunks avec overlap\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=10000, chunk_overlap=1000)\n",
        "    chunks = splitter.split_text(text)\n",
        "    return chunks  # list of strings\n",
        "\n",
        "def get_vector_store(chunks):\n",
        "    \"\"\"Cr√©e et sauvegarde un vector store FAISS\"\"\"\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")  # type: ignore\n",
        "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "\n",
        "def get_conversational_chain():\n",
        "    \"\"\"Cr√©e la cha√Æne conversationnelle avec Gemini\"\"\"\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not available in the context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
        "    Context:\\n {context}?\\n\n",
        "    Question: \\n{question}\\n\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                                   client=genai,\n",
        "                                   temperature=0.3,\n",
        "                                   )\n",
        "    prompt = PromptTemplate(template=prompt_template,\n",
        "                            input_variables=[\"context\", \"question\"])\n",
        "    chain = load_qa_chain(llm=model, chain_type=\"stuff\", prompt=prompt)\n",
        "    return chain\n",
        "\n",
        "def clear_chat_history():\n",
        "    \"\"\"Remet √† z√©ro l'historique de chat\"\"\"\n",
        "    st.session_state.messages = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Upload some PDFs and ask me a question\"}]\n",
        "\n",
        "def user_input(user_question):\n",
        "    \"\"\"Traite la question de l'utilisateur et retourne une r√©ponse\"\"\"\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(\n",
        "        model=\"models/embedding-001\")  # type: ignore\n",
        "\n",
        "    new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "\n",
        "    chain = get_conversational_chain()\n",
        "\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    response = chain(\n",
        "        {\"input_documents\": docs, \"context\": context, \"question\": user_question},\n",
        "        return_only_outputs=True, )\n",
        "\n",
        "    return response['output_text']\n",
        "\n",
        "def save_user_info(name, phone, email):\n",
        "    \"\"\"Sauvegarde les informations utilisateur dans un fichier CSV\"\"\"\n",
        "    file_exists = os.path.isfile('user_info.csv')\n",
        "    with open('user_info.csv', mode='a', newline='') as file:\n",
        "        fieldnames = ['Name', 'Phone', 'Email']\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow({'Name': name, 'Phone': phone, 'Email': email})\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(  # Configure the Streamlit page settings\n",
        "        page_title=\"Chat with PDF using Gemini\",  # Set the browser tab title\n",
        "        page_icon=\"ü§ñ\",  # Set the favicon/icon for the page\n",
        "        layout=\"wide\"  # Use a wide layout for more horizontal space\n",
        "    )\n",
        "\n",
        "    # Sidebar for uploading PDF files\n",
        "    with st.sidebar:  # Begin sidebar container\n",
        "        st.title(\"Menu\")  # Display the title \"Menu\" in the sidebar\n",
        "        pdf_docs = st.file_uploader(  # file uploader widget\n",
        "            \"Upload your PDF Files and Click on the Submit & Process Button\",\n",
        "            accept_multiple_files=True)  # Allow multiple PDF uploads\n",
        "        if st.button(\"Submit & Process\"):  # Add a button \"Submit & Process\" to trigger processing\n",
        "            with st.spinner(\"Processing...\"):  # Show a spinner while processing\n",
        "                raw_text = get_pdf_text(pdf_docs)  # Extract raw text from uploaded PDFs\n",
        "                text_chunks = get_text_chunks(raw_text)  # Split the text into smaller chunks\n",
        "                get_vector_store(text_chunks)  # Build or update vector store for retrieval\n",
        "                st.success(\"Done\")  # Show a success message once processing finishes\n",
        "\n",
        "    # Main content area for displaying chat messages\n",
        "    st.title(\"Chat with PDF files using Gemini üôã‚Äç‚ôÇÔ∏è\")  # Display the main page title\n",
        "    st.write(\"Welcome to the chat!\")  # Display a welcome message\n",
        "\n",
        "    # Add clear chat history button in sidebar\n",
        "    with st.sidebar:\n",
        "        if st.button('Clear Chat History'):  # Add a button 'Clear Chat History'\n",
        "            clear_chat_history()\n",
        "\n",
        "    # Chat input\n",
        "    if \"messages\" not in st.session_state.keys():  # If no messages stored yet\n",
        "        st.session_state.messages = [  # Initialize chat history with a default assistant message\n",
        "            {\"role\": \"assistant\", \"content\": \"Upload some PDFs and ask me a question\"}]\n",
        "\n",
        "    for message in st.session_state.messages:  # Loop over stored chat messages\n",
        "        with st.chat_message(message[\"role\"]):  # Render each message with the correct role\n",
        "            st.markdown(f\"**{message['role'].capitalize()}:** {message['content']}\")  # Display the message content\n",
        "\n",
        "    if prompt := st.chat_input():  # If the user enters a new chat prompt\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})  # Add the user message to session state\n",
        "        with st.chat_message(\"user\"):  # Render the user's message in the chat\n",
        "            st.markdown(f\"**User:** {prompt}\")  # Display the user's input\n",
        "\n",
        "        # Check for specific user request to call them\n",
        "        if \"call me\" in prompt.lower():  # If prompt contains 'call me'\n",
        "            st.session_state.collecting_info = True  # Flag to start collecting user contact info\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":  # If the last message isn't from the assistant yet\n",
        "            with st.chat_message(\"assistant\"):  # Prepare to render the assistant's response\n",
        "                with st.spinner(\"Thinking...\"):  # Show a spinner while the model generates a response\n",
        "                    response = user_input(prompt)  # Call the retrieval+LLM pipeline to get an answer\n",
        "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})  # Store the assistant's reply\n",
        "                    st.markdown(f\"**Assistant:** {response}\")  # Display the assistant's response\n",
        "\n",
        "    # Collect user information\n",
        "    if \"collecting_info\" in st.session_state and st.session_state.collecting_info:  # If flagged to collect info\n",
        "        st.subheader(\"Contact Information\")  # Prompt for contact form\n",
        "        with st.form(key=\"contact_form\"):  # Begin a form for user contact details\n",
        "            name = st.text_input(\"Name\")  # Input field for name\n",
        "            phone = st.text_input(\"Phone Number\")  # Input field for phone number\n",
        "            email = st.text_input(\"Email Address\")  # Input field for email address\n",
        "            submit_button = st.form_submit_button(\"Submit Contact Info\")  # Button to submit the form\n",
        "\n",
        "            if submit_button:  # When the form is submitted\n",
        "                save_user_info(name, phone, email)  # Save the user info to a CSV file\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": f\"Thank you, {name}. We will contact you at {phone} or {email}.\"\n",
        "                })  # Thank the user\n",
        "                st.session_state.collecting_info = False  # Stop collecting info\n",
        "                st.experimental_rerun()  # Refresh the app to show the new message\n",
        "\n",
        "if __name__ == \"__main__\":  # Entry point check\n",
        "    main()  # Run the main function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvQ1_f0eNLrm",
        "outputId": "06a708ec-6d1b-43bc-e702-09d27a2091f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lancez l'application Streamlit :"
      ],
      "metadata": {
        "id": "VRzMg0xKNoXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acc√©dez √† l'application via le tunnel ngrok :"
      ],
      "metadata": {
        "id": "SzNBGqHSNegL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py \\\n",
        "  --server.port 8501 \\\n",
        "  --server.address 0.0.0.0 \\\n",
        "  --server.enableCORS false \\\n",
        "  --server.enableXsrfProtection false \\\n",
        "  &>/dev/null &"
      ],
      "metadata": {
        "id": "dMlQgP-IPDQP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Authentification (si vous en avez un)\n",
        "ngrok.set_auth_token(\"30Y4I20WWMblLpoQmZBTJyvdfis_4tfap1sfjHNepzEv9yrdk\")\n",
        "\n",
        "# Lancement du tunnel\n",
        "public_url = ngrok.connect(addr='8501', proto='http')\n",
        "print(f\"üåç Your app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OqwBbm1PIQ6",
        "outputId": "9e45e853-5eaa-4fe4-fad3-159c2414f873"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç Your app is live at: NgrokTunnel: \"https://a78b85513fed.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}