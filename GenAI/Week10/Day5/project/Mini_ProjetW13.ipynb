{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 0 â€“ Installations & environnement"
      ],
      "metadata": {
        "id": "fYN1Yr9oGx_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tIUHp2gC560",
        "outputId": "127b39d8-974c-4d60-b04a-2b8b2510ff08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Installation/MAJ paquets de baseâ€¦\n",
            "ðŸ“¦ Installation MCPâ€¦\n",
            "âš ï¸  mcp-server-filesystem non disponible sur ce runtime â€” on utilisera le fallback plus tard.\n",
            "ðŸ”Ž VÃ©rifications dâ€™importsâ€¦\n",
            "âœ… Ollama dÃ©jÃ  installÃ©\n",
            "\n",
            "âœ… RÃ‰CAPITULATIF\n",
            "streamlit  â†’ 1.48.1\n",
            "pandas     â†’ 2.3.2\n",
            "openai     â†’ 1.101.0\n",
            "nest-asyncioâ†’ 1.6.0\n",
            "pyngrok    â†’ 7.3.0\n",
            "cloudflaredâ†’ 1.0.0.2\n",
            "mcp        â†’ 1.13.1\n",
            "mcp-server-fetch    â†’ 2025.4.7\n",
            "mcp-server-filesystemâ†’ NOT INSTALLED\n",
            "ollama bin : /usr/local/bin/ollama\n",
            "fetch bin  : /usr/local/bin/mcp-server-fetch\n",
            "fs bin     : None\n",
            "\n",
            "ðŸŽ‰ PARTIE 0 OK â€” tu peux continuer aux parties suivantes.\n"
          ]
        }
      ],
      "source": [
        "# PARTIE 0 â€” Installations & environnement (force install + vÃ©rifications)\n",
        "import sys, subprocess, shutil, os\n",
        "\n",
        "def pipi(*pkgs):\n",
        "    # installe/upgrade silencieusement\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"--quiet\", *pkgs]\n",
        "    return subprocess.check_call(cmd)\n",
        "\n",
        "# 1) Paquets essentiels (streamlit + dÃ©pendances)\n",
        "print(\"ðŸ“¦ Installation/MAJ paquets de baseâ€¦\")\n",
        "pipi(\n",
        "    \"streamlit>=1.36.0\",\n",
        "    \"pandas>=2.2.2\",\n",
        "    \"openai>=1.40.0\",\n",
        "    \"nest-asyncio>=1.6.0\",\n",
        "    \"pyngrok>=7.1.3\",\n",
        "    \"cloudflared>=0.4\",\n",
        ")\n",
        "\n",
        "# 2) Paquets MCP (fetch officiel) + filesystem (si dispo)\n",
        "print(\"ðŸ“¦ Installation MCPâ€¦\")\n",
        "pipi(\"mcp>=1.0.1\", \"mcp-server-fetch>=0.1.2\")\n",
        "try:\n",
        "    pipi(\"mcp-server-filesystem>=0.1.2\")\n",
        "    _fs_pkg_ok = True\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"âš ï¸  mcp-server-filesystem non disponible sur ce runtime â€” on utilisera le fallback plus tard.\")\n",
        "    _fs_pkg_ok = False\n",
        "\n",
        "# 3) VÃ©rifications dâ€™imports (streamlit surtout)\n",
        "print(\"ðŸ”Ž VÃ©rifications dâ€™importsâ€¦\")\n",
        "try:\n",
        "    import streamlit as st  # noqa: F401\n",
        "    _streamlit_ok = True\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸  Import streamlit a Ã©chouÃ©, on force une rÃ©installation propreâ€¦\", e)\n",
        "    # RÃ©installe en forÃ§ant\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--force-reinstall\", \"streamlit>=1.36.0\"])\n",
        "    import importlib\n",
        "    importlib.invalidate_caches()\n",
        "    try:\n",
        "        import streamlit as st  # noqa: F401\n",
        "        _streamlit_ok = True\n",
        "    except Exception as e2:\n",
        "        _streamlit_ok = False\n",
        "        print(\"âŒ  Ã‰chec import streamlit aprÃ¨s rÃ©installation:\", e2)\n",
        "\n",
        "# 4) (Optionnel) Installer Ollama si absent (utile pour les parties suivantes)\n",
        "if shutil.which(\"ollama\") is None:\n",
        "    print(\"ðŸ“¦ Installation d'Ollama (peut prendre ~30s)â€¦\")\n",
        "    # Colab accepte le pipe shell -> sh\n",
        "    subprocess.check_call(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True)\n",
        "else:\n",
        "    print(\"âœ… Ollama dÃ©jÃ  installÃ©\")\n",
        "\n",
        "# 5) Affichage dâ€™Ã©tat\n",
        "import importlib.metadata as md\n",
        "def v(pkg):\n",
        "    try: return md.version(pkg)\n",
        "    except md.PackageNotFoundError: return \"NOT INSTALLED\"\n",
        "\n",
        "print(\"\\nâœ… RÃ‰CAPITULATIF\")\n",
        "print(\"streamlit  â†’\", v(\"streamlit\"))\n",
        "print(\"pandas     â†’\", v(\"pandas\"))\n",
        "print(\"openai     â†’\", v(\"openai\"))\n",
        "print(\"nest-asyncioâ†’\", v(\"nest-asyncio\"))\n",
        "print(\"pyngrok    â†’\", v(\"pyngrok\"))\n",
        "print(\"cloudflaredâ†’\", v(\"cloudflared\"))\n",
        "print(\"mcp        â†’\", v(\"mcp\"))\n",
        "print(\"mcp-server-fetch    â†’\", v(\"mcp-server-fetch\"))\n",
        "print(\"mcp-server-filesystemâ†’\", v(\"mcp-server-filesystem\"))\n",
        "print(\"ollama bin :\", shutil.which(\"ollama\"))\n",
        "print(\"fetch bin  :\", shutil.which(\"mcp-server-fetch\"))\n",
        "print(\"fs bin     :\", shutil.which(\"mcp-server-filesystem\"))\n",
        "\n",
        "# 6) Garde-fous clairs\n",
        "assert _streamlit_ok, \"Streamlit n'est pas importable. Relance cette cellule; si le problÃ¨me persiste, redÃ©marre le runtime.\"\n",
        "os.environ.setdefault(\"OLLAMA_MODEL\", \"llama3.2:1b\")  # modÃ¨le lÃ©ger par dÃ©faut\n",
        "print(\"\\nðŸŽ‰ PARTIE 0 OK â€” tu peux continuer aux parties suivantes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 1 â€“ DÃ©marrer le serveur Ollama en arriÃ¨re-plan"
      ],
      "metadata": {
        "id": "fR_f2tJcG11n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§© PARTIE 1-bis â€” Restart Ollama + wait + pull modÃ¨le\n",
        "import subprocess, time, socket, os, json, urllib.request\n",
        "\n",
        "MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.2:1b\")\n",
        "\n",
        "# 1) stop Ã©ventuels processus\n",
        "subprocess.run([\"pkill\", \"-f\", \"ollama\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(2)\n",
        "\n",
        "# 2) start ollama serve\n",
        "ollama_proc = subprocess.Popen([\"ollama\", \"serve\"],\n",
        "                               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# 3) wait port\n",
        "def wait_for_port(host=\"127.0.0.1\", port=11434, timeout=90):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            with socket.create_connection((host, port), timeout=2):\n",
        "                return True\n",
        "        except OSError:\n",
        "            time.sleep(1)\n",
        "    return False\n",
        "\n",
        "if not wait_for_port():\n",
        "    raise RuntimeError(\"âŒ Ollama ne rÃ©pond pas sur 127.0.0.1:11434. Installe/dÃ©marre 'ollama serve'.\")\n",
        "\n",
        "# 4) pull du modÃ¨le si absent\n",
        "def have_model(name: str) -> bool:\n",
        "    try:\n",
        "        with urllib.request.urlopen(\"http://127.0.0.1:11434/api/tags\", timeout=5) as r:\n",
        "            if r.status == 200:\n",
        "                tags = json.loads(r.read().decode(\"utf-8\"))\n",
        "                return any(m.get(\"name\")==name for m in tags.get(\"models\",[]))\n",
        "    except Exception:\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "if not have_model(MODEL):\n",
        "    print(f\"â¬‡ï¸ Pull modÃ¨le {MODEL}â€¦\")\n",
        "    code = subprocess.call([\"ollama\", \"pull\", MODEL])\n",
        "    if code != 0 or not have_model(MODEL):\n",
        "        raise RuntimeError(f\"âŒ Ã‰chec du pull du modÃ¨le {MODEL}. Essaie un modÃ¨le plus lÃ©ger (ex: llama3.2:1b).\")\n",
        "\n",
        "print(f\"âœ… Ollama OK, modÃ¨le prÃªt: {MODEL}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROSHaucnG1t8",
        "outputId": "71ee9794-abc8-4855-fcbd-fab15d46871b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ollama OK, modÃ¨le prÃªt: llama3.2:1b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 2 â€“ CrÃ©ation du serveur MCP perso (dossiers)"
      ],
      "metadata": {
        "id": "7yAQNj7DG6SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2ï¸âƒ£ CrÃ©er les dossiers nÃ©cessaires\n",
        "import os, pathlib\n",
        "\n",
        "MCP_DEMO_DIR = \"/tmp/mcp_demo\"\n",
        "ALLOWED_FS_DIR = f\"{MCP_DEMO_DIR}/allowed\"\n",
        "\n",
        "pathlib.Path(MCP_DEMO_DIR).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(ALLOWED_FS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Dossiers crÃ©Ã©s: {MCP_DEMO_DIR}, {ALLOWED_FS_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCtNmdusG6IO",
        "outputId": "25ab3045-2954-4d04-ae20-b34740cab9fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dossiers crÃ©Ã©s: /tmp/mcp_demo, /tmp/mcp_demo/allowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 3 â€“ Serveur MCP personnalisÃ© (fix import pandas)"
      ],
      "metadata": {
        "id": "5UGOnU36G-ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3ï¸âƒ£ CrÃ©ation du serveur MCP personnalisÃ©\n",
        "%%writefile /tmp/mcp_demo/my_mcp_server.py\n",
        "# /tmp/mcp_demo/app.py\n",
        "# ------------------------------------------------------------\n",
        "# Streamlit UI pour MCP + Ollama (Llama 3.1) â€” robuste + fallback + logs\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import asyncio\n",
        "import pathlib\n",
        "import textwrap\n",
        "import importlib.util\n",
        "\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# OÃ¹ un Ã©ventuel lanceur externe redirige les logs Streamlit (voir ta Partie 10)\n",
        "LOG_PATH = \"/tmp/mcp_demo/streamlit.log\"\n",
        "\n",
        "# ============================================================\n",
        "# MCP helper (stdio JSON-RPC)\n",
        "# ============================================================\n",
        "class MCPServer:\n",
        "    def __init__(self, name: str, cmd: list[str]):\n",
        "        self.name = name\n",
        "        self.cmd = cmd\n",
        "        self.proc: asyncio.subprocess.Process | None = None\n",
        "\n",
        "    async def start(self):\n",
        "        # Lance le processus serveur MCP (stdio)\n",
        "        self.proc = await asyncio.create_subprocess_exec(\n",
        "            *self.cmd,\n",
        "            stdin=asyncio.subprocess.PIPE,\n",
        "            stdout=asyncio.subprocess.PIPE,\n",
        "            stderr=asyncio.subprocess.PIPE,\n",
        "        )\n",
        "\n",
        "        # Envoie \"initialize\"\n",
        "        init_msg = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 1,\n",
        "            \"method\": \"initialize\",\n",
        "            \"params\": {\n",
        "                \"protocolVersion\": \"2024-11-05\",\n",
        "                \"capabilities\": {},\n",
        "                \"clientInfo\": {\"name\": \"streamlit-app\"},\n",
        "            },\n",
        "        }\n",
        "        assert self.proc.stdin is not None\n",
        "        self.proc.stdin.write((json.dumps(init_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        await asyncio.sleep(0.8)  # petit dÃ©lai pour laisser dÃ©marrer\n",
        "\n",
        "    async def list_tools(self):\n",
        "        if not self.proc or not self.proc.stdin or not self.proc.stdout:\n",
        "            raise RuntimeError(f\"{self.name}: process not started\")\n",
        "\n",
        "        list_msg = {\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/list\"}\n",
        "        self.proc.stdin.write((json.dumps(list_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if not line:\n",
        "            # Essaie de lire stderr pour donner un indice\n",
        "            stderr_tail = \"\"\n",
        "            try:\n",
        "                if self.proc.stderr:\n",
        "                    stderr_tail = (await self.proc.stderr.read(4096)).decode(errors=\"ignore\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            raise RuntimeError(f\"{self.name}: no response to tools/list. Stderr: {stderr_tail[:500]}\")\n",
        "\n",
        "        data = json.loads(line.decode().strip())\n",
        "        return data.get(\"result\", {}).get(\"tools\", [])\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: dict):\n",
        "        if not self.proc or not self.proc.stdin or not self.proc.stdout:\n",
        "            raise RuntimeError(f\"{self.name}: process not started\")\n",
        "\n",
        "        call_msg = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 3,\n",
        "            \"method\": \"tools/call\",\n",
        "            \"params\": {\"name\": tool_name, \"arguments\": arguments},\n",
        "        }\n",
        "        self.proc.stdin.write((json.dumps(call_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if line:\n",
        "            return json.loads(line.decode().strip())\n",
        "        return {\"error\": \"no output from tools/call\"}\n",
        "\n",
        "    async def shutdown(self):\n",
        "        # ArrÃªt doux â†’ puis kill si nÃ©cessaire\n",
        "        if self.proc:\n",
        "            try:\n",
        "                self.proc.terminate()\n",
        "                await asyncio.sleep(0.5)\n",
        "                if self.proc.returncode is None:\n",
        "                    self.proc.kill()\n",
        "                await self.proc.wait()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Boot MCP servers (robuste: binaire OU module) + timeouts\n",
        "# ============================================================\n",
        "ALLOWED_FS_DIR = \"/tmp/mcp_demo/allowed\"\n",
        "pathlib.Path(ALLOWED_FS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _has_module(mod: str) -> bool:\n",
        "    return importlib.util.find_spec(mod) is not None\n",
        "\n",
        "def _bin_or_module(bin_name: str, module_name: str, extra_args: list[str] | None = None) -> list[str]:\n",
        "    \"\"\"\n",
        "    PrÃ©fÃ¨re le binaire s'il est prÃ©sent, sinon lance `python -m module`.\n",
        "    \"\"\"\n",
        "    path = shutil.which(bin_name)\n",
        "    if path:\n",
        "        return [bin_name] + (extra_args or [])\n",
        "    if _has_module(module_name):\n",
        "        return [sys.executable, \"-m\", module_name] + (extra_args or [])\n",
        "    raise RuntimeError(\n",
        "        f\"Ni binaire '{bin_name}' ni module '{module_name}' trouvÃ©.\\n\"\n",
        "        \"â†’ Installe dans ce runtime: pip install -q mcp-server-fetch mcp-server-filesystem mcp\"\n",
        "    )\n",
        "\n",
        "# Commandes robustes (binaire â†’ module)\n",
        "fetch_cmd = _bin_or_module(\"mcp-server-fetch\", \"mcp_server_fetch\")\n",
        "fs_cmd    = _bin_or_module(\"mcp-server-filesystem\", \"mcp_server_filesystem\", [ALLOWED_FS_DIR])\n",
        "\n",
        "# Custom server Python (fichier local)\n",
        "MY_MCP_PATH = \"/tmp/mcp_demo/my_mcp_server.py\"\n",
        "my_cmd = [sys.executable, MY_MCP_PATH] if os.path.exists(MY_MCP_PATH) else [sys.executable, \"-m\", \"runpy\", MY_MCP_PATH]\n",
        "\n",
        "# Instances serveurs\n",
        "fetch_srv = MCPServer(\"fetch\", fetch_cmd)\n",
        "fs_srv    = MCPServer(\"filesystem\", fs_cmd)\n",
        "my_srv    = MCPServer(\"my_mcp\", my_cmd)\n",
        "\n",
        "# Helper: appelle tools/list avec timeout pour Ã©viter de bloquer l'UI\n",
        "async def _list_tools_with_timeout(server: MCPServer, name: str, timeout: float = 6.0):\n",
        "    try:\n",
        "        return await asyncio.wait_for(server.list_tools(), timeout=timeout)\n",
        "    except asyncio.TimeoutError:\n",
        "        return {\"__error__\": f\"{name}: timeout on tools/list (>{timeout}s)\"}\n",
        "    except Exception as e:\n",
        "        return {\"__error__\": f\"{name}: {type(e).__name__}: {e}\"}\n",
        "\n",
        "# DÃ©marrage + collecte des outils (robuste)\n",
        "async def start_all():\n",
        "    # 1) DÃ©marre les 3 serveurs en parallÃ¨le\n",
        "    try:\n",
        "        await asyncio.gather(fetch_srv.start(), fs_srv.start(), my_srv.start())\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Ã‰chec au dÃ©marrage d'un serveur MCP: {e}\")\n",
        "\n",
        "    # 2) RÃ©cupÃ¨re les outils avec timeouts pour ne pas bloquer\n",
        "    fetch_tools = await _list_tools_with_timeout(fetch_srv, \"fetch\")\n",
        "    fs_tools    = await _list_tools_with_timeout(fs_srv, \"filesystem\")\n",
        "    my_tools    = await _list_tools_with_timeout(my_srv, \"my_mcp\")\n",
        "\n",
        "    # 3) AggrÃ¨ge les erreurs Ã©ventuelles\n",
        "    errors = []\n",
        "    for name, tools in ((\"fetch\", fetch_tools), (\"filesystem\", fs_tools), (\"my_mcp\", my_tools)):\n",
        "        if isinstance(tools, dict) and \"__error__\" in tools:\n",
        "            errors.append(tools[\"__error__\"])\n",
        "\n",
        "    if errors:\n",
        "        # On stoppe ici pour afficher lâ€™erreur dans lâ€™UI (pas de spinner infini)\n",
        "        raise RuntimeError(\" / \".join(errors))\n",
        "\n",
        "    # 4) Construit la liste et la map des outils\n",
        "    all_tools = fetch_tools + fs_tools + my_tools\n",
        "    tool_map = {\n",
        "        \"fetch\": {t[\"name\"] for t in fetch_tools},\n",
        "        \"fs\":    {t[\"name\"] for t in fs_tools},\n",
        "        \"my\":    {t[\"name\"] for t in my_tools},\n",
        "    }\n",
        "\n",
        "    if not all_tools:\n",
        "        raise RuntimeError(\"Aucun outil MCP disponible (fetch/filesystem/my_mcp ont renvoyÃ© 0 outil).\")\n",
        "\n",
        "    return all_tools, tool_map\n",
        "\n",
        "# ArrÃªt propre (accessible depuis lâ€™UI)\n",
        "async def stop_all():\n",
        "    await asyncio.gather(fetch_srv.shutdown(), fs_srv.shutdown(), my_srv.shutdown())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# LLM & Orchestrateur (Ollama via OpenAI client-compatible)\n",
        "# ============================================================\n",
        "OLLAMA_BASE = os.environ.get(\"OLLAMA_ENDPOINT\", \"http://localhost:11434/v1\")\n",
        "MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.1\")\n",
        "client = OpenAI(base_url=OLLAMA_BASE, api_key=os.environ.get(\"OLLAMA_API_KEY\", \"ollama\"))\n",
        "\n",
        "async def run_agent(goal: str, all_tools: list[dict], tool_map: dict[str, set[str]]):\n",
        "    SYSTEM_PROMPT = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "        You are an AI assistant with access to tools. You must use JSON format for all responses.\n",
        "\n",
        "        Available tools:\n",
        "        {chr(10).join([f\"- {t['name']}: {t.get('description', 'No description')}\" for t in all_tools])}\n",
        "\n",
        "        Response format:\n",
        "        - To use a tool: {{\"tool\": \"tool_name\", \"arguments\": {{\"arg1\": \"value1\", \"arg2\": \"value2\"}}}}\n",
        "        - To give final answer: {{\"answer\": \"your final answer\"}}\n",
        "\n",
        "        Always think step by step and use the appropriate tools.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": goal},\n",
        "    ]\n",
        "\n",
        "    outputs: list[str] = []\n",
        "    for step in range(8):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL, messages=messages, temperature=0.1, max_tokens=1000\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "\n",
        "            # Tente de parser en JSON\n",
        "            try:\n",
        "                decision = json.loads(content)\n",
        "            except json.JSONDecodeError:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": \"Please respond with valid JSON format only.\"})\n",
        "                outputs.append(f\"âš ï¸ Invalid JSON response: {content}\")\n",
        "                continue\n",
        "\n",
        "            if \"answer\" in decision:\n",
        "                outputs.append(f\"âœ… {decision['answer']}\")\n",
        "                break\n",
        "\n",
        "            if \"tool\" in decision and \"arguments\" in decision:\n",
        "                tool_name = decision[\"tool\"]\n",
        "                arguments = decision[\"arguments\"]\n",
        "\n",
        "                server = None\n",
        "                if tool_name in tool_map[\"fetch\"]:\n",
        "                    server = fetch_srv\n",
        "                elif tool_name in tool_map[\"fs\"]:\n",
        "                    server = fs_srv\n",
        "                elif tool_name in tool_map[\"my\"]:\n",
        "                    server = my_srv\n",
        "\n",
        "                if server:\n",
        "                    result = await server.call_tool(tool_name, arguments)\n",
        "                    observation = str(result)\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
        "                    outputs.append(\n",
        "                        json.dumps(\n",
        "                            {\"step\": step + 1, \"tool\": tool_name, \"arguments\": arguments, \"result\": observation},\n",
        "                            indent=2,\n",
        "                        )\n",
        "                    )\n",
        "                else:\n",
        "                    err = f\"Tool '{tool_name}' not found\"\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Error: {err}\"})\n",
        "                    outputs.append(f\"âŒ {err}\")\n",
        "            else:\n",
        "                err = \"Invalid response format - must contain either 'answer' or 'tool' with 'arguments'\"\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": err})\n",
        "                outputs.append(f\"âŒ {err}\")\n",
        "        except Exception as e:\n",
        "            outputs.append(f\"âŒ Error: {e}\")\n",
        "            break\n",
        "    else:\n",
        "        outputs.append(\"âŒ Maximum steps reached without completing the task.\")\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Streamlit UI\n",
        "# ============================================================\n",
        "st.set_page_config(page_title=\"ðŸ¤– MCP Agent avec Ollama\", layout=\"wide\", initial_sidebar_state=\"expanded\")\n",
        "st.title(\"ðŸ¤– MCP Agent avec Ollama\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "Cette dÃ©mo utilise:\n",
        "- **Ollama** avec Llama 3.1\n",
        "- **Serveurs MCP** (fetch, filesystem, custom)\n",
        "- **Streamlit** pour l'interface\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Boot (avec erreurs visibles)\n",
        "if \"boot_done\" not in st.session_state:\n",
        "    try:\n",
        "        with st.spinner(\"DÃ©marrage des serveurs MCPâ€¦\"):\n",
        "            all_tools, tool_map = asyncio.run(start_all())\n",
        "        st.session_state.boot_done = True\n",
        "        st.session_state.all_tools = all_tools\n",
        "        st.session_state.tool_map = tool_map\n",
        "    except Exception as e:\n",
        "        st.error(f\"âŒ Ã‰chec du dÃ©marrage des serveurs MCP : {e}\")\n",
        "        st.info(\n",
        "            \"VÃ©rifie que les paquets sont installÃ©s (`mcp-server-fetch`, `mcp-server-filesystem`, `mcp`), \"\n",
        "            \"que `/tmp/mcp_demo/allowed` existe, et que `my_mcp_server.py` est accessible.\"\n",
        "        )\n",
        "        st.stop()\n",
        "\n",
        "with st.expander(\"âš™ï¸ Configuration\"):\n",
        "    st.write(\"**Serveurs MCP actifs (process lancÃ©s):**\")\n",
        "    st.write(f\"- ðŸ“¡ Fetch Server: âœ…\")\n",
        "    st.write(f\"- ðŸ“ Filesystem Server: âœ…\")\n",
        "    st.write(f\"- ðŸ› ï¸ Custom Server: âœ…\")\n",
        "    st.write(\"**Outils disponibles:**\")\n",
        "    for tool in st.session_state.all_tools:\n",
        "        st.write(f\"- `{tool['name']}`: {tool.get('description', 'No description')}\")\n",
        "\n",
        "# Zone de saisie & presets\n",
        "goal = st.text_area(\n",
        "    \"ðŸŽ¯ Objectif (en anglais pour de meilleurs rÃ©sultats):\",\n",
        "    height=120,\n",
        "    placeholder=\"Ex: Download https://example.com/data.csv, convert to JSON, summarize, and save to summary.txt\",\n",
        ")\n",
        "\n",
        "c1, c2, c3 = st.columns(3)\n",
        "with c1:\n",
        "    if st.button(\"ðŸ“¥ TÃ©lÃ©charger et convertir\"):\n",
        "        goal = \"Download https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv, convert to JSON, and show first 3 records\"\n",
        "with c2:\n",
        "    if st.button(\"ðŸ“ RÃ©sumer du texte\"):\n",
        "        goal = (\n",
        "            \"Summarize this text: 'Artificial intelligence is transforming many industries. \"\n",
        "            \"Machine learning algorithms can now recognize patterns in data that humans might miss. \"\n",
        "            \"This technology is being used in healthcare, finance, and transportation. \"\n",
        "            \"The future of AI looks promising with continued advancements.'\"\n",
        "        )\n",
        "with c3:\n",
        "    if st.button(\"ðŸ“ Lister les fichiers\"):\n",
        "        goal = \"List files in the current directory and show their contents\"\n",
        "\n",
        "# ExÃ©cution agent\n",
        "if st.button(\"â–¶ï¸ ExÃ©cuter\", type=\"primary\"):\n",
        "    if not goal.strip():\n",
        "        st.warning(\"Veuillez entrer un objectif\")\n",
        "    else:\n",
        "        progress = st.progress(0)\n",
        "        status = st.empty()\n",
        "        out = st.empty()\n",
        "        with st.spinner(\"ExÃ©cutionâ€¦\"):\n",
        "            outputs = asyncio.run(run_agent(goal, st.session_state.all_tools, st.session_state.tool_map))\n",
        "        lines: list[str] = []\n",
        "        steps = max(1, len(outputs))\n",
        "        for i, item in enumerate(outputs):\n",
        "            lines.append(item)\n",
        "            out.code(\"\\n\".join(lines), language=\"json\")\n",
        "            progress.progress(min((i + 1) / steps, 1.0))\n",
        "            status.text(f\"Step {i + 1}/{steps}\")\n",
        "        status.success(\"âœ… TÃ¢che terminÃ©e!\")\n",
        "\n",
        "# Sidebar: outils de debug & nettoyage\n",
        "st.sidebar.markdown(\"### ðŸªµ Logs & Nettoyage\")\n",
        "\n",
        "# Bouton \"Voir les logs\" â€” affiche streamlit.log si prÃ©sent\n",
        "if st.sidebar.button(\"Voir les logs\"):\n",
        "    if os.path.exists(LOG_PATH):\n",
        "        try:\n",
        "            with open(LOG_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                content = f.read()\n",
        "            st.sidebar.success(f\"Ouverture de {LOG_PATH}\")\n",
        "            # On affiche dans la zone principale pour avoir plus d'espace\n",
        "            st.subheader(\"ðŸªµ Contenu de streamlit.log\")\n",
        "            st.code(content[-20000:] or \"(log vide)\", language=\"bash\")\n",
        "        except Exception as e:\n",
        "            st.sidebar.error(f\"Impossible de lire {LOG_PATH} : {e}\")\n",
        "    else:\n",
        "        st.sidebar.warning(f\"Fichier de log introuvable : {LOG_PATH}\\n\"\n",
        "                           \"âž¡ï¸ Lance l'app via une cellule qui redirige stdout/err vers ce fichier.\")\n",
        "\n",
        "if st.sidebar.button(\"ArrÃªter les serveurs MCP\"):\n",
        "    asyncio.run(stop_all())\n",
        "    st.sidebar.success(\"Serveurs arrÃªtÃ©s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJn3Dq6FFSCH",
        "outputId": "ced82e84-45f4-45b8-aa4c-4bb796ed5f62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/mcp_demo/my_mcp_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 4 â€“ Clone des serveurs tiers (dossiers & fichier exemple)"
      ],
      "metadata": {
        "id": "cIRAOIrSHCGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''#@title 4ï¸âƒ£ PrÃ©paration des dossiers\n",
        "import pathlib\n",
        "BASE = pathlib.Path(\"/tmp/mcp_demo\")\n",
        "BASE.mkdir(exist_ok=True)\n",
        "ALLOWED_FS = BASE / \"allowed\"\n",
        "ALLOWED_FS.mkdir(exist_ok=True)\n",
        "\n",
        "with open(ALLOWED_FS / \"example.txt\", \"w\") as f:\n",
        "    f.write(\"Ceci est un fichier d'exemple\\nLigne 2\\nLigne 3\")\n",
        "print(\"âœ… example.txt crÃ©Ã©\")'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B6YenbJyHB-x",
        "outputId": "35252e6b-9ad6-4174-9d04-90a814ce72a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#@title 4ï¸âƒ£ PrÃ©paration des dossiers\\nimport pathlib\\nBASE = pathlib.Path(\"/tmp/mcp_demo\")\\nBASE.mkdir(exist_ok=True)\\nALLOWED_FS = BASE / \"allowed\"\\nALLOWED_FS.mkdir(exist_ok=True)\\n\\nwith open(ALLOWED_FS / \"example.txt\", \"w\") as f:\\n    f.write(\"Ceci est un fichier d\\'exemple\\nLigne 2\\nLigne 3\")\\nprint(\"âœ… example.txt crÃ©Ã©\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 5 â€“ Helper MCPServer (inchangÃ© + robuste)"
      ],
      "metadata": {
        "id": "GljcyWw4HGg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§© PARTIE 5 â€” Classe helper pour les serveurs MCP (corrigÃ©e)\n",
        "# Lit et \"Ã©carte\" la rÃ©ponse Ã  `initialize` avant d'envoyer d'autres requÃªtes,\n",
        "# pour Ã©viter que tools/list rÃ©cupÃ¨re la mauvaise ligne.\n",
        "\n",
        "import asyncio, json, sys\n",
        "from typing import Callable, Any, Dict, Optional\n",
        "\n",
        "class MCPServer:\n",
        "    def __init__(self, name: str, cmd: list[str]):\n",
        "        self.name = name\n",
        "        self.cmd = cmd\n",
        "        self.proc: Optional[asyncio.subprocess.Process] = None\n",
        "        self._msg_id = 0\n",
        "\n",
        "    # --- utilitaires internes ---\n",
        "    def _next_id(self) -> int:\n",
        "        self._msg_id += 1\n",
        "        return self._msg_id\n",
        "\n",
        "    async def _write(self, payload: Dict[str, Any]) -> None:\n",
        "        \"\"\"Ã‰crit une ligne JSON sur stdin du serveur MCP.\"\"\"\n",
        "        if not self.proc or not self.proc.stdin:\n",
        "            raise RuntimeError(f\"[{self.name}] Process not started\")\n",
        "        line = json.dumps(payload, ensure_ascii=False) + \"\\n\"\n",
        "        self.proc.stdin.write(line.encode(\"utf-8\"))\n",
        "        await self.proc.stdin.drain()\n",
        "\n",
        "    async def _read_until(\n",
        "        self,\n",
        "        predicate: Callable[[Dict[str, Any]], bool],\n",
        "        timeout: float = 15.0,\n",
        "        max_lines: int = 200,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Lit des lignes JSON depuis stdout jusqu'Ã  ce que `predicate(obj)` soit True.\n",
        "        Ignore silencieusement les lignes non-JSON (logs). Timeout sÃ©curisÃ©.\n",
        "        \"\"\"\n",
        "        if not self.proc or not self.proc.stdout:\n",
        "            raise RuntimeError(f\"[{self.name}] Process not started\")\n",
        "\n",
        "        lines = 0\n",
        "        while lines < max_lines:\n",
        "            try:\n",
        "                raw = await asyncio.wait_for(self.proc.stdout.readline(), timeout=timeout)\n",
        "            except asyncio.TimeoutError:\n",
        "                raise asyncio.TimeoutError(f\"[{self.name}] Timeout lecture (>{timeout}s)\")\n",
        "            if not raw:\n",
        "                raise RuntimeError(f\"[{self.name}] EOF sur stdout (process arrÃªtÃ© ?)\")\n",
        "            text = raw.decode(\"utf-8\", errors=\"replace\").strip()\n",
        "            lines += 1\n",
        "            try:\n",
        "                obj = json.loads(text)\n",
        "            except json.JSONDecodeError:\n",
        "                # Certains serveurs Ã©crivent des logs â†’ on ignore ce qui n'est pas JSON\n",
        "                continue\n",
        "            if predicate(obj):\n",
        "                return obj\n",
        "        raise RuntimeError(f\"[{self.name}] Trop de lignes sans match (>{max_lines})\")\n",
        "\n",
        "    # --- cycle de vie ---\n",
        "    async def start(self) -> None:\n",
        "        \"\"\"DÃ©marre le processus MCP et traite la sÃ©quence initialize â†’ rÃ©ponse.\"\"\"\n",
        "        print(f\"[{self.name}] DÃ©marrage â†’ {' '.join(self.cmd)}\")\n",
        "        self.proc = await asyncio.create_subprocess_exec(\n",
        "            *self.cmd,\n",
        "            stdin=asyncio.subprocess.PIPE,\n",
        "            stdout=asyncio.subprocess.PIPE,\n",
        "            stderr=asyncio.subprocess.PIPE,  # on ne lit pas stderr ici (logs)\n",
        "        )\n",
        "\n",
        "        # Envoie initialize\n",
        "        init_id = self._next_id()\n",
        "        init_msg = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": init_id,\n",
        "            \"method\": \"initialize\",\n",
        "            \"params\": {\n",
        "                \"protocolVersion\": \"2024-11-05\",\n",
        "                \"capabilities\": {},\n",
        "                \"clientInfo\": {\"name\": \"colab-client\"},\n",
        "            },\n",
        "        }\n",
        "        await self._write(init_msg)\n",
        "\n",
        "        # Lit (et \"Ã©carte\") la rÃ©ponse d'init AVANT toute autre requÃªte\n",
        "        def _is_init(resp: Dict[str, Any]) -> bool:\n",
        "            return resp.get(\"id\") == init_id and \"result\" in resp\n",
        "\n",
        "        await self._read_until(_is_init, timeout=15.0)\n",
        "        print(f\"[{self.name}] âœ… InitialisÃ©\")\n",
        "\n",
        "    async def shutdown(self) -> None:\n",
        "        \"\"\"ArrÃªte proprement le processus MCP.\"\"\"\n",
        "        if self.proc:\n",
        "            try:\n",
        "                self.proc.terminate()\n",
        "                await asyncio.sleep(0.7)\n",
        "                if self.proc.returncode is None:\n",
        "                    self.proc.kill()\n",
        "                await self.proc.wait()\n",
        "            except Exception as e:\n",
        "                print(f\"[{self.name}] Erreur arrÃªt: {e}\", file=sys.stderr)\n",
        "\n",
        "    # --- appels JSON-RPC ---\n",
        "    async def list_tools(self) -> list[Dict[str, Any]]:\n",
        "        \"\"\"Appelle tools/list et renvoie la liste des outils.\"\"\"\n",
        "        if not self.proc:\n",
        "            raise RuntimeError(f\"[{self.name}] Process not started\")\n",
        "\n",
        "        req_id = self._next_id()\n",
        "        req = {\"jsonrpc\": \"2.0\", \"id\": req_id, \"method\": \"tools/list\"}\n",
        "        await self._write(req)\n",
        "\n",
        "        def _is_tools(resp: Dict[str, Any]) -> bool:\n",
        "            # On privilÃ©gie le match par id, mais on tolÃ¨re les serveurs qui ne renvoient pas l'id.\n",
        "            if resp.get(\"id\") == req_id and \"result\" in resp:\n",
        "                tools = resp[\"result\"].get(\"tools\")\n",
        "                return isinstance(tools, list)\n",
        "            if \"result\" in resp and isinstance(resp[\"result\"].get(\"tools\"), list):\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        resp = await self._read_until(_is_tools, timeout=15.0)\n",
        "        return resp.get(\"result\", {}).get(\"tools\", []) or []\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Appelle tools/call avec {name, arguments} et renvoie la rÃ©ponse brute (result/error).\"\"\"\n",
        "        if not self.proc:\n",
        "            return {\"error\": \"Process not started\"}\n",
        "\n",
        "        req_id = self._next_id()\n",
        "        req = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": req_id,\n",
        "            \"method\": \"tools/call\",\n",
        "            \"params\": {\"name\": tool_name, \"arguments\": arguments},\n",
        "        }\n",
        "        await self._write(req)\n",
        "\n",
        "        def _is_call(resp: Dict[str, Any]) -> bool:\n",
        "            return resp.get(\"id\") == req_id and (\"result\" in resp or \"error\" in resp)\n",
        "\n",
        "        try:\n",
        "            resp = await self._read_until(_is_call, timeout=35.0)\n",
        "        except asyncio.TimeoutError:\n",
        "            return {\"error\": f\"Timeout calling tool {tool_name}\"}\n",
        "        return resp\n"
      ],
      "metadata": {
        "id": "ApZi1zm3HGY7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 6 â€“ DÃ©marrage des serveurs MCP + mÃ©morisation des outils"
      ],
      "metadata": {
        "id": "y7oto4K8HKI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§¹ PARTIE 6-fix â€” Purge de mcp-server-fetch pour forcer le fallback\n",
        "import sys, os, shutil, subprocess\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"mcp-server-fetch\"], check=False)\n",
        "bin_path = shutil.which(\"mcp-server-fetch\")\n",
        "if bin_path:\n",
        "    try:\n",
        "        os.remove(bin_path)\n",
        "        print(\"ðŸ—‘ï¸  SupprimÃ©:\", bin_path)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸  Impossible de supprimer\", bin_path, \"â†’\", e)\n",
        "else:\n",
        "    print(\"âœ… Aucun binaire mcp-server-fetch actif dans le PATH.\")\n",
        "os.environ[\"MCP_FORCE_FETCH_FALLBACK\"] = \"1\"\n",
        "print(\"âœ… Purge terminÃ©e. Relance maintenant la PARTIE 6 v3.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aelDXYhDhCNd",
        "outputId": "1c01bd88-4194-4c2f-ae77-62f659cd56e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Aucun binaire mcp-server-fetch actif dans le PATH.\n",
            "âœ… Purge terminÃ©e. Relance maintenant la PARTIE 6 v3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§© PARTIE 6 v3 â€” DÃ©marrage MCP robuste (timeouts stricts + fallbacks fetch/fs/my)\n",
        "# NÃ©cessite la PARTIE 5 corrigÃ©e (classe MCPServer qui lit la rÃ©ponse initialize)\n",
        "\n",
        "import sys, os, asyncio, importlib.util, pathlib, subprocess, json, textwrap\n",
        "\n",
        "# --- prÃ©requis: PARTIE 5 doit avoir dÃ©fini MCPServer ---\n",
        "if \"MCPServer\" not in globals():\n",
        "    raise RuntimeError(\"La PARTIE 5 (MCPServer corrigÃ©e) doit Ãªtre exÃ©cutÃ©e avant la PARTIE 6 v3.\")\n",
        "\n",
        "# --- constantes & chemins ---\n",
        "BASE_DIR = \"/tmp/mcp_demo\"\n",
        "ALLOWED_FS_DIR = f\"{BASE_DIR}/allowed\"\n",
        "pathlib.Path(ALLOWED_FS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FS_FALLBACK_PATH   = f\"{BASE_DIR}/fs_mcp_server.py\"\n",
        "FETCH_FALLBACK_PATH= f\"{BASE_DIR}/fetch_mcp_server.py\"\n",
        "MY_FALLBACK_PATH   = f\"{BASE_DIR}/my_mcp_fallback.py\"\n",
        "MY_MCP_PATH        = f\"{BASE_DIR}/my_mcp_server.py\"  # de la PARTIE 3\n",
        "\n",
        "# --- timeouts stricts ---\n",
        "START_TIMEOUT_S = float(os.environ.get(\"MCP_START_TIMEOUT_S\", \"6.0\"))\n",
        "LIST_TIMEOUT_S  = float(os.environ.get(\"MCP_LIST_TIMEOUT_S\",  \"5.0\"))\n",
        "\n",
        "# --- utils module / install ---\n",
        "def _has_module(mod: str) -> bool:\n",
        "    return importlib.util.find_spec(mod) is not None\n",
        "\n",
        "def _pip_install(pkg: str) -> bool:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", pkg])\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ pip install Ã©chouÃ© pour {pkg}: {e}\")\n",
        "        return False\n",
        "\n",
        "def _ensure_module(mod: str, pip_name: str | None = None) -> bool:\n",
        "    if _has_module(mod):\n",
        "        return True\n",
        "    if pip_name and _pip_install(pip_name):\n",
        "        return _has_module(mod)\n",
        "    return False\n",
        "\n",
        "# --- Ã©crire fallbacks si besoin ---\n",
        "def _ensure_fs_fallback():\n",
        "    if os.path.exists(FS_FALLBACK_PATH): return\n",
        "    code = r'''\n",
        "#!/usr/bin/env python3\n",
        "import asyncio, json, sys, os, pathlib\n",
        "from typing import Any, Dict\n",
        "\n",
        "ROOT = os.path.abspath(sys.argv[1]) if len(sys.argv) > 1 else os.getcwd()\n",
        "pathlib.Path(ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def safe_join(root: str, rel: str) -> str:\n",
        "    p = os.path.abspath(os.path.join(root, rel))\n",
        "    if not p.startswith(root): raise ValueError(\"Path outside allowed directory\")\n",
        "    return p\n",
        "\n",
        "class FsMCP:\n",
        "    def __init__(self, root: str):\n",
        "        self.root = root\n",
        "        self.tools = [\n",
        "            {\"name\":\"list_dir\",\"description\":\"List files/dirs\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"path\":{\"type\":\"string\"}},\"required\":[]}},\n",
        "            {\"name\":\"read_file\",\"description\":\"Read text file\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"path\":{\"type\":\"string\"}},\"required\":[\"path\"]}},\n",
        "            {\"name\":\"write_file\",\"description\":\"Write text file\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"path\":{\"type\":\"string\"},\"content\":{\"type\":\"string\"}},\"required\":[\"path\",\"content\"]}},\n",
        "        ]\n",
        "\n",
        "    async def handle(self, msg: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        mid = msg.get(\"id\"); method = msg.get(\"method\"); params = msg.get(\"params\", {})\n",
        "        if method == \"initialize\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"tools\":{}},\n",
        "                    \"serverInfo\":{\"name\":\"fs_fallback\",\"version\":\"0.1.0\"}}}\n",
        "        if method == \"tools/list\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"tools\": self.tools}}\n",
        "        if method == \"tools/call\":\n",
        "            name = params.get(\"name\"); args = params.get(\"arguments\", {})\n",
        "            try:\n",
        "                out = await self.call(name, args)\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"content\":[{\"type\":\"text\",\"text\":out}]}}\n",
        "            except Exception as e:\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32603,\"message\":str(e)}}\n",
        "        return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32601,\"message\":f\"Unknown method {method}\"}}\n",
        "\n",
        "    async def call(self, name: str, args: Dict[str, Any]) -> str:\n",
        "        if name == \"list_dir\":\n",
        "            rel = args.get(\"path\",\".\"); p = safe_join(self.root, rel)\n",
        "            if not os.path.isdir(p): return f\"Not a directory: {rel}\"\n",
        "            items = []\n",
        "            for e in sorted(os.listdir(p)):\n",
        "                t = \"dir\" if os.path.isdir(os.path.join(p,e)) else \"file\"\n",
        "                items.append(f\"{e} ({t})\")\n",
        "            return \"Directory listing for \"+rel+\":\\n\"+\"\\n\".join(items)\n",
        "        if name == \"read_file\":\n",
        "            rel = args[\"path\"]; p = safe_join(self.root, rel)\n",
        "            if not os.path.exists(p): return f\"File not found: {rel}\"\n",
        "            if os.path.isdir(p): return f\"Is a directory: {rel}\"\n",
        "            with open(p,\"r\",encoding=\"utf-8\",errors=\"replace\") as f: data=f.read()\n",
        "            return f\"=== {rel} ===\\n{data}\"\n",
        "        if name == \"write_file\":\n",
        "            rel = args[\"path\"]; content = args[\"content\"]\n",
        "            p = safe_join(self.root, rel); os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "            with open(p,\"w\",encoding=\"utf-8\") as f: f.write(content)\n",
        "            return f\"Wrote {len(content)} bytes to {rel}\"\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "async def main():\n",
        "    srv = FsMCP(ROOT)\n",
        "    loop = asyncio.get_event_loop()\n",
        "    while True:\n",
        "        line = await loop.run_in_executor(None, sys.stdin.readline)\n",
        "        if not line: break\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try: msg = json.loads(line)\n",
        "        except Exception: continue\n",
        "        resp = await srv.handle(msg)\n",
        "        print(json.dumps(resp), flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "    pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    with open(FS_FALLBACK_PATH, \"w\") as f: f.write(code)\n",
        "    os.chmod(FS_FALLBACK_PATH, 0o755)\n",
        "\n",
        "def _ensure_fetch_fallback():\n",
        "    if os.path.exists(FETCH_FALLBACK_PATH): return\n",
        "    code = r'''\n",
        "#!/usr/bin/env python3\n",
        "import asyncio, json, sys, urllib.request, urllib.error, os, pathlib\n",
        "from typing import Any, Dict\n",
        "ALLOWED = os.environ.get(\"ALLOWED_FS_DIR\") or \"/tmp/mcp_demo/allowed\"\n",
        "pathlib.Path(ALLOWED).mkdir(parents=True, exist_ok=True)\n",
        "def safe_join(root: str, rel: str) -> str:\n",
        "    p = os.path.abspath(os.path.join(root, rel))\n",
        "    if not p.startswith(root): raise ValueError(\"Path outside allowed directory\")\n",
        "    return p\n",
        "class FetchMCP:\n",
        "    def __init__(self):\n",
        "        self.tools = [\n",
        "            {\"name\":\"http_get\",\"description\":\"GET a URL (status+snippet)\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\"},\"max_bytes\":{\"type\":\"integer\",\"default\":20000}},\"required\":[\"url\"]}},\n",
        "            {\"name\":\"download_file\",\"description\":\"Download URL to allowed dir\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\"},\"path\":{\"type\":\"string\"}},\"required\":[\"url\",\"path\"]}},\n",
        "        ]\n",
        "    async def handle(self, msg: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        mid = msg.get(\"id\"); method = msg.get(\"method\"); params = msg.get(\"params\", {})\n",
        "        if method == \"initialize\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"tools\":{}},\n",
        "                    \"serverInfo\":{\"name\":\"fetch_fallback\",\"version\":\"0.1.0\"}}}\n",
        "        if method == \"tools/list\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"tools\": self.tools}}\n",
        "        if method == \"tools/call\":\n",
        "            name = params.get(\"name\"); args = params.get(\"arguments\", {})\n",
        "            try:\n",
        "                out = await self.call(name, args)\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"content\":[{\"type\":\"text\",\"text\":out}]}}\n",
        "            except Exception as e:\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32603,\"message\":str(e)}}\n",
        "        return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32601,\"message\":f\"Unknown method {method}\"}}\n",
        "    async def call(self, name: str, args: Dict[str, Any]) -> str:\n",
        "        if name == \"http_get\":\n",
        "            url = args[\"url\"]; max_bytes = int(args.get(\"max_bytes\", 20000))\n",
        "            req = urllib.request.Request(url, headers={\"User-Agent\":\"MCP-Fetch/0.1\"})\n",
        "            with urllib.request.urlopen(req, timeout=20) as r:\n",
        "                status = r.status; raw = r.read(max_bytes)\n",
        "            snippet = raw.decode(\"utf-8\", errors=\"replace\")\n",
        "            return f\"HTTP {status}\\n\\n{snippet}\"\n",
        "        if name == \"download_file\":\n",
        "            url = args[\"url\"]; rel = args[\"path\"]\n",
        "            dst = safe_join(ALLOWED, rel); os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "            req = urllib.request.Request(url, headers={\"User-Agent\":\"MCP-Fetch/0.1\"})\n",
        "            with urllib.request.urlopen(req, timeout=60) as r, open(dst, \"wb\") as f:\n",
        "                total = 0\n",
        "                while True:\n",
        "                    chunk = r.read(65536)\n",
        "                    if not chunk: break\n",
        "                    f.write(chunk); total += len(chunk)\n",
        "            return f\"Downloaded {total} bytes to {os.path.relpath(dst, ALLOWED)}\"\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "async def main():\n",
        "    srv = FetchMCP()\n",
        "    loop = asyncio.get_event_loop()\n",
        "    while True:\n",
        "        line = await loop.run_in_executor(None, sys.stdin.readline)\n",
        "        if not line: break\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try: msg = json.loads(line)\n",
        "        except Exception: continue\n",
        "        resp = await srv.handle(msg)\n",
        "        print(json.dumps(resp), flush=True)\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "    pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    with open(FETCH_FALLBACK_PATH, \"w\") as f: f.write(code)\n",
        "    os.chmod(FETCH_FALLBACK_PATH, 0o755)\n",
        "\n",
        "def _ensure_my_fallback():\n",
        "    if os.path.exists(MY_FALLBACK_PATH): return\n",
        "    code = r'''\n",
        "#!/usr/bin/env python3\n",
        "import asyncio, json, sys, datetime, re\n",
        "\n",
        "class MyFallback:\n",
        "    def __init__(self):\n",
        "        self.tools = [\n",
        "            {\"name\":\"get_current_time\",\"description\":\"Get the current time and date\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{},\"required\":[]}},\n",
        "            {\"name\":\"calculate\",\"description\":\"Perform basic math\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"expression\":{\"type\":\"string\"}},\"required\":[\"expression\"]}},\n",
        "            {\"name\":\"text_stats\",\"description\":\"Text statistics\",\n",
        "             \"inputSchema\":{\"type\":\"object\",\"properties\":{\"text\":{\"type\":\"string\"}},\"required\":[\"text\"]}},\n",
        "        ]\n",
        "    async def handle(self, msg):\n",
        "        mid = msg.get(\"id\"); method = msg.get(\"method\"); params = msg.get(\"params\", {})\n",
        "        if method == \"initialize\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"tools\":{}},\n",
        "                    \"serverInfo\":{\"name\":\"my_fallback\",\"version\":\"0.1.0\"}}}\n",
        "        if method == \"tools/list\":\n",
        "            return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"tools\": self.tools}}\n",
        "        if method == \"tools/call\":\n",
        "            name = params.get(\"name\"); args = params.get(\"arguments\", {})\n",
        "            try:\n",
        "                out = await self.call(name, args)\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"result\":{\"content\":[{\"type\":\"text\",\"text\":out}]}}\n",
        "            except Exception as e:\n",
        "                return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32603,\"message\":str(e)}}\n",
        "        return {\"jsonrpc\":\"2.0\",\"id\":mid,\"error\":{\"code\":-32601,\"message\":f\"Unknown method {method}\"}}\n",
        "    async def call(self, name, args):\n",
        "        if name == \"get_current_time\":\n",
        "            return \"Current time: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        if name == \"calculate\":\n",
        "            expr = str(args.get(\"expression\",\"\"))\n",
        "            # Autoriser chiffres, opÃ©rateurs simples, espaces, parenthÃ¨ses, points\n",
        "            if not re.fullmatch(r\"[0-9+\\-*/().,\\s]+\", expr):\n",
        "                return \"Error: Expression contains invalid characters\"\n",
        "            try:\n",
        "                res = eval(expr, {\"__builtins__\": {}}, {})\n",
        "            except Exception as e:\n",
        "                return f\"Error calculating '{expr}': {e}\"\n",
        "            return f\"Result of '{expr}' = {res}\"\n",
        "        if name == \"text_stats\":\n",
        "            text = str(args.get(\"text\",\"\"))\n",
        "            words = len(text.split())\n",
        "            chars = len(text)\n",
        "            nosp  = len(text.replace(\" \",\"\"))\n",
        "            lines = len(text.splitlines()) or (1 if text else 0)\n",
        "            return f\"Text statistics:\\n- Characters: {chars}\\n- Characters (no spaces): {nosp}\\n- Words: {words}\\n- Lines: {lines}\"\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "async def main():\n",
        "    srv = MyFallback()\n",
        "    loop = asyncio.get_event_loop()\n",
        "    while True:\n",
        "        line = await loop.run_in_executor(None, sys.stdin.readline)\n",
        "        if not line: break\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try: msg = json.loads(line)\n",
        "        except Exception: continue\n",
        "        resp = await srv.handle(msg)\n",
        "        print(json.dumps(resp), flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "    pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    with open(MY_FALLBACK_PATH, \"w\") as f: f.write(code)\n",
        "    os.chmod(MY_FALLBACK_PATH, 0o755)\n",
        "\n",
        "# --- fabrique commandes primaires/fallback ---\n",
        "def _fs_cmd_primary():\n",
        "    if _ensure_module(\"mcp_server_filesystem\", \"mcp-server-filesystem\"):\n",
        "        return [sys.executable, \"-m\", \"mcp_server_filesystem\", ALLOWED_FS_DIR]\n",
        "    return None\n",
        "\n",
        "def _fs_cmd_fallback():\n",
        "    _ensure_fs_fallback()\n",
        "    return [sys.executable, FS_FALLBACK_PATH, ALLOWED_FS_DIR]\n",
        "\n",
        "def _fetch_cmd_primary():\n",
        "    if _ensure_module(\"mcp_server_fetch\", \"mcp-server-fetch\"):\n",
        "        return [sys.executable, \"-m\", \"mcp_server_fetch\"]\n",
        "    return None\n",
        "\n",
        "def _fetch_cmd_fallback():\n",
        "    _ensure_fetch_fallback()\n",
        "    return [sys.executable, FETCH_FALLBACK_PATH]\n",
        "\n",
        "def _my_cmd_primary():\n",
        "    return [sys.executable, MY_MCP_PATH] if os.path.exists(MY_MCP_PATH) else None\n",
        "\n",
        "def _my_cmd_fallback():\n",
        "    _ensure_my_fallback()\n",
        "    return [sys.executable, MY_FALLBACK_PATH]\n",
        "\n",
        "# --- instanciation globale des serveurs (remplacÃ©s si fallback) ---\n",
        "fetch_srv = None\n",
        "fs_srv    = None\n",
        "my_srv    = None\n",
        "os.environ[\"ALLOWED_FS_DIR\"] = ALLOWED_FS_DIR  # utile pour fetch fallback\n",
        "\n",
        "# --- helpers stricts ---\n",
        "async def _start_with_timeout(server: MCPServer, name: str, timeout: float) -> bool:\n",
        "    try:\n",
        "        await asyncio.wait_for(server.start(), timeout=timeout)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ start {name}: {e}\")\n",
        "        return False\n",
        "\n",
        "async def _tools_with_timeout(server: MCPServer, name: str, timeout: float):\n",
        "    try:\n",
        "        return await asyncio.wait_for(server.list_tools(), timeout=timeout)\n",
        "    except Exception as e:\n",
        "        return {\"__error__\": f\"{name}: {type(e).__name__}: {e}\"}\n",
        "\n",
        "def _tools_ok(tools) -> bool:\n",
        "    return isinstance(tools, list) and len(tools) > 0\n",
        "\n",
        "# --- pipeline start_all v3 ---\n",
        "async def start_all_servers_v3():\n",
        "    global fetch_srv, fs_srv, my_srv\n",
        "\n",
        "    # FETCH\n",
        "    primary = _fetch_cmd_primary()\n",
        "    fetch_srv = MCPServer(\"fetch\", primary or _fetch_cmd_fallback())\n",
        "    ok = await _start_with_timeout(fetch_srv, \"fetch-primary\" if primary else \"fetch-fallback\", START_TIMEOUT_S)\n",
        "    if not ok and primary:\n",
        "        print(\"âš ï¸ bascule fetch â†’ fallback\")\n",
        "        fetch_srv = MCPServer(\"fetch\", _fetch_cmd_fallback())\n",
        "        if not await _start_with_timeout(fetch_srv, \"fetch-fallback\", START_TIMEOUT_S):\n",
        "            raise RuntimeError(\"fetch: primaire et fallback ont Ã©chouÃ© au dÃ©marrage\")\n",
        "\n",
        "    # FS\n",
        "    primary = _fs_cmd_primary()\n",
        "    fs_srv = MCPServer(\"filesystem\", primary or _fs_cmd_fallback())\n",
        "    ok = await _start_with_timeout(fs_srv, \"fs-primary\" if primary else \"fs-fallback\", START_TIMEOUT_S)\n",
        "    if not ok and primary:\n",
        "        print(\"âš ï¸ bascule filesystem â†’ fallback\")\n",
        "        fs_srv = MCPServer(\"filesystem\", _fs_cmd_fallback())\n",
        "        if not await _start_with_timeout(fs_srv, \"fs-fallback\", START_TIMEOUT_S):\n",
        "            raise RuntimeError(\"filesystem: primaire et fallback ont Ã©chouÃ© au dÃ©marrage\")\n",
        "\n",
        "    # MY\n",
        "    primary = _my_cmd_primary()\n",
        "    my_srv = MCPServer(\"my_mcp\", primary or _my_cmd_fallback())\n",
        "    ok = await _start_with_timeout(my_srv, \"my-primary\" if primary else \"my-fallback\", START_TIMEOUT_S)\n",
        "    if not ok and primary:\n",
        "        print(\"âš ï¸ bascule my_mcp â†’ fallback\")\n",
        "        my_srv = MCPServer(\"my_mcp\", _my_cmd_fallback())\n",
        "        if not await _start_with_timeout(my_srv, \"my-fallback\", START_TIMEOUT_S):\n",
        "            raise RuntimeError(\"my_mcp: primaire et fallback ont Ã©chouÃ© au dÃ©marrage\")\n",
        "\n",
        "    # LIST TOOLS (strict)\n",
        "    fetch_tools = await _tools_with_timeout(fetch_srv, \"fetch\", LIST_TIMEOUT_S)\n",
        "    if (isinstance(fetch_tools, dict) and \"__error__\" in fetch_tools) or not _tools_ok(fetch_tools):\n",
        "        # dernier recours: re-bascule fetch->fallback puis relist\n",
        "        print(\"âš ï¸ fetch tools/list KO â†’ re-bascule sur fallback\")\n",
        "        try: await fetch_srv.shutdown()\n",
        "        except: pass\n",
        "        fetch_srv = MCPServer(\"fetch\", _fetch_cmd_fallback())\n",
        "        if await _start_with_timeout(fetch_srv, \"fetch-fallback\", START_TIMEOUT_S):\n",
        "            fetch_tools = await _tools_with_timeout(fetch_srv, \"fetch\", LIST_TIMEOUT_S)\n",
        "\n",
        "    fs_tools  = await _tools_with_timeout(fs_srv,  \"filesystem\", LIST_TIMEOUT_S)\n",
        "    my_tools  = await _tools_with_timeout(my_srv,  \"my_mcp\",     LIST_TIMEOUT_S)\n",
        "\n",
        "    # agrÃ©gation\n",
        "    all_tools = []\n",
        "    tool_map = {\"fetch\": set(), \"fs\": set(), \"my\": set()}\n",
        "    errors = []\n",
        "\n",
        "    for name, tools, key in ((\"fetch\", fetch_tools, \"fetch\"), (\"filesystem\", fs_tools, \"fs\"), (\"my_mcp\", my_tools, \"my\")):\n",
        "        if isinstance(tools, list) and tools:\n",
        "            all_tools += tools\n",
        "            tool_map[key] = {t[\"name\"] for t in tools}\n",
        "            print(f\"âœ… {name}: {len(tools)} outil(s)\")\n",
        "        else:\n",
        "            msg = tools.get(\"__error__\") if isinstance(tools, dict) else f\"{name}: aucun outil\"\n",
        "            errors.append(msg)\n",
        "            print(f\"âš ï¸ {msg}\")\n",
        "\n",
        "    if not all_tools:\n",
        "        raise RuntimeError(\"Aucun outil MCP disponible. DÃ©tails: \" + \" | \".join(errors))\n",
        "\n",
        "    print(f\"âœ… Total outils MCP: {len(all_tools)}\")\n",
        "    return all_tools, tool_map\n",
        "\n",
        "async def stop_all_servers_v3():\n",
        "    tasks = []\n",
        "    for srv in (fetch_srv, fs_srv, my_srv):\n",
        "        if srv:\n",
        "            tasks.append(srv.shutdown())\n",
        "    if tasks:\n",
        "        await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "# --- API publique utilisÃ©e par les autres parties ---\n",
        "async def start_all(): return await start_all_servers_v3()\n",
        "async def stop_all():  await stop_all_servers_v3()\n",
        "\n",
        "print(f\"âœ… PARTIE 6 v3 chargÃ©e â€” timeouts: start={START_TIMEOUT_S}s, list={LIST_TIMEOUT_S}s; fallbacks actifs (fetch/fs/my).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866jGqtSHJ80",
        "outputId": "aa4b8dbf-28fc-403d-efda-c2a6857d61a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… PARTIE 6 v3 chargÃ©e â€” timeouts: start=6.0s, list=5.0s; fallbacks actifs (fetch/fs/my).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 7 â€“ Client LLM (Ollama)"
      ],
      "metadata": {
        "id": "cBGAe9WqHOVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§© PARTIE 7 â€” Client LLM (Ollama) robuste pour Colab\n",
        "import os, time, socket, json, urllib.request, urllib.error, subprocess\n",
        "from openai import OpenAI\n",
        "\n",
        "# ModÃ¨le : utilise OLLAMA_MODEL si dÃ©fini, sinon un modÃ¨le lÃ©ger par dÃ©faut\n",
        "MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.2:1b\")\n",
        "\n",
        "def wait_for_port(host=\"127.0.0.1\", port=11434, timeout=60):\n",
        "    \"\"\"Attend que le serveur Ollama Ã©coute sur le port donnÃ©.\"\"\"\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            with socket.create_connection((host, port), timeout=2):\n",
        "                return True\n",
        "        except OSError:\n",
        "            time.sleep(1)\n",
        "    return False\n",
        "\n",
        "def ollama_tags():\n",
        "    \"\"\"Retourne le JSON de /api/tags ou {} si indisponible.\"\"\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(\"http://127.0.0.1:11434/api/tags\", timeout=5) as r:\n",
        "            if r.status == 200:\n",
        "                return json.loads(r.read().decode(\"utf-8\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "def have_model(name: str) -> bool:\n",
        "    tags = ollama_tags()\n",
        "    # structure: {\"models\":[{\"name\":\"llama3.2:1b\", ...}, ...]}\n",
        "    models = tags.get(\"models\", [])\n",
        "    return any(m.get(\"name\") == name for m in models)\n",
        "\n",
        "def pull_model(name: str):\n",
        "    print(f\"â¬‡ï¸ Pull du modÃ¨le '{name}' (petit et rapide pour Colab si c'est la 1Ã¨re fois)â€¦\")\n",
        "    try:\n",
        "        subprocess.check_call([\"ollama\", \"pull\", name])\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise RuntimeError(f\"Ã‰chec du pull du modÃ¨le '{name}': {e}\")\n",
        "\n",
        "# 1) VÃ©rifie que le serveur Ollama rÃ©pond (PARTIE 1 est censÃ©e l'avoir lancÃ©)\n",
        "if not wait_for_port():\n",
        "    raise RuntimeError(\"âŒ Ollama ne rÃ©pond pas sur 127.0.0.1:11434. Lance la PARTIE 1 avant.\")\n",
        "\n",
        "# 2) VÃ©rifie/installe le modÃ¨le demandÃ©\n",
        "if not have_model(MODEL):\n",
        "    pull_model(MODEL)\n",
        "    # Re-vÃ©rifie\n",
        "    if not have_model(MODEL):\n",
        "        raise RuntimeError(f\"âŒ Le modÃ¨le '{MODEL}' n'apparaÃ®t pas dans /api/tags aprÃ¨s le pull.\")\n",
        "\n",
        "# 3) Client OpenAI (API compatible Ollama)\n",
        "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "\n",
        "# 4) Test de la connexion + complÃ©tion (retries)\n",
        "last_err = None\n",
        "for attempt in range(3):\n",
        "    try:\n",
        "        _ = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "            max_tokens=8\n",
        "        )\n",
        "        print(\"âœ… ConnectÃ© Ã  Ollama avec succÃ¨s\")\n",
        "        print(f\"ðŸ“‹ ModÃ¨le utilisÃ©: {MODEL}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "        if attempt < 2:\n",
        "            print(f\"â³ Retry connexion Ollama ({attempt+1}/3)â€¦\")\n",
        "            time.sleep(3)\n",
        "        else:\n",
        "            print(f\"âŒ Erreur de connexion Ã  Ollama: {e}\\n\"\n",
        "                  f\"   â€¢ VÃ©rifie que 'ollama serve' tourne (PARTIE 1)\\n\"\n",
        "                  f\"   â€¢ VÃ©rifie que le modÃ¨le existe: !ollama list\\n\"\n",
        "                  f\"   â€¢ Tu peux changer le modÃ¨le via: os.environ['OLLAMA_MODEL'] = 'llama3.2:1b' (ou autre)\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "on_Cidz6HONF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9409b5-3b6a-465f-dd75-0db1828d0e39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ConnectÃ© Ã  Ollama avec succÃ¨s\n",
            "ðŸ“‹ ModÃ¨le utilisÃ©: llama3.2:1b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 8 â€“ Orchestrateur agentique (+ wrapper collecteur)"
      ],
      "metadata": {
        "id": "JTek6qxrHR4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§© PARTIE 8 â€” Orchestrateur agentique + UI Streamlit (STRICT : pas d'exÃ©cution en bare mode)\n",
        "\n",
        "'''import os\n",
        "\n",
        "#Option d'override (si tu VEUX quand mÃªme faire tourner l'UI dans la cellule) :\n",
        "ALLOW_PART8_UI = os.environ.get(\"ALLOW_PART8_UI\", \"0\") == \"1\"\n",
        "\n",
        "# DÃ©tection du bare mode (exÃ©cution dans une cellule, sans `streamlit run`)\n",
        "#def _is_bare_mode() -> bool:\n",
        "    #try:\n",
        "        from streamlit.runtime.scriptrunner import get_script_run_ctx  # type: ignore\n",
        "        return get_script_run_ctx() is None\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "if _is_bare_mode() and not ALLOW_PART8_UI:\n",
        "    raise SystemExit(\n",
        "        \"â›” Cette PARTIE 8 ne s'exÃ©cute pas en bare mode.\\n\"\n",
        "        \"âž¡ï¸ Utilise la PARTIE 10 pour lancer l'UI Web (streamlit run + tunnel).\\n\"\n",
        "        \"ðŸ’¡ Si tu veux forcer l'UI dans cette cellule (non recommandÃ©), exÃ©cute avant :\\n\"\n",
        "        \"    import os; os.environ['ALLOW_PART8_UI']='1'\\n\"\n",
        "    )\n",
        "\n",
        "# --- Si on arrive ici, c'est que tu as explicitement autorisÃ© l'UI en cellule (ALLOW_PART8_UI=1) ---\n",
        "# On injecte la version UI (la mÃªme logique que prÃ©cÃ©demment), mais **on nâ€™Ã©crit rien**\n",
        "# ici pour Ã©viter de rallonger : tu peux coller lâ€™ancienne UI de la PARTIE 8 si besoin.\n",
        "print(\"âœ… ALLOW_PART8_UI=1 -> tu peux coller ici la version UI de la PARTIE 8 si tu veux tester en cellule.\")'''\n"
      ],
      "metadata": {
        "id": "8MNP_Y6bHRxX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "805d4783-6a36-413b-9d1a-9e19320b8e94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import os\\n\\n#Option d\\'override (si tu VEUX quand mÃªme faire tourner l\\'UI dans la cellule) :\\nALLOW_PART8_UI = os.environ.get(\"ALLOW_PART8_UI\", \"0\") == \"1\"\\n\\n# DÃ©tection du bare mode (exÃ©cution dans une cellule, sans `streamlit run`)\\n#def _is_bare_mode() -> bool:\\n    #try:\\n        from streamlit.runtime.scriptrunner import get_script_run_ctx  # type: ignore\\n        return get_script_run_ctx() is None\\n    except Exception:\\n        return True\\n\\nif _is_bare_mode() and not ALLOW_PART8_UI:\\n    raise SystemExit(\\n        \"â›” Cette PARTIE 8 ne s\\'exÃ©cute pas en bare mode.\\n\"\\n        \"âž¡ï¸ Utilise la PARTIE 10 pour lancer l\\'UI Web (streamlit run + tunnel).\\n\"\\n        \"ðŸ’¡ Si tu veux forcer l\\'UI dans cette cellule (non recommandÃ©), exÃ©cute avant :\\n\"\\n        \"    import os; os.environ[\\'ALLOW_PART8_UI\\']=\\'1\\'\\n\"\\n    )\\n\\n# --- Si on arrive ici, c\\'est que tu as explicitement autorisÃ© l\\'UI en cellule (ALLOW_PART8_UI=1) ---\\n# On injecte la version UI (la mÃªme logique que prÃ©cÃ©demment), mais **on nâ€™Ã©crit rien**\\n# ici pour Ã©viter de rallonger : tu peux coller lâ€™ancienne UI de la PARTIE 8 si besoin.\\nprint(\"âœ… ALLOW_PART8_UI=1 -> tu peux coller ici la version UI de la PARTIE 8 si tu veux tester en cellule.\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 9 â€“ Application Streamlit autonome (exÃ©cutable en Colab)"
      ],
      "metadata": {
        "id": "Hy55tUlJHVl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9ï¸âƒ£ Ã‰crire l'app Streamlit autonome\n",
        "%%writefile /tmp/mcp_demo/app.py\n",
        "import asyncio, json, textwrap, time, subprocess, os\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------- MCP helper (copiÃ© simplifiÃ©) ----------\n",
        "class MCPServer:\n",
        "    def __init__(self, name, cmd):\n",
        "        self.name, self.cmd = name, cmd\n",
        "        self.proc = None\n",
        "\n",
        "    async def start(self):\n",
        "        self.proc = await asyncio.create_subprocess_exec(\n",
        "            *self.cmd,\n",
        "            stdin=asyncio.subprocess.PIPE,\n",
        "            stdout=asyncio.subprocess.PIPE,\n",
        "            stderr=asyncio.subprocess.PIPE\n",
        "        )\n",
        "        init_msg = {\n",
        "            \"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\",\n",
        "            \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}, \"clientInfo\": {\"name\": \"streamlit-app\"}}\n",
        "        }\n",
        "        self.proc.stdin.write((json.dumps(init_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        await asyncio.sleep(0.8)\n",
        "\n",
        "    async def list_tools(self):\n",
        "        list_msg = {\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/list\"}\n",
        "        self.proc.stdin.write((json.dumps(list_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if line:\n",
        "            data = json.loads(line.decode().strip())\n",
        "            return data.get(\"result\", {}).get(\"tools\", [])\n",
        "        return []\n",
        "\n",
        "    async def call_tool(self, tool_name, arguments):\n",
        "        call_msg = {\"jsonrpc\": \"2.0\", \"id\": 3, \"method\": \"tools/call\", \"params\": {\"name\": tool_name, \"arguments\": arguments}}\n",
        "        self.proc.stdin.write((json.dumps(call_msg) + \"\\n\").encode())\n",
        "        await self.proc.stdin.drain()\n",
        "        line = await self.proc.stdout.readline()\n",
        "        if line:\n",
        "            return json.loads(line.decode().strip())\n",
        "        return {\"error\": \"no output\"}\n",
        "\n",
        "    async def shutdown(self):\n",
        "        if self.proc:\n",
        "            try:\n",
        "                self.proc.terminate()\n",
        "                await asyncio.sleep(0.5)\n",
        "                if self.proc.returncode is None:\n",
        "                    self.proc.kill()\n",
        "                await self.proc.wait()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# ---------- Boot MCP servers ----------\n",
        "ALLOWED_FS_DIR = \"/tmp/mcp_demo/allowed\"\n",
        "fetch_srv = MCPServer(\"fetch\", [\"mcp-server-fetch\"])\n",
        "fs_srv    = MCPServer(\"fs\", [\"mcp-server-filesystem\", ALLOWED_FS_DIR])\n",
        "my_srv    = MCPServer(\"my_mcp\", [\"python3\", \"/tmp/mcp_demo/my_mcp_server.py\"])\n",
        "\n",
        "async def start_all():\n",
        "    await asyncio.gather(fetch_srv.start(), fs_srv.start(), my_srv.start())\n",
        "    fetch_tools = await fetch_srv.list_tools()\n",
        "    fs_tools    = await fs_srv.list_tools()\n",
        "    my_tools    = await my_srv.list_tools()\n",
        "    all_tools = fetch_tools + fs_tools + my_tools\n",
        "    tool_map = {\"fetch\": {t[\"name\"] for t in fetch_tools},\n",
        "                \"fs\":    {t[\"name\"] for t in fs_tools},\n",
        "                \"my\":    {t[\"name\"] for t in my_tools}}\n",
        "    return all_tools, tool_map\n",
        "\n",
        "# ---------- LLM & Orchestrator ----------\n",
        "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "MODEL = \"llama3.1\"\n",
        "\n",
        "async def run_agent(goal: str, all_tools, tool_map):\n",
        "    SYSTEM_PROMPT = textwrap.dedent(f\"\"\"\n",
        "    You are an AI assistant with access to tools. You must use JSON format for all responses.\n",
        "\n",
        "    Available tools:\n",
        "    {chr(10).join([f\"- {t['name']}: {t.get('description', 'No description')}\" for t in all_tools])}\n",
        "\n",
        "    Response format:\n",
        "    - To use a tool: {{\"tool\": \"tool_name\", \"arguments\": {{\"arg1\": \"value1\", \"arg2\": \"value2\"}}}}\n",
        "    - To give final answer: {{\"answer\": \"your final answer\"}}\n",
        "\n",
        "    Always think step by step and use the appropriate tools.\n",
        "    \"\"\")\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": goal}]\n",
        "\n",
        "    outputs = []\n",
        "    for step in range(8):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL, messages=messages, temperature=0.1, max_tokens=1000\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "            try:\n",
        "                decision = json.loads(content)\n",
        "            except json.JSONDecodeError:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": \"Please respond with valid JSON format only.\"})\n",
        "                outputs.append(f\"âš ï¸ Invalid JSON response: {content}\")\n",
        "                continue\n",
        "\n",
        "            if \"answer\" in decision:\n",
        "                outputs.append(f\"âœ… {decision['answer']}\")\n",
        "                break\n",
        "\n",
        "            elif \"tool\" in decision and \"arguments\" in decision:\n",
        "                tool_name = decision[\"tool\"]\n",
        "                arguments = decision[\"arguments\"]\n",
        "                server = None\n",
        "                if tool_name in tool_map[\"fetch\"]:\n",
        "                    server = fetch_srv\n",
        "                elif tool_name in tool_map[\"fs\"]:\n",
        "                    server = fs_srv\n",
        "                elif tool_name in tool_map[\"my\"]:\n",
        "                    server = my_srv\n",
        "\n",
        "                if server:\n",
        "                    result = await server.call_tool(tool_name, arguments)\n",
        "                    observation = str(result)\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
        "                    outputs.append(json.dumps({\n",
        "                        \"step\": step + 1, \"tool\": tool_name, \"arguments\": arguments, \"result\": observation\n",
        "                    }, indent=2))\n",
        "                else:\n",
        "                    err = f\"Tool '{tool_name}' not found\"\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                    messages.append({\"role\": \"user\", \"content\": f\"Error: {err}\"})\n",
        "                    outputs.append(f\"âŒ {err}\")\n",
        "            else:\n",
        "                err = \"Invalid response format - must contain either 'answer' or 'tool' with 'arguments'\"\n",
        "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "                messages.append({\"role\": \"user\", \"content\": err})\n",
        "                outputs.append(f\"âŒ {err}\")\n",
        "        except Exception as e:\n",
        "            outputs.append(f\"âŒ Error: {str(e)}\")\n",
        "            break\n",
        "    else:\n",
        "        outputs.append(\"âŒ Maximum steps reached without completing the task.\")\n",
        "    return outputs\n",
        "\n",
        "# ---------- Streamlit UI ----------\n",
        "st.set_page_config(page_title=\"ðŸ¤– MCP Agent avec Ollama\", layout=\"wide\", initial_sidebar_state=\"expanded\")\n",
        "st.title(\"ðŸ¤– MCP Agent avec Ollama\")\n",
        "st.markdown(\"\"\"\n",
        "Cette dÃ©mo utilise:\n",
        "- **Ollama** avec Llama 3.1\n",
        "- **Serveurs MCP** (fetch, filesystem, custom)\n",
        "- **Streamlit** pour l'interface\n",
        "\"\"\")\n",
        "\n",
        "if \"boot_done\" not in st.session_state:\n",
        "    with st.spinner(\"DÃ©marrage des serveurs MCPâ€¦\"):\n",
        "        all_tools, tool_map = asyncio.run(start_all())\n",
        "    st.session_state.boot_done = True\n",
        "    st.session_state.all_tools = all_tools\n",
        "    st.session_state.tool_map = tool_map\n",
        "\n",
        "with st.expander(\"âš™ï¸ Configuration\"):\n",
        "    st.write(\"**Serveurs MCP actifs:**\")\n",
        "    st.write(f\"- ðŸ“¡ Fetch Server: âœ…\")\n",
        "    st.write(f\"- ðŸ“ Filesystem Server: âœ…\")\n",
        "    st.write(f\"- ðŸ› ï¸ Custom Server: âœ…\")\n",
        "    st.write(\"**Outils disponibles:**\")\n",
        "    for tool in st.session_state.all_tools:\n",
        "        st.write(f\"- `{tool['name']}`: {tool.get('description','No description')}\")\n",
        "\n",
        "goal = st.text_area(\n",
        "    \"ðŸŽ¯ Objectif (en anglais pour de meilleurs rÃ©sultats):\",\n",
        "    height=120,\n",
        "    placeholder=\"Ex: Download https://example.com/data.csv, convert to JSON, summarize, and save to summary.txt\"\n",
        ")\n",
        "\n",
        "cols = st.columns(3)\n",
        "with cols[0]:\n",
        "    if st.button(\"ðŸ“¥ TÃ©lÃ©charger et convertir\"):\n",
        "        goal = \"Download https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv, convert to JSON, and show first 3 records\"\n",
        "with cols[1]:\n",
        "    if st.button(\"ðŸ“ RÃ©sumer du texte\"):\n",
        "        goal = \"Summarize this text: 'Artificial intelligence is transforming many industries. Machine learning algorithms can now recognize patterns in data that humans might miss. This technology is being used in healthcare, finance, and transportation. The future of AI looks promising with continued advancements.'\"\n",
        "with cols[2]:\n",
        "    if st.button(\"ðŸ“ Lister les fichiers\"):\n",
        "        goal = \"List files in the current directory and show their contents\"\n",
        "\n",
        "if st.button(\"â–¶ï¸ ExÃ©cuter\", type=\"primary\"):\n",
        "    if not goal.strip():\n",
        "        st.warning(\"Veuillez entrer un objectif\")\n",
        "    else:\n",
        "        progress = st.progress(0)\n",
        "        status   = st.empty()\n",
        "        output   = st.empty()\n",
        "        with st.spinner(\"ExÃ©cutionâ€¦\"):\n",
        "            outputs = asyncio.run(run_agent(goal, st.session_state.all_tools, st.session_state.tool_map))\n",
        "        lines = []\n",
        "        for i, item in enumerate(outputs):\n",
        "            lines.append(item)\n",
        "            output.code(\"\\n\".join(lines), language=\"json\")\n",
        "            progress.progress(min((i+1)/max(1,len(outputs)), 1.0))\n",
        "            status.text(f\"Step {i+1}/{max(1,len(outputs))}\")\n",
        "        status.success(\"âœ… TÃ¢che terminÃ©e!\")\n",
        "\n",
        "st.sidebar.markdown(\"### ðŸ§¹ Nettoyage\")\n",
        "if st.sidebar.button(\"ArrÃªter les serveurs MCP\"):\n",
        "    asyncio.run(fetch_srv.shutdown())\n",
        "    asyncio.run(fs_srv.shutdown())\n",
        "    asyncio.run(my_srv.shutdown())\n",
        "    st.sidebar.success(\"Serveurs arrÃªtÃ©s\")\n"
      ],
      "metadata": {
        "id": "9MiGhj5BHVRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c07a4f-2b84-4136-8ca3-9e055e2ce539"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/mcp_demo/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rapide sanity check\n",
        "import asyncio\n",
        "\n",
        "try:\n",
        "    tools, tmap = asyncio.run(start_all())\n",
        "    print(\"OK, outils:\", [t[\"name\"] for t in tools])\n",
        "    asyncio.run(stop_all())\n",
        "except Exception as e:\n",
        "    print(\"Erreur MCP:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILEol38IS08c",
        "outputId": "fee45d91-45fd-4e58-d5d8-66da4600e051"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur MCP: asyncio.run() cannot be called from a running event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3366835543.py:9: RuntimeWarning: coroutine 'start_all' was never awaited\n",
            "  print(\"Erreur MCP:\", e)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§ª PARTIE 9.5 â€” Sanity check MCP â€” compatible Colab (pas d'asyncio.run direct)\n",
        "import asyncio, time\n",
        "import nest_asyncio; nest_asyncio.apply()\n",
        "\n",
        "def run_async(coro):\n",
        "    \"\"\"Si une loop tourne dÃ©jÃ  (Colab), retourne un Task; sinon exÃ©cute et retourne le rÃ©sultat.\"\"\"\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "    except RuntimeError:\n",
        "        loop = None\n",
        "    if loop and loop.is_running():\n",
        "        return asyncio.ensure_future(coro)\n",
        "    else:\n",
        "        return asyncio.run(coro)\n",
        "\n",
        "def await_result(x, poll_s: float = 0.05):\n",
        "    \"\"\"Attend un Task/Future; si x est dÃ©jÃ  une valeur, la renvoie telle quelle.\"\"\"\n",
        "    if not isinstance(x, (asyncio.Task, asyncio.Future)):\n",
        "        return x\n",
        "    while not x.done():\n",
        "        time.sleep(poll_s)\n",
        "    return x.result()\n",
        "\n",
        "# âš ï¸ PrÃ©-requis : PARTIE 5 (corrigÃ©e) et PARTIE 6 v2 doivent Ãªtre exÃ©cutÃ©es avant.\n",
        "if \"start_all\" not in globals() or \"stop_all\" not in globals():\n",
        "    raise RuntimeError(\"ExÃ©cute la PARTIE 5 (corrigÃ©e) puis la PARTIE 6 v2 avant ce sanity check.\")\n",
        "\n",
        "# DÃ©marrer â†’ lister â†’ arrÃªter avec logs\n",
        "print(\"â³ DÃ©marrage des serveurs MCP...\")\n",
        "started = run_async(start_all())\n",
        "try:\n",
        "    tools, tmap = await_result(started, poll_s=0.2)\n",
        "    print(\"âœ… Outils disponibles :\", [t[\"name\"] for t in tools])\n",
        "except Exception as e:\n",
        "    print(\"âŒ Erreur pendant start_all:\", e)\n",
        "\n",
        "print(\"â³ ArrÃªt des serveurs MCP...\")\n",
        "try:\n",
        "    await_result(run_async(stop_all()))\n",
        "    print(\"âœ… Serveurs MCP stoppÃ©s proprement.\")\n",
        "except Exception as e:\n",
        "    print(\"âŒ Erreur pendant stop_all:\", e)\n"
      ],
      "metadata": {
        "id": "CqBM2H84Zpuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8df26a7-e74f-41e3-f2ed-d3958440df14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ DÃ©marrage des serveurs MCP...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 10 â€“ Lancer Streamlit dans Colab"
      ],
      "metadata": {
        "id": "tetQqjFhHdn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ”Ÿ PARTIE 10 â€” URL externe via ngrok (avec authtoken)\n",
        "import os, time, subprocess, urllib.request, urllib.error\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# â¬‡ï¸ Mets tes tokens ici si tu veux\n",
        "os.environ[\"MY_API_TOKEN\"] = \"TON_TOKEN_APP_ICI\"            # si besoin dans app.py\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = \"30Y4I20WWMblLpoQmZBTJyvdfis_4tfap1sfjHNepzEv9yrdk\"   # OBLIGATOIRE pour ngrok\n",
        "\n",
        "# 1) VÃ©rifier authtoken ngrok\n",
        "authtoken = os.environ.get(\"NGROK_AUTHTOKEN\", \"\").strip()\n",
        "if not authtoken:\n",
        "    raise RuntimeError(\n",
        "        \"Aucun NGROK_AUTHTOKEN trouvÃ©. Va sur https://dashboard.ngrok.com/get-started/your-authtoken \"\n",
        "        \"puis place-le dans os.environ['NGROK_AUTHTOKEN'].\"\n",
        "    )\n",
        "\n",
        "# 2) DÃ©marrer Streamlit en arriÃ¨re-plan\n",
        "print(\"ðŸš€ Lancement de Streamlit (port 8501)â€¦\")\n",
        "log_path = \"/tmp/mcp_demo/streamlit.log\"\n",
        "streamlit_cmd = [\n",
        "    \"streamlit\", \"run\", \"/tmp/mcp_demo/app.py\",\n",
        "    \"--server.port=8501\",\n",
        "    \"--server.headless=true\",\n",
        "    \"--server.address=0.0.0.0\",\n",
        "    \"--server.enableCORS=false\",\n",
        "    \"--server.enableXsrfProtection=false\",\n",
        "    \"--server.enableWebsocketCompression=false\",\n",
        "    \"--browser.gatherUsageStats=false\",\n",
        "    \"--server.fileWatcherType=none\",\n",
        "]\n",
        "with open(log_path, \"w\") as lf:\n",
        "    streamlit_proc = subprocess.Popen(\n",
        "        streamlit_cmd, stdout=lf, stderr=subprocess.STDOUT, text=True\n",
        "    )\n",
        "print(f\"ðŸ“„ Logs: {log_path}\")\n",
        "\n",
        "# 3) Attendre que Ã§a rÃ©ponde en local\n",
        "def wait_http_ready(url, timeout=40):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            with urllib.request.urlopen(url, timeout=2) as r:\n",
        "                if r.status in (200, 302, 403):\n",
        "                    return True\n",
        "        except Exception:\n",
        "            time.sleep(1)\n",
        "    return False\n",
        "\n",
        "if not wait_http_ready(\"http://127.0.0.1:8501\"):\n",
        "    raise RuntimeError(\"Streamlit ne rÃ©pond pas en local (voir logs).\")\n",
        "\n",
        "print(\"âœ… Streamlit OK en local.\")\n",
        "\n",
        "# 4) Configurer ngrok + ouvrir le tunnel\n",
        "print(\"ðŸŒ Ouverture du tunnel ngrokâ€¦\")\n",
        "ngrok.set_auth_token(authtoken)\n",
        "public = ngrok.connect(8501, \"http\")\n",
        "public_url = str(public)\n",
        "print(f\"ðŸ”— URL publique: {public_url}\")\n",
        "\n",
        "# 5) Probe rapide externe (optionnel)\n",
        "try:\n",
        "    with urllib.request.urlopen(public_url, timeout=15) as r:\n",
        "        print(f\"ðŸ©º Probe externe: HTTP {r.status}\")\n",
        "except urllib.error.HTTPError as e:\n",
        "    print(f\"ðŸ©º Probe externe: HTTP {e.code}\")\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ©º Probe externe: error {e}\")\n",
        "\n",
        "print(\"ðŸ‘‰ Ouvre cette URL dans un onglet navigateur (pas en iframe):\")\n",
        "print(public_url)\n"
      ],
      "metadata": {
        "id": "8obhiGduHdba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57026241-26f4-45fa-d728-db08e7f6a81d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Lancement de Streamlit (port 8501)â€¦\n",
            "ðŸ“„ Logs: /tmp/mcp_demo/streamlit.log\n",
            "âœ… Streamlit OK en local.\n",
            "ðŸŒ Ouverture du tunnel ngrokâ€¦\n",
            "ðŸ”— URL publique: NgrokTunnel: \"https://ae553af22eaa.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "ðŸ©º Probe externe: error <urlopen error unknown url type: ngroktunnel>\n",
            "ðŸ‘‰ Ouvre cette URL dans un onglet navigateur (pas en iframe):\n",
            "NgrokTunnel: \"https://ae553af22eaa.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logs Streamlit (100 derniÃ¨res lignes)\n",
        "!tail -n 100 /tmp/mcp_demo/streamlit.log\n"
      ],
      "metadata": {
        "id": "KquttwIk9wKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which mcp-server-fetch\n",
        "!which mcp-server-filesystem\n",
        "!python3 -c \"import mcp, sys; print('mcp', mcp.__version__); import pandas; print('pandas', pandas.__version__)\"\n"
      ],
      "metadata": {
        "id": "CM5a80mt99yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list\n",
        "!curl -s http://localhost:11434/api/tags\n"
      ],
      "metadata": {
        "id": "U-o9lf7t9-27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§© PARTIE 11 â€“ Nettoyage automatique"
      ],
      "metadata": {
        "id": "OqL8EuURHlAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1ï¸âƒ£1ï¸âƒ£ Nettoyage des ressources\n",
        "import atexit, asyncio\n",
        "\n",
        "async def cleanup():\n",
        "    print(\"ðŸ§¹ Nettoyage des serveurs MCP (notebook)â€¦\")\n",
        "    try:\n",
        "        await asyncio.gather(\n",
        "            fetch_srv.shutdown(),\n",
        "            fs_srv.shutdown(),\n",
        "            my_srv.shutdown()\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if 'ollama_process' in globals() and ollama_process:\n",
        "            ollama_process.terminate()\n",
        "    except:\n",
        "        pass\n",
        "    print(\"âœ… Nettoyage terminÃ©\")\n",
        "\n",
        "atexit.register(lambda: asyncio.get_event_loop().run_until_complete(cleanup()))\n"
      ],
      "metadata": {
        "id": "Qd_2ahuDHkxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}