{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Reconstructed Challenge Taxonomy\n",
        "(Major sub-categories in bold, inner categories in bullet form; the diagram mirrors the hierarchy presented in § 6.2–6.4.)"
      ],
      "metadata": {
        "id": "kQNaNHVgvYEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Application-Developer Challenges\n",
        "├─ 1. Data & Model Challenges\n",
        "│   ├─ Data quality / imbalance\n",
        "│   ├─ Model selection / fine-tuning\n",
        "│   ├─ Prompt-dataset mismatch\n",
        "│   ├─ Version drift (model changes)\n",
        "│   └─ Evaluation / ground-truth scarcity\n",
        "├─ 2. Prompt Engineering & LLM Behavior\n",
        "│   ├─ Prompt brittleness / sensitivity\n",
        "│   ├─ Hallucination control\n",
        "│   ├─ Token-limit & context-window friction\n",
        "│   └─ Unpredictable output format\n",
        "├─ 3. Integration & Orchestration\n",
        "│   ├─ Chaining multi-step LLM calls\n",
        "│   ├─ Latency & throughput tuning\n",
        "│   ├─ Token-cost budgeting\n",
        "│   └─ Error handling / fallbacks\n",
        "├─ 4. Evaluation & Testing\n",
        "│   ├─ Lack of deterministic unit tests\n",
        "│   ├─ Subjective or task-specific metrics\n",
        "│   └─ Regression testing across model versions\n",
        "├─ 5. Security & Privacy\n",
        "│   ├─ Prompt injection / adversarial inputs\n",
        "│   ├─ Leakage of proprietary data\n",
        "│   └─ Regulatory compliance (PII, GDPR, etc.)\n",
        "└─ 6. Human-AI Collaboration\n",
        "    ├─ Inter-team prompt sharing\n",
        "    ├─ End-user trust & explainability\n",
        "    └─ Over-reliance on “vibe checks”"
      ],
      "metadata": {
        "id": "3JHhbrCFvYKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Key Methodological Design Decisions"
      ],
      "metadata": {
        "id": "LCcFuRstvYNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Multi-source data triangulation\n",
        "        Semi-structured interviews (n = 21) + issue posts from public repos (n = 744) + StackOverflow threads (n = 517).\n",
        "        Purpose: reduce single-source bias and reach saturation faster.\n",
        "    Two-phase coding procedure\n",
        "        Open coding → 1,043 raw codes → axial grouping → 6 high-level categories.\n",
        "        Selective coding → constant comparison until no new themes emerged (theoretical saturation).\n",
        "    Inter-rater reliability protocol\n",
        "        Two independent coders (κ = 0.78 initial → 0.86 after reconciliation).\n",
        "        Disagreements resolved via discussion and a third researcher when κ < 0.8.\n",
        "    Participant diversity controls\n",
        "        Purposive sampling across company size, domain, and LLM experience (1–7 yrs).\n",
        "        Ensures external validity for both indie and enterprise contexts."
      ],
      "metadata": {
        "id": "QLi_xX6nvYPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Validity & Reliability Safeguards"
      ],
      "metadata": {
        "id": "pOUJTvEJvsu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Threat Addressed         | Safeguard Implemented                                                              |\n",
        "| ------------------------ | ---------------------------------------------------------------------------------- |\n",
        "| **Construct validity**   | Pilot interview guide, member checking with 4 participants.                        |\n",
        "| **Internal reliability** | Dual coding + Cohen’s κ; iterative codebook refinement.                            |\n",
        "| **External validity**    | Stratified sampling; dataset spans 4 verticals (health, finance, ed-tech, gaming). |\n",
        "| **Confirmability**       | Audit trail: all codes, memos, and decision logs stored in a shared repository.    |\n"
      ],
      "metadata": {
        "id": "LhRe0f2Bvsod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Dominant Challenge Patterns (Quantitative)"
      ],
      "metadata": {
        "id": "yboLVAORvzPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the coded corpus:\n",
        "\n",
        "    ≈ 34 % of all issue posts relate to Prompt Engineering & LLM Behavior (prompt brittleness & hallucination).\n",
        "    ≈ 27 % fall under Integration & Orchestration (latency, chaining, cost).\n",
        "    ≈ 18 % concern Evaluation & Testing (lack of deterministic tests).\n",
        "    Remaining 21 % distributed across Data, Security, and Human-AI collaboration.\n",
        "\n",
        "Interpretation: developer pain has shifted upstream—from “can I train a model?” to “can I make the model behave deterministically in production?”"
      ],
      "metadata": {
        "id": "Vf289NwPvz0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Implications for LLM Platform/API Design\n"
      ],
      "metadata": {
        "id": "4qvcVtNtv5eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Challenge Hot-Spot          | Design Implication                                                                                                 |\n",
        "| --------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
        "| Prompt brittleness          | Provide **prompt regression suites** (version diff + automatic prompt mutation testing).                           |\n",
        "| Evaluation scarcity         | Offer **built-in task-specific eval harnesses** with synthetic adversarial sets & human-in-the-loop review UI.     |\n",
        "| Latency/cost tuning         | Expose **per-call budget/latency SLAs** + **token-bucket middleware** for automatic fallbacks.                     |\n",
        "| Security (prompt injection) | Ship **static & runtime prompt scanners** (similar to SQL-injection liners).                                       |\n",
        "| Model drift                 | Surface **semantic diff alerts** when a new model version changes the distribution of outputs on a golden dataset. |\n"
      ],
      "metadata": {
        "id": "RQb5ZBh3v5wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Cross-Cutting Themes: Paper vs. My (Expected) Developer Reality"
      ],
      "metadata": {
        "id": "pQXspylrwBvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Theme                       | Evidence from Paper                                                             | My Experience / Expectation                                                                                |\n",
        "| --------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
        "| **“Prompt as Code”**        | Developers version prompts in Git, but lack diff semantics.                     | I already store prompts in YAML and manually eyeball changes—would love semantic diff.                     |\n",
        "| **Cost is the new latency** | 27 % of issues mention token-budgeting; developers build home-grown throttlers. | My side-projects hit OpenAI rate limits; I hacked a Redis token bucket—should be a built-in feature.       |\n",
        "| **Evaluation paralysis**    | No consensus on metrics; teams rely on ad-hoc vibe checks.                      | When fine-tuning chatbots, I struggled to quantify “helpfulness”; paper validates that tooling is missing. |\n"
      ],
      "metadata": {
        "id": "CHHtJW8xwBjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Two Original Tool / Community Ideas"
      ],
      "metadata": {
        "id": "wLU3DBrDwHg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " PromptLab OSS\n",
        "        A VS-Code extension + GitHub Action that treats prompts like source code: semantic diff, unit tests via synthetic assertions, and a “model canary” that reruns golden examples on every model release.\n",
        "        Community repo of battle-tested prompts + crowd-sourced eval datasets.\n",
        "    LLM Cost & Latency Playground\n",
        "        Browser-based sandbox where developers paste a prompt chain; the tool auto-generates a Pareto frontier of (latency, cost, quality) across multiple model endpoints.\n",
        "        Includes a shareable “budget card” (YAML) that can be committed to CI to enforce SLAs."
      ],
      "metadata": {
        "id": "KcFfOjuqwHVO"
      }
    }
  ]
}