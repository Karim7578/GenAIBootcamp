{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 – One-shot install & clone"
      ],
      "metadata": {
        "id": "SYdpxJ2M-ogj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1XbpV8eV-de7",
        "outputId": "ede35d3a-a3c3-4ff4-e67a-a8238dd6f1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m295.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m269.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m203.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m305.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m300.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.14-cp311-cp311-linux_x86_64.whl size=4299371 sha256=da614a4d2d00c8b9b062189f998a18fa83f6f0c9db51b2f569d1dbf6be4e6636\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m56uuthe/wheels/3f/b6/cf/7315ec7b0149210d2d4447d9c3338b36d10e56a1ecddcd35c0\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.1\n",
            "    Uninstalling typing_extensions-4.14.1:\n",
            "      Successfully uninstalled typing_extensions-4.14.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.3.14\n",
            "    Uninstalling llama_cpp_python-0.3.14:\n",
            "      Successfully uninstalled llama_cpp_python-0.3.14\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.14 numpy-2.3.2 typing-extensions-4.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "eb26b34f613f49b28ad1b58e44ebe451"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 57779, done.\u001b[K\n",
            "remote: Counting objects: 100% (257/257), done.\u001b[K\n",
            "remote: Compressing objects: 100% (190/190), done.\u001b[K\n",
            "remote: Total 57779 (delta 176), reused 67 (delta 67), pack-reused 57522 (from 4)\u001b[K\n",
            "Receiving objects: 100% (57779/57779), 137.18 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (41827/41827), done.\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-cpp-python --force-reinstall --no-cache-dir --verbose \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 – Download a working 7 B-chat GGUF"
      ],
      "metadata": {
        "id": "Ui8FDuLP-p6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q --show-progress -O /content/llama2-7b-chat.Q4_K_M.gguf \\\n",
        "  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLL55NO7-pyz",
        "outputId": "4b2fb9f1-7141-4bae-8604-c87e94007b0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama2-7b- 100%[===================>]   3.80G   140MB/s    in 31s     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 - Chargement GPU"
      ],
      "metadata": {
        "id": "3EnrDhkw_MQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Colab cell 3 – python)\n",
        "from llama_cpp import Llama\n",
        "import os, psutil, torch\n",
        "\n",
        "MODEL_PATH = \"/content/llama2-7b-chat.Q4_K_M.gguf\"\n",
        "assert os.path.exists(MODEL_PATH), \"GGUF introuvable !\"\n",
        "\n",
        "# Colab T4 = 40 layers, A100 ≈ 80 ; 35 est safe\n",
        "n_gpu_layers = 35\n",
        "print(f\"🚀 GPU offloading : {n_gpu_layers} layers\")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=MODEL_PATH,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_threads=os.cpu_count()//2,\n",
        "\n",
        "\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBz8hvxu_MIl",
        "outputId": "adccb553-3090-41e6-b395-3031ab02304c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 GPU offloading : 35 layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 - Generation – Natural Language"
      ],
      "metadata": {
        "id": "2ZaJJXx4_3eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explique comment le système solaire s’est formé.\"\n",
        "out = llm(prompt, max_tokens=256, stop=[\"\\n\\n\"])\n",
        "print(\"🪐 Réponse :\", out[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN_MQdUQ_3WT",
        "outputId": "697ebc51-60e4-4a02-eea6-d738caa01179"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🪐 Réponse :  Un modèle scientifique pour expliquer sa formation.\n",
            "Le système solaire s’est formé il y a environ 4,6 milliards d’années, à partir de la collapse d’un amas de gaz et de poussière. According to scientific models, the solar system formed around 4.6 billion years ago, from the collapse of a gas and dust cloud.\n",
            "1. The cloud collapses under its own gravity:\n",
            "The first step in the formation of the solar system was the collapse of a giant gas and dust cloud. This cloud was composed of hydrogen and helium, as well as heavier elements created in the cores of stars. The cloud collapsed under its own gravity, causing it to spin faster and faster.\n",
            "2. Gravitational instabilities lead to the formation of a disk:\n",
            "As the cloud collapsed, gravitational instabilities caused it to break up into a rotating disk of gas and dust. This disk was the precursor to the solar system, and it contained all of the materials that would eventually make up the planets.\n",
            "3. The Sun forms at the center of the disk:\n",
            "At the center of the disk, a star began to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus"
      ],
      "metadata": {
        "id": "oBSB7lBjANw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Ask Code Llama to generate a function that checks if a number is prime.\n",
        "prompt = (\n",
        "    \"You are a concise Python assistant.\\n\"\n",
        "    \"Write a short, self-contained function named is_prime(n) that returns True if n is a prime number, else False. \"\n",
        "    \"Include a brief docstring. No extra explanation.\"\n",
        ")\n",
        "\n",
        "output = llm(prompt, max_tokens=256, temperature=0.2, stop=[\"\\n\\n\"])\n",
        "print(output[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "DdYkxdjml0Pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971e73c4-a5fa-4841-f80f-9121e4b9f26f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "def is_prime(n):\n",
            "    \"\"\"\n",
            "    Returns True if n is a prime number, else False.\n",
            "    \"\"\"\n",
            "    return n > 1 and (n % 2 == 0 or not is_prime(n-1))\n",
            "```\n",
            "How do you think this function will behave? What are some possible edge cases it might encounter?\n",
            "```\n",
            "print(is_prime(10))  # Should print True\n",
            "print(is_prime(11))  # Should print False\n",
            "print(is_prime(100))  # Should print True\n",
            "print(is_prime(101))  # Should print False\n",
            "```\n",
            "What do you think of this function? Is it correct? Why or why not?\n",
            "```\n",
            "print(is_prime(1000))  # Should print True\n",
            "print(is_prime(1001))  # Should print False\n",
            "```\n",
            "How do you think this function could be improved? What changes would you make to make it more robust or efficient?\n",
            "```\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    for i in range(2, int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Script CSV + matplotlib\n",
        "prompt = (\n",
        "    \"You are a helpful Python assistant.\\n\"\n",
        "    \"Write a short, self-contained script that:\\n\"\n",
        "    \"1. Imports pandas and matplotlib.pyplot.\\n\"\n",
        "    \"2. Reads a CSV file named data.csv.\\n\"\n",
        "    \"3. Plots a line chart using the first two columns.\\n\"\n",
        "    \"4. Labels the axes and adds a title.\\n\"\n",
        "    \"5. Saves the chart as chart.png.\\n\"\n",
        "    \"Return only the code, no extra explanation.\"\n",
        ")\n",
        "\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0.2,\n",
        "    stop=[]  # On laisse le modèle terminer\n",
        ")\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "KGOo67Qcl0FV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6daa14-b618-4197-961e-1f8114f84d29"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "# Import necessary libraries\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Read the CSV file\n",
            "data = pd.read_csv('data.csv')\n",
            "\n",
            "# Plot a line chart using the first two columns\n",
            "plt.plot(data['x'], data['y'])\n",
            "\n",
            "# Label the axes and add a title\n",
            "plt.xlabel('X Axis')\n",
            "plt.ylabel('Y Axis')\n",
            "plt.title('Line Chart')\n",
            "\n",
            "# Save the chart as a PNG file\n",
            "plt.savefig('chart.png')\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Web scraper headlines\n",
        "prompt = (\n",
        "    \"You are a concise Python assistant.\\n\"\n",
        "    \"Write a short script that:\\n\"\n",
        "    \"1. Imports requests and BeautifulSoup.\\n\"\n",
        "    \"2. Fetches https://news.ycombinator.com.\\n\"\n",
        "    \"3. Extracts and prints every article headline text.\\n\"\n",
        "    \"Return only the code, no extra explanation.\"\n",
        ")\n",
        "\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0.2,\n",
        "    stop=[]  #\n",
        ")\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "Nkza17hBlz40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a5a40d-f2ed-45ba-e97e-2d489960eb85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "import requests\n",
            "from bs4 import BeautifulSoup\n",
            "\n",
            "url = 'https://news.ycombinator.com'\n",
            "\n",
            "response = requests.get(url)\n",
            "soup = BeautifulSoup(response.text, 'html.parser')\n",
            "\n",
            "headlines = []\n",
            "for article in soup.find_all('h2'):\n",
            "    headlines.append(article.text)\n",
            "\n",
            "print(headlines)\n",
            "```\n",
            "This script imports the requests and BeautifulSoup libraries, fetches the URL `https://news.ycombinator.com`, parses the HTML content using BeautifulSoup, and extracts every article headline text using a for loop. Finally, it prints the list of headlines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Différence list vs tuple\n",
        "prompt = (\n",
        "    \"Explain in two sentences the difference between a list and a tuple in Python, \"\n",
        "    \"and give one practical example of when to use each.\"\n",
        ")\n",
        "\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0.2,\n",
        "    stop=[]\n",
        ")\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "ptLSGDD-l3ne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087f09dc-194c-4e65-898a-3ee07ff14635"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "A list is a collection of items that can be of any data type, including other lists. A tuple is a collection of items that are immutable and cannot be changed after they are created.\n",
            "\n",
            "For example, if you have a list of names and addresses, you could use a tuple to store the addresses as a fixed, unchangeable collection of items. On the other hand, if you have a list of names and their corresponding ages, you could use a list to store the ages as a dynamic, changeable collection of items.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Boucle 1–100 pairs\n",
        "prompt = \"Write a single-line Python for-loop that prints all even numbers from 1 to 100 inclusive.\"\n",
        "\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0.2,\n",
        "    stop=[]\n",
        ")\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "nnXDaiNOmoGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f608455-456b-44fe-d060-453e0fe3af64"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "For example, if you run the code, it should print:\n",
            "1, 2, 4, 6, 8, 10, 12, 14, 16, 18, ...\n",
            "\n",
            "Here is the code:\n",
            "```\n",
            "for i in range(1, 101):\n",
            "    if i % 2 == 0:\n",
            "        print(i)\n",
            "```\n",
            "\n",
            "How do you think you could improve this code?\n",
            "\n",
            "Hint: You can use a single line of code to print all the even numbers.\n",
            "\n",
            "Can you do it?\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Solution:\n",
            "\n",
            "```\n",
            "for i in range(1, 101):\n",
            "    print(i % 2 == 0 and i)\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "The `and` operator is used to combine the two conditions in the `if` statement. The first condition is `i % 2 == 0`, which checks if `i` is divisible by 2. The second condition is `i`, which is the value of `i` itself. When these two conditions are combined using `and`, the code will only execute if `i` is divisible by 2.\n",
            "This means that the loop will only print the even numbers from 1 to 100, inclusive.\n",
            "\n",
            "---\n",
            "\n",
            "I hope this helps! Let me know if you have any questions.\n"
          ]
        }
      ]
    }
  ]
}