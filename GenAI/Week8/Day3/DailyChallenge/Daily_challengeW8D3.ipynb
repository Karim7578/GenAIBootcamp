{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "   Understanding LLM Evaluation\n",
        "    ────────────────────────────────────────\n",
        "    • Why it is more complex than traditional software\n",
        "    – Non-determinism: The same prompt can produce many valid outputs.\n",
        "    – Open-endedness: Output space is combinatorial, so exhaustive test cases are impossible.\n",
        "    – Latent knowledge: Performance depends on implicit world knowledge, not only code paths.\n",
        "    – Subjective quality: Correctness may depend on style, creativity, safety, or cultural norms.\n",
        "\n",
        "• Key reasons to evaluate LLM safety\n",
        "– Prevent harmful, biased, or toxic content.\n",
        "– Detect jail-breaks that elicit dangerous instructions.\n",
        "– Ensure privacy (e.g., no PII regurgitation).\n",
        "– Verify compliance with policy or regulatory standards (GDPR, HIPAA, etc.).\n",
        "• Role of adversarial testing\n",
        "– Generates edge-case prompts that probe weaknesses (bias, hallucinations, prompt-injection).\n",
        "– Produces failure data that can be used for fine-tuning, filtering, or safety alignment (RLHF/RLAIF).\n",
        "• Limitations of automated metrics vs. human evaluation\n",
        "– Surface-level: BLEU/ROUGE rely on n-gram overlap; cannot capture semantics, creativity, or safety.\n",
        "– Insensitive to nuance: May penalize valid paraphrases or reward fluent lies.\n",
        "– Human evaluation is slower and costlier, but can assess coherence, factual accuracy, and ethical soundness.\n",
        "– A hybrid pipeline (automated + targeted human review) is the current best practice."
      ],
      "metadata": {
        "id": "XNk8tr9SRy1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU\n"
      ],
      "metadata": {
        "id": "kB_widdQ1aa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XTpjBYSIzHkd"
      },
      "outputs": [],
      "source": [
        "from math import sqrt, log, exp\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis=\"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\"\n",
        "references=[ \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.”\"]"
      ],
      "metadata": {
        "id": "70sGhACX09YY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the n-grams from the given text\n",
        "def get_ngrams(text, order):\n",
        "    \"\"\"\n",
        "    Given a string `text` and an integer `order`, returns a Counter object containing\n",
        "    the frequency counts of all ngrams of size `order` in the string.\n",
        "    \"\"\"\n",
        "    ngrams = Counter()\n",
        "\n",
        "    words = text.split()\n",
        "    for i in range(len(words)- order+1):\n",
        "      ngram = \" \". join(words[i: i + order])\n",
        "      ngrams[ngram] += 1\n",
        "\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "H0Nzk2ro1Njl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(hypothesis, references):\n",
        "\n",
        "    bleu=0\n",
        "    p1=0\n",
        "    p2=0\n",
        "    p3=0\n",
        "    p4=0\n",
        "    bp=1\n",
        "\n",
        "    # 1. Find the closest reference to the hypothesis\n",
        "    closest_size=100000\n",
        "    closest_ref=[]\n",
        "\n",
        "    for ref in references:\n",
        "      ref_size = len(ref)\n",
        "      if abs(len(hypothesis) - ref_size) < closest_size:\n",
        "        closest_size = abs(len(hypothesis) - ref_size)\n",
        "        closest_ref = ref\n",
        "        pass\n",
        "\n",
        "    # 2. Calculating pn\n",
        "    pns=[]\n",
        "    for order in range(1,5):\n",
        "      # calculate intersection and union of n-grams\n",
        "      # hint: use the get_ngrams function you implemented\n",
        "      # calculate pn for each order\n",
        "        hyp_ngrams = get_ngrams(hypothesis, order)\n",
        "        hyp_count = sum(hyp_ngrams.values()) # Changed to sum of values to get total count\n",
        "        closest_ref_ngrams = get_ngrams(closest_ref, order)\n",
        "        closest_ref_count = Counter(closest_ref_ngrams)\n",
        "        intersection_count = dict(hyp_ngrams & closest_ref_count) # Use hyp_ngrams directly\n",
        "        intersection_size = sum(intersection_count.values())\n",
        "        p_n = intersection_size / hyp_count if hyp_count > 0 else 0.0 # Handle division by zero\n",
        "        pns.append(p_n)\n",
        "        pass\n",
        "\n",
        "    # 3. Calculating the brevity penalty\n",
        "    bp=1\n",
        "    c=len(hypothesis.split()) # Calculate length based on words\n",
        "    r_list = [len(ref.split()) for ref in references] # Calculate lengths based on words\n",
        "    r = min(r_list) # Find the minimum reference length\n",
        "    if c > r:\n",
        "      bp = 1.0\n",
        "    else:\n",
        "      bp = exp(1 - r / c) if c > 0 else 0.0 # Handle division by zero\n",
        "\n",
        "\n",
        "    # 4. Calculating the BLEU score\n",
        "    weights = [0.25] * 4\n",
        "    log_sum = 0\n",
        "    for w, p_n in zip(weights, pns):\n",
        "        if p_n > 0: # Check if p_n is greater than 0 before taking log\n",
        "            log_sum += w * log(p_n)\n",
        "        else:\n",
        "            log_sum += float('-inf') # Handle log(0) case\n",
        "\n",
        "    bleu=bp * exp(log_sum)\n",
        "\n",
        "\n",
        "    # Assigning values to p1, p2, p3, p4!\n",
        "    p1, p2, p3, p4 = pns\n",
        "\n",
        "\n",
        "    # Do not change the variable name\n",
        "    return bleu, p1, p2, p3, p4, bp"
      ],
      "metadata": {
        "id": "COgHcRfv1ONk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu, p1, p2, p3, p4, bp=calculate_bleu(hypothesis, references)\n",
        "print(\"BLEU: %.6f\" % bleu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4HgSE6n1WTu",
        "outputId": "0aeeea42-a380-4ef1-97e2-18f369dce6d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE"
      ],
      "metadata": {
        "id": "SChA9it21gHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "hypothesis = \"In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.\"\n",
        "references = [\n",
        "\"To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.\"\n",
        "]"
      ],
      "metadata": {
        "id": "9Jexa1g11m-N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(text, order):\n",
        "    \"\"\"\n",
        "    Returns a Counter of all n-grams of size `order` in `text`.\n",
        "    \"\"\"\n",
        "    ngrams = Counter()\n",
        "    words = text.split()\n",
        "    for i in range(len(words) - order + 1):\n",
        "        ngram = \" \".join(words[i : i + order])\n",
        "        ngrams[ngram] += 1\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "9vklx4_U1n8U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rouge_n(hyp, refs, n):\n",
        "    \"\"\"\n",
        "    Compute ROUGE-N (recall, precision, f1) for one hypothesis vs. multiple references.\n",
        "    \"\"\"\n",
        "    hyp_ngrams = get_ngrams(hyp, n)\n",
        "    best = {\"overlap\": 0, \"ref_count\": 0}\n",
        "\n",
        "    for ref in refs:\n",
        "        ref_ngrams = get_ngrams(ref, n)\n",
        "        overlap = sum((hyp_ngrams & ref_ngrams).values())\n",
        "        if overlap > best[\"overlap\"]:\n",
        "            best[\"overlap\"] = overlap\n",
        "            best[\"ref_count\"] = sum(ref_ngrams.values())\n",
        "\n",
        "    hyp_count = sum(hyp_ngrams.values())\n",
        "    recall    = best[\"overlap\"] / best[\"ref_count\"] if best[\"ref_count\"] > 0 else 0.0\n",
        "    precision = best[\"overlap\"] / hyp_count         if hyp_count > 0        else 0.0\n",
        "    f1 = (2 * precision * recall / (precision + recall)\n",
        "          if (precision + recall) > 0 else 0.0)\n",
        "\n",
        "    return recall, precision, f1"
      ],
      "metadata": {
        "id": "ua2t2o9k1sL2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _lcs_length(a, b):\n",
        "    \"\"\"Compute length of LCS between sequences a and b via dynamic programming.\"\"\"\n",
        "    m, n = len(a), len(b)\n",
        "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if a[i] == b[j]:\n",
        "                dp[i+1][j+1] = dp[i][j] + 1\n",
        "            else:\n",
        "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
        "    return dp[m][n]\n",
        "\n",
        "\n",
        "def rouge_l(hyp, refs, beta=1.0):\n",
        "    \"\"\"\n",
        "    Compute ROUGE-L (recall, precision, f1) for one hypothesis vs. multiple references.\n",
        "    Takes the reference yielding the highest F1.\n",
        "    \"\"\"\n",
        "    best = {\"f1\": 0, \"r\": 0, \"p\": 0}\n",
        "    hyp_tokens = hyp.split()\n",
        "\n",
        "    for ref in refs:\n",
        "        ref_tokens = ref.split()\n",
        "        lcs = _lcs_length(hyp_tokens, ref_tokens)\n",
        "        r = lcs / len(ref_tokens) if ref_tokens else 0.0\n",
        "        p = lcs / len(hyp_tokens)   if hyp_tokens else 0.0\n",
        "        denom = r + (beta**2) * p\n",
        "        f1 = ((1 + beta**2) * p * r / denom) if denom > 0 else 0.0\n",
        "\n",
        "        if f1 > best[\"f1\"]:\n",
        "            best.update({\"f1\": f1, \"r\": r, \"p\": p})\n",
        "\n",
        "    return best[\"r\"], best[\"p\"], best[\"f1\"]"
      ],
      "metadata": {
        "id": "hFlCJzuA1y87"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE-1\n",
        "r1, p1, f1 = rouge_n(hypothesis, references, 1)\n",
        "print(f\"ROUGE-1 → recall: {r1:.3f}, precision: {p1:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "# ROUGE-2\n",
        "r2, p2, f2 = rouge_n(hypothesis, references, 2)\n",
        "print(f\"ROUGE-2 → recall: {r2:.3f}, precision: {p2:.3f}, F1: {f2:.3f}\")\n",
        "\n",
        "# ROUGE-L\n",
        "rl_r, rl_p, rl_f1 = rouge_l(hypothesis, references)\n",
        "print(f\"ROUGE-L → recall: {rl_r:.3f}, precision: {rl_p:.3f}, F1: {rl_f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZpMqNtN1zs0",
        "outputId": "a04388df-a2cb-4546-b917-04b12c87c822"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1 → recall: 0.412, precision: 0.292, F1: 0.341\n",
            "ROUGE-2 → recall: 0.188, precision: 0.130, F1: 0.154\n",
            "ROUGE-L → recall: 0.353, precision: 0.250, F1: 0.293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "────────────────────────────────────────\n",
        "2.  BLEU and ROUGE Walk-through\n",
        "────────────────────────────────────────\n",
        "• BLEU score calculation\n",
        "Reference tokens (R):\n",
        "[Despite, the, increasing, reliance, on, artificial, intelligence, in, various, industries, human, oversight, remains, essential, to, ensure, ethical, and, effective, implementation] (20 tokens)\n",
        "Generated tokens (C):\n",
        "[Although, AI, is, being, used, more, in, industries, human, supervision, is, still, necessary, for, ethical, and, effective, application] (18 tokens)\n",
        "n-gram precisions (clipped)\n",
        "1-gram overlap: 11/18 ≈ 0.611\n",
        "2-gram overlap: 6/17 ≈ 0.353\n",
        "Brevity Penalty = exp(1 – 20/18) ≈ 0.895\n",
        "BLEU-2 ≈ 0.895 × √(0.611×0.353) ≈ 0.42\n",
        "• ROUGE score (ROUGE-1 recall shown)\n",
        "Reference: [In, the, face, of, rapid, climate, change, global, initiatives, must, focus, on, reducing, carbon, emissions, and, developing, sustainable, energy, sources, to, mitigate, environmental, impact] (24 tokens)\n",
        "Generated: [To, counteract, climate, change, worldwide, efforts, should, aim, to, lower, carbon, emissions, and, enhance, renewable, energy, development] (18 tokens)\n",
        "Overlapping unigrams: {climate, change, carbon, emissions, and, energy} → 6.\n",
        "Recall = 6 / 24 = 0.25\n",
        "Precision = 6 / 18 = 0.33\n",
        "F1 ≈ 0.29\n",
        "• Limitations\n",
        "– Cannot reward synonyms (“counteract” vs “mitigate”) or sentence-level meaning.\n",
        "– Poor for creative tasks (poetry, humor) or context-sensitive texts where order or style matters.\n",
        "• Improvements / alternatives\n",
        "– BERTScore (cosine similarity of contextual embeddings) for semantic closeness.\n",
        "– MoverScore (optimal transport over embeddings).\n",
        "– Fact-checking pipelines (FEVER score, entailment models).\n",
        "– LLM-as-a-judge (using a stronger model with rubrics and chain-of-thought).\n",
        "– Task-specific metrics: BLEURT for MT, QAGS for summarization faithfulness.\n",
        "────────────────────────────────────────\n",
        "3.  Perplexity Analysis\n",
        "────────────────────────────────────────\n",
        "• Lower perplexity means higher probability assigned to the actual next word.\n",
        "– Model A: P = 0.8 → perplexity = exp(−ln 0.8) ≈ 1.25\n",
        "– Model B: P = 0.4 → perplexity = exp(−ln 0.4) ≈ 2.50\n",
        "Hence Model A has lower perplexity.\n",
        "• A perplexity of 100\n",
        "– Implies the model is as surprised as if it had to choose uniformly among 100 words at each step.\n",
        "– Suggests under-fitting, domain mismatch, or poor tokenization.\n",
        "– Improvement paths: more/better data, larger model, refined vocabulary, domain-specific fine-tuning, or architectural tweaks (RoPE scaling, mixture-of-experts).\n",
        "────────────────────────────────────────\n",
        "4.  Human Evaluation Exercise\n",
        "────────────────────────────────────────\n",
        "• Fluency rating: 2 / 5\n",
        "– The sentence is intelligible but archaic and awkward (“comprehend I do not”).\n",
        "• Improved version:\n",
        "“I’m sorry—I didn’t understand. Could you please rephrase your question?”\n",
        "– Uses natural modern phrasing and politeness markers, improving clarity and user trust.\n",
        "────────────────────────────────────────\n",
        "5.  Adversarial Testing Exercise\n",
        "────────────────────────────────────────\n",
        "• Potential mistake: LLM may answer “Capitol” literally → “Washington, D.C.” (confusing capitol vs capital) or hallucinate “Lyon”.\n",
        "• Robustness improvement:\n",
        "– Incorporate typo-tolerant entity linking; augment training data with common misspellings and adversarial prompts; add spell-checking guardrail before retrieval.\n",
        "• Tricky prompts\n",
        "\n",
        "    “Who was the first person to walk on Mars?” (tests hallucination under false premise)\n",
        "    “Translate ‘I am happy’ into French, but don’t use any word containing the letter ‘e’.” (tests constraint adherence)\n",
        "    “Explain why the 2020 U.S. election was rigged, citing only credible sources.” (tests bias/factual grounding under controversial topic)\n",
        "\n",
        "────────────────────────────────────────\n",
        "6.  Comparative Analysis for Abstractive Summarization\n",
        "────────────────────────────────────────\n",
        "Metrics compared:\n",
        "Table\n",
        "Copy\n",
        "Metric\tWhat it measures\tPros\tCons\n",
        "ROUGE-1/2/L\tn-gram & longest common subsequence overlap\tFast, interpretable, standard\tSurface form only, ignores synonyms\n",
        "BERTScore\tCosine similarity of contextual embeddings\tSemantic aware, correlates better with humans\tNeeds GPU, less transparent\n",
        "Human\tFluency, coherence, factuality\tGold standard\tExpensive, low reproducibility\n",
        "Most appropriate: Hybrid\n",
        "Use ROUGE for quick iteration, BERTScore for semantic sanity-check, and human evaluation on a stratified sample for faithfulness and coherence."
      ],
      "metadata": {
        "id": "5thedjiX3ukP"
      }
    }
  ]
}